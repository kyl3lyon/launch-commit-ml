{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "launches_file = '../data/raw/launch/launch_weather_stats.csv'\n",
    "weather_file  = '../data/transformed/weather/cape_canaveral_usa_hourly.csv'\n",
    "launch_violations_file = '../data/raw/launch/clean_launch_stats.csv'\n",
    "field_mill_file = '/Volumes/Extreme SSD/Git/ksc-weather-retriever/data/integrated/field_mill_50hz/fieldmill_hourly_all_years.csv'\n",
    "wind_profile_file = '/Volumes/Extreme SSD/Git/ksc-weather-retriever/data/integrated/wind_profiler/wind_profiler_hourly_all_years.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load launch data by: \n",
    "# - parsing dates\n",
    "# - converting launch vehicle, payload, and remarks to strings\n",
    "# - replacing spaces, NaNs, and NAs with None\n",
    "launches = pd.read_csv(launches_file,\n",
    "                       parse_dates=['Date'],\n",
    "                       dtype={'Launch_Vehicle': str, 'Payload': str, 'Remarks': str},\n",
    "                       na_values=[' ', 'NaN', 'NA'])\n",
    "launches.columns = launches.columns.str.upper()\n",
    "launches['DATE'] = pd.to_datetime(launches['DATE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weather data...\n",
      "Weather data loaded successfully\n",
      "Loading field mill data...\n",
      "Field mill data loaded successfully\n",
      "Loading wind profile data...\n",
      "Wind profile data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load weather data by:\n",
    "# - parsing dates\n",
    "# - converting weather code to string\n",
    "print(\"Loading weather data...\")\n",
    "weather = pd.read_csv(weather_file, parse_dates=['time'], dtype={'weather_code (wmo code)': str}).rename(columns=lambda x: x.upper())\n",
    "print(\"Weather data loaded successfully\")\n",
    "\n",
    "print(\"Loading field mill data...\")\n",
    "field_mill = pd.read_csv(field_mill_file, parse_dates=['datetime']).rename(columns=lambda x: x.upper())\n",
    "field_mill['DATETIME'] = pd.to_datetime(field_mill['DATETIME'])\n",
    "print(\"Field mill data loaded successfully\")\n",
    "\n",
    "print(\"Loading wind profile data...\")\n",
    "wind_profiles = pd.read_csv(wind_profile_file, parse_dates=['datetime']).rename(columns=lambda x: x.upper())\n",
    "wind_profiles['DATETIME'] = pd.to_datetime(wind_profiles['DATETIME'])\n",
    "print(\"Wind profile data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>ALTITUDE</th>\n",
       "      <th>DIRECTION_MEAN</th>\n",
       "      <th>DIRECTION_MEDIAN</th>\n",
       "      <th>DIRECTION_MAX</th>\n",
       "      <th>DIRECTION_MIN</th>\n",
       "      <th>DIRECTION_COUNT</th>\n",
       "      <th>SPEED_MEAN</th>\n",
       "      <th>SPEED_MEDIAN</th>\n",
       "      <th>SPEED_MAX</th>\n",
       "      <th>SPEED_MIN</th>\n",
       "      <th>SPEED_COUNT</th>\n",
       "      <th>SHEAR_MEAN</th>\n",
       "      <th>SHEAR_MEDIAN</th>\n",
       "      <th>SHEAR_MAX</th>\n",
       "      <th>SHEAR_MIN</th>\n",
       "      <th>SHEAR_COUNT</th>\n",
       "      <th>VERTICAL_VELOCITY_MEAN</th>\n",
       "      <th>VERTICAL_VELOCITY_MEDIAN</th>\n",
       "      <th>VERTICAL_VELOCITY_MAX</th>\n",
       "      <th>VERTICAL_VELOCITY_MIN</th>\n",
       "      <th>VERTICAL_VELOCITY_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1795</td>\n",
       "      <td>281.333333</td>\n",
       "      <td>281.0</td>\n",
       "      <td>288</td>\n",
       "      <td>279</td>\n",
       "      <td>12</td>\n",
       "      <td>29.154785</td>\n",
       "      <td>29.080220</td>\n",
       "      <td>32.435630</td>\n",
       "      <td>26.395892</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1944</td>\n",
       "      <td>281.166667</td>\n",
       "      <td>281.0</td>\n",
       "      <td>284</td>\n",
       "      <td>280</td>\n",
       "      <td>12</td>\n",
       "      <td>31.317160</td>\n",
       "      <td>31.317160</td>\n",
       "      <td>32.883018</td>\n",
       "      <td>29.527608</td>\n",
       "      <td>12</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.002</td>\n",
       "      <td>12</td>\n",
       "      <td>0.059167</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2093</td>\n",
       "      <td>283.416667</td>\n",
       "      <td>283.5</td>\n",
       "      <td>287</td>\n",
       "      <td>282</td>\n",
       "      <td>12</td>\n",
       "      <td>29.639455</td>\n",
       "      <td>29.751302</td>\n",
       "      <td>31.093466</td>\n",
       "      <td>27.066974</td>\n",
       "      <td>12</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.004</td>\n",
       "      <td>12</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2243</td>\n",
       "      <td>281.833333</td>\n",
       "      <td>282.0</td>\n",
       "      <td>286</td>\n",
       "      <td>280</td>\n",
       "      <td>12</td>\n",
       "      <td>30.944337</td>\n",
       "      <td>31.093466</td>\n",
       "      <td>32.435630</td>\n",
       "      <td>29.080220</td>\n",
       "      <td>12</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>12</td>\n",
       "      <td>0.094167</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2392</td>\n",
       "      <td>280.583333</td>\n",
       "      <td>281.0</td>\n",
       "      <td>282</td>\n",
       "      <td>279</td>\n",
       "      <td>12</td>\n",
       "      <td>31.168031</td>\n",
       "      <td>30.981619</td>\n",
       "      <td>32.883018</td>\n",
       "      <td>29.527608</td>\n",
       "      <td>12</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>12</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DATETIME  ALTITUDE  DIRECTION_MEAN  DIRECTION_MEDIAN  DIRECTION_MAX  \\\n",
       "0 2018-01-01      1795      281.333333             281.0            288   \n",
       "1 2018-01-01      1944      281.166667             281.0            284   \n",
       "2 2018-01-01      2093      283.416667             283.5            287   \n",
       "3 2018-01-01      2243      281.833333             282.0            286   \n",
       "4 2018-01-01      2392      280.583333             281.0            282   \n",
       "\n",
       "   DIRECTION_MIN  DIRECTION_COUNT  SPEED_MEAN  SPEED_MEDIAN  SPEED_MAX  \\\n",
       "0            279               12   29.154785     29.080220  32.435630   \n",
       "1            280               12   31.317160     31.317160  32.883018   \n",
       "2            282               12   29.639455     29.751302  31.093466   \n",
       "3            280               12   30.944337     31.093466  32.435630   \n",
       "4            279               12   31.168031     30.981619  32.883018   \n",
       "\n",
       "   SPEED_MIN  SPEED_COUNT  SHEAR_MEAN  SHEAR_MEDIAN  SHEAR_MAX  SHEAR_MIN  \\\n",
       "0  26.395892           12    0.000000         0.000      0.000      0.000   \n",
       "1  29.527608           12    0.006750         0.007      0.011      0.002   \n",
       "2  27.066974           12    0.006333         0.006      0.009      0.004   \n",
       "3  29.080220           12    0.004750         0.005      0.007      0.002   \n",
       "4  29.527608           12    0.003250         0.003      0.005      0.002   \n",
       "\n",
       "   SHEAR_COUNT  VERTICAL_VELOCITY_MEAN  VERTICAL_VELOCITY_MEDIAN  \\\n",
       "0           12                0.073333                     0.090   \n",
       "1           12                0.059167                     0.075   \n",
       "2           12                0.096667                     0.110   \n",
       "3           12                0.094167                     0.100   \n",
       "4           12                0.067500                     0.080   \n",
       "\n",
       "   VERTICAL_VELOCITY_MAX  VERTICAL_VELOCITY_MIN  VERTICAL_VELOCITY_COUNT  \n",
       "0                   0.17                  -0.10                       12  \n",
       "1                   0.16                  -0.14                       12  \n",
       "2                   0.17                  -0.09                       12  \n",
       "3                   0.18                  -0.12                       12  \n",
       "4                   0.17                  -0.13                       12  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_profiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load launch violations data by:\n",
    "# - dropping image columns\n",
    "launch_violations = pd.read_csv(launch_violations_file).rename(columns=lambda x: x.upper())\n",
    "img_cols = ['GOES_IMG_PATH', 'SHEAR_IMG_PATH', 'WARNIGN_IMG_PATH', 'SBCAPE_CIN_IMG_PATH']\n",
    "launch_violations = launch_violations.drop(columns=img_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert binary columns to 1/0 values and fill missing values in PAYLOAD/REMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to binary (1 for 1.0, 0 for NaN)\n",
    "binary_columns = ['COUNTDOWN', 'LAUNCHED', 'NON_WX_SCRUB', 'WX_SCRUB', \n",
    "                 'NON_WX_DELAY', 'WX DELAY', 'LCC_SCRUB_DELAY', 'USER_WX_SCRUB_DELAY']\n",
    "\n",
    "launches[binary_columns] = launches[binary_columns].fillna(0).astype(int)\n",
    "\n",
    "# Fill NaN values in PAYLOAD and REMARKS columns\n",
    "launches['PAYLOAD'] = launches['PAYLOAD'].fillna('N/A')\n",
    "launches['REMARKS'] = launches['REMARKS'].fillna('NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns from launch_violations where the column names contain “DURATION”, “COUNT”, or “VIOLATED”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_violations = launch_violations.drop(columns=[col for col in launch_violations.columns if 'DURATION' in col or 'COUNT' in col or 'VIOLATED' in col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify launch notes into categories like “clouds”, “wind”, “lightning”, etc., based on keyword matching. If no match is found, it checks for “delay” or “scrub” and returns “other”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_launch_notes(notes):\n",
    "    \"\"\"\n",
    "    Categorizes launch notes into simplified explanations.\n",
    "\n",
    "    Args:\n",
    "        notes: A string containing the launch note.\n",
    "\n",
    "    Returns:\n",
    "        A string representing the category of the note.\n",
    "    \"\"\"\n",
    "    # Special case for \"NONE\"\n",
    "    if notes == \"NONE\":\n",
    "        return \"none\"\n",
    "\n",
    "    # Define categories with their keywords, ordered by priority\n",
    "    categories = {\n",
    "        \"clouds\": [\n",
    "            \"cumulus\", \"Cu Cloud\", \"Cu cloud\", \"cloud\",\n",
    "            \"anvil\", \"Anvil\", \"Detached Anvil\", \"attached anvil\", \n",
    "            \"Thickcloud\", \"Thick Cloud\", \"CLOUDS\", \"clouds\", \"CLDS\",\n",
    "            \"CLD\"\n",
    "        ],\n",
    "        \"wind\": [\n",
    "            \"wind\", \"Wind\", \"Wx\", \"WX\",\n",
    "            \"upper level wind\", \"Upper Level Wind\",\n",
    "            \"ground wind\", \"winds\", \"LOADS\"\n",
    "        ],\n",
    "        \"lightning\": [\n",
    "            \"lightning\", \"field mill\", \"Field Mill\", \n",
    "            \"Surface Electric Fields\", \"LLCC\",\n",
    "            \"electric field\", \"LTG\", \"THUNDERSTORM\"\n",
    "        ],\n",
    "        \"range\": [\n",
    "            \"range\", \"Range\", \"airspace\", \"restricted area\", \n",
    "            \"radar\", \"FTS\", \"helicopter\", \"cruise ship\",\n",
    "            \"tracking\", \"recovery\"\n",
    "        ],\n",
    "        \"vehicle\": [\n",
    "            \"Falcon\", \"vehicle\", \"Vehicle\", \"engine\", \"leak\",\n",
    "            \"valve\", \"hydrogen\", \"helium\", \"propellant\", \"LV\",\n",
    "            \"stage\", \"tank\", \"cryo\", \"H2\", \"instrumentation\",\n",
    "            \"hardware\"\n",
    "        ],\n",
    "        \"ground_systems\": [\n",
    "            \"Ground system\", \"ground\", \"ULA equip\", \n",
    "            \"user anomaly\", \"comms\", \"camera\", \n",
    "            \"mission assurance\", \"mission readiness\",\n",
    "            \"customer ground systems\", \"ground systems\",\n",
    "            \"data review\"\n",
    "        ],\n",
    "        \"weather_other\": [\n",
    "            \"weather\", \"Weather\", \"rain\", \"shower\", \"icing\"\n",
    "        ],\n",
    "        \"payload\": [\n",
    "            \"payload\", \"Payload\", \"user\", \"customer\",\n",
    "            \"spacecraft\", \"S/C\"\n",
    "        ],\n",
    "        \"drone\": [\"Droneship\", \"drone\", \"recovery ship\"],\n",
    "        \"abort\": [\"abort\", \"Abort\"],\n",
    "        \"other\": [\"Technical issue\", \"Tracking issues\", \"Shift of T-0\"]\n",
    "    }\n",
    "\n",
    "    # Convert note to lowercase for case-insensitive matching\n",
    "    note_lower = notes.lower()\n",
    "\n",
    "    # First check for exact phrases that might need special handling\n",
    "    if \"upper level wind\" in note_lower:\n",
    "        return \"wind\"\n",
    "    if \"second stage\" in note_lower or \"2nd stage\" in note_lower:\n",
    "        return \"vehicle\"\n",
    "    if \"t-0\" in note_lower and \"delay\" in note_lower:\n",
    "        # Look for specific reason after \"due to\"\n",
    "        if \"due to\" in note_lower:\n",
    "            reason = note_lower.split(\"due to\")[1].strip()\n",
    "            # Recursively categorize the reason\n",
    "            return categorize_launch_notes(reason)\n",
    "        return \"other\"\n",
    "\n",
    "    # Check each category's keywords\n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(rf\"(?<!\\w){re.escape(keyword.lower())}(?!\\w)\", note_lower):\n",
    "                return category\n",
    "\n",
    "    # If no category found, check for delay/scrub\n",
    "    if re.search(r\"delay|scrub\", note_lower):\n",
    "        return \"other\"\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "launches['REMARKS_CATEGORY'] = launches['REMARKS'].apply(categorize_launch_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REMARKS_CATEGORY\n",
       "none              403\n",
       "unknown           146\n",
       "wind               81\n",
       "vehicle            53\n",
       "clouds             47\n",
       "range              31\n",
       "lightning          22\n",
       "other              15\n",
       "payload            11\n",
       "ground_systems      9\n",
       "abort               7\n",
       "weather_other       4\n",
       "drone               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "launches['REMARKS_CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts wind speeds from meters per second (m/s) to miles per hour (mph) for SPEED_MEAN, SPEED_MEDIAN, SPEED_MAX, and SPEED_MIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert wind speeds from m/s to mph\n",
    "wind_profiles['SPEED_MEAN'] = wind_profiles['SPEED_MEAN'] * 2.23694\n",
    "wind_profiles['SPEED_MEDIAN'] = wind_profiles['SPEED_MEDIAN'] * 2.23694\n",
    "wind_profiles['SPEED_MAX'] = wind_profiles['SPEED_MAX'] * 2.23694\n",
    "wind_profiles['SPEED_MIN'] = wind_profiles['SPEED_MIN'] * 2.23694\n",
    "\n",
    "# Convert wind shear (/sec) to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin wind profile data into atmospheric layers (e.g., boundary layer, troposphere, stratosphere) based on altitude, aggregates measurements (e.g., direction, speed, shear) by hour, and renames columns for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/cmd4ttnn79d91jcrl066qh4r0000gn/T/ipykernel_34097/3267685395.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  pd.Grouper(key='DATETIME', freq='H'),\n",
      "/var/folders/cl/cmd4ttnn79d91jcrl066qh4r0000gn/T/ipykernel_34097/3267685395.py:23: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grouped = df.groupby([\n"
     ]
    }
   ],
   "source": [
    "def bin_wind_profiles_by_layer(df):\n",
    "    \"\"\"\n",
    "    Bin wind profile data into atmospheric layers and aggregate measurements hourly.\n",
    "    \"\"\"\n",
    "    bins = [0, 1000, 3000, 5000, 7000, 9000, 10000, 12000, 15000, 16000, 18000, float('inf')]\n",
    "    labels = ['surface_layer_1000hpa', 'boundary_layer_900hpa', 'low_troposphere_700hpa', 'mid_low_troposphere_500hpa', 'mid_troposphere_400hpa',\n",
    "              'high_mid_troposphere_300hpa', 'upper_troposphere_200hpa', 'lower_stratosphere_150hpa', 'mid_stratosphere_100hpa',\n",
    "              'upper_stratosphere_50hpa', 'top_stratosphere_10hpa']\n",
    "\n",
    "    df = df.copy()\n",
    "    df['ATMOSPHERIC_LAYER'] = pd.cut(df['ALTITUDE'], bins=bins, labels=labels)\n",
    "\n",
    "    # Restructure measurements to avoid redundant names\n",
    "    measurement_cols = {\n",
    "        # 'DIRECTION_MEAN': ['mean', 'max'],\n",
    "        'SPEED_MEAN': ['mean', 'max'],\n",
    "        'SHEAR_MEAN': ['mean', 'max'],\n",
    "        # 'DIRECTION_MAX': ['mean', 'max'],\n",
    "        'SPEED_MAX': ['mean', 'max'],\n",
    "        'SHEAR_MAX': ['mean', 'max'],\n",
    "    }\n",
    "\n",
    "    grouped = df.groupby([\n",
    "        pd.Grouper(key='DATETIME', freq='H'),\n",
    "        'ATMOSPHERIC_LAYER'\n",
    "    ]).agg(measurement_cols)\n",
    "\n",
    "    # Rename columns to be more concise\n",
    "    new_names = {\n",
    "        # 'DIRECTION_MEAN_mean': 'DIRECTION_AVG',\n",
    "        # 'DIRECTION_MEAN_max': 'DIRECTION_AVG_PEAK',\n",
    "        'SPEED_MEAN_mean': 'SPEED_AVG',\n",
    "        'SPEED_MEAN_max': 'SPEED_AVG_PEAK',\n",
    "        'SHEAR_MEAN_mean': 'SHEAR_AVG',\n",
    "        'SHEAR_MEAN_max': 'SHEAR_AVG_PEAK',\n",
    "        # 'DIRECTION_MAX_mean': 'DIRECTION_PEAK_AVG',\n",
    "        # 'DIRECTION_MAX_max': 'DIRECTION_MAX_PEAK',\n",
    "        'SPEED_MAX_mean': 'SPEED_MAX_AVG',\n",
    "        'SPEED_MAX_max': 'SPEED_MAX_PEAK',\n",
    "        'SHEAR_MAX_mean': 'SHEAR_MAX_AVG',\n",
    "        'SHEAR_MAX_max': 'SHEAR_MAX_PEAK',\n",
    "    }\n",
    "\n",
    "    # Flatten and rename columns\n",
    "    grouped.columns = [f'{col[0]}_{col[1]}' for col in grouped.columns]\n",
    "    grouped = grouped.reset_index()\n",
    "    grouped = grouped.rename(columns=new_names)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# Bin wind profiles by atmospheric layer\n",
    "binned_wind_profiles = bin_wind_profiles_by_layer(wind_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert DATE columns to datetime for consistent merging, filters data from 2018 onwards, merges launches and launch_violations on DATE, reorders columns, renames and drops specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert launch_violations DATE to datetime for consistent merging\n",
    "launch_violations['DATE'] = pd.to_datetime(launch_violations['DATE'])\n",
    "\n",
    "# Convert launches DATE to datetime.date to match launch_violations\n",
    "launches['DATE'] = launches['DATE'].dt.date\n",
    "launch_violations['DATE'] = launch_violations['DATE'].dt.date\n",
    "\n",
    "# Filter data from 2018 onwards\n",
    "launches = launches[launches['DATE'] >= pd.to_datetime('2018-01-01').date()]\n",
    "launch_violations = launch_violations[launch_violations['DATE'] >= pd.to_datetime('2018-01-01').date()]\n",
    "\n",
    "# Merge launches and launch_violations datasets on date\n",
    "# Reorder columns to keep START_LCC_EVAL and END_LCC_EVAL next to DATE\n",
    "merged_df = pd.merge(\n",
    "    launches,\n",
    "    launch_violations,\n",
    "    on='DATE',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Reorder columns to put evaluation times next to DATE\n",
    "date_cols = ['DATE', 'START_LCC_EVAL', 'END_LCC_EVAL']\n",
    "other_cols = [col for col in merged_df.columns if col not in date_cols]\n",
    "merged_df = merged_df[date_cols + other_cols]\n",
    "\n",
    "# Rename 'LAUNCH_VEHICLE_x' and 'PAYLOAD_x' to 'LAUNCH_VEHICLE' and 'PAYLOAD'\n",
    "merged_df = merged_df.rename(columns={'LAUNCH_VEHICLE_x': 'LAUNCH_VEHICLE', 'PAYLOAD_x': 'PAYLOAD'})\n",
    "\n",
    "# Drop the columns 'LAUNCH_VEHICLE_y' and 'PAYLOAD_y' from the merged dataframe\n",
    "merged_df = merged_df.drop(columns=['LAUNCH_VEHICLE_y', 'PAYLOAD_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter merged_df to keep rows with indexes specified in indexes_to_keep and all non-indexes_to_keep rows that are unique by the ‘DATE’ column, removing unwanted duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only specific indexes from duplicates by:\n",
    "# - Keeping the indexes that are in the indexes_to_keep list\n",
    "# - Keeping all duplicates that are not in the indexes_to_keep list\n",
    "indexes_to_keep = [181, 184, 209, 212, 229, 232]\n",
    "merged_df = merged_df.loc[merged_df.index.isin(indexes_to_keep) | ~merged_df.duplicated(subset=['DATE'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../data/transformed/launch/cape_canaveral_launches.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process weather data by converting relevant columns to datetime, filtering weather records for each launch’s 8-hour window leading up to the launch, and padding any missing data with zeros if necessary. For each launch, the function compiles weather features into arrays. These arrays are then organized into a dataframe, which can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_arrays(merged_df, weather_df):\n",
    "    '''\n",
    "    Create a dataframe of weather arrays for each launch by:\n",
    "    - Converting END_LCC_EVAL and TIME to datetime\n",
    "    - Converting weather TIME from UTC to Eastern time\n",
    "    - Looping through each launch and appending the weather data for the 8 hours before the launch to the lists\n",
    "    - Creating a dataframe from the lists\n",
    "    '''\n",
    "    # Convert END_LCC_EVAL to datetime if it's not already\n",
    "    merged_df['END_LCC_EVAL'] = pd.to_datetime(merged_df['END_LCC_EVAL'])\n",
    "    merged_df['START_LCC_EVAL'] = pd.to_datetime(merged_df['START_LCC_EVAL'])\n",
    "    merged_df['DATE'] = pd.to_datetime(merged_df['DATE'])\n",
    "    weather_df['TIME'] = pd.to_datetime(weather_df['TIME'])\n",
    "\n",
    "    # Initialize lists to store our results\n",
    "    all_dates = []\n",
    "    all_start_times = []\n",
    "    all_end_times = []\n",
    "    all_temps = []\n",
    "    all_cloud_cover = []\n",
    "    all_wind_speed_10m = []\n",
    "    all_wind_speed_100m = []\n",
    "    all_wind_dir_10m = []\n",
    "    all_wind_dir_100m = []\n",
    "    all_wind_gusts = []\n",
    "    all_pressure = []\n",
    "    all_rain = []\n",
    "    all_humidity = []\n",
    "    all_weather_code = []\n",
    "\n",
    "    # For each launch\n",
    "    for _, row in merged_df.iterrows():\n",
    "        end_time = row['END_LCC_EVAL']\n",
    "        if pd.isna(end_time):\n",
    "            continue\n",
    "\n",
    "        # Get the 8 hours before end time\n",
    "        start_time = end_time - pd.Timedelta(hours=8)\n",
    "\n",
    "        # Filter weather data for this time window\n",
    "        mask = (weather_df['TIME'] >= start_time) & (weather_df['TIME'] <= end_time)\n",
    "        relevant_weather = weather_df[mask].sort_values('TIME', ascending=False)\n",
    "\n",
    "        # Pad with zeros if less than 8 hours\n",
    "        pad_length = 8 - len(relevant_weather)\n",
    "\n",
    "        temps = relevant_weather['TEMPERATURE_2M (°F)'].tolist() + [0] * pad_length\n",
    "        cloud = relevant_weather['CLOUD_COVER (%)'].tolist() + [0] * pad_length\n",
    "        wind_10m = relevant_weather['WIND_SPEED_10M (MP/H)'].tolist() + [0] * pad_length\n",
    "        wind_100m = relevant_weather['WIND_SPEED_100M (MP/H)'].tolist() + [0] * pad_length\n",
    "        wind_dir_10m = relevant_weather['WIND_DIRECTION_10M (°)'].tolist() + [0] * pad_length\n",
    "        wind_dir_100m = relevant_weather['WIND_DIRECTION_100M (°)'].tolist() + [0] * pad_length\n",
    "        gusts = relevant_weather['WIND_GUSTS_10M (MP/H)'].tolist() + [0] * pad_length\n",
    "        pressure = relevant_weather['PRESSURE_MSL (HPA)'].tolist() + [0] * pad_length\n",
    "        rain = relevant_weather['RAIN (INCH)'].tolist() + [0] * pad_length\n",
    "        humidity = relevant_weather['RELATIVE_HUMIDITY_2M (%)'].tolist() + [0] * pad_length\n",
    "        weather_code = relevant_weather['WEATHER_CODE (WMO CODE)'].tolist() + [0] * pad_length\n",
    "\n",
    "        # Append to our lists\n",
    "        all_dates.append(row['DATE'])\n",
    "        all_start_times.append(row['START_LCC_EVAL'])\n",
    "        all_end_times.append(row['END_LCC_EVAL'])\n",
    "        all_temps.append(temps[:8])  # Ensure we only take first 8 values\n",
    "        all_cloud_cover.append(cloud[:8])\n",
    "        all_wind_speed_10m.append(wind_10m[:8])\n",
    "        all_wind_speed_100m.append(wind_100m[:8])\n",
    "        all_wind_dir_10m.append(wind_dir_10m[:8])\n",
    "        all_wind_dir_100m.append(wind_dir_100m[:8])\n",
    "        all_wind_gusts.append(gusts[:8])\n",
    "        all_pressure.append(pressure[:8])\n",
    "        all_rain.append(rain[:8])\n",
    "        all_humidity.append(humidity[:8])\n",
    "        all_weather_code.append(weather_code[:8])\n",
    "\n",
    "    # Create the final dataframe\n",
    "    weather_arrays_df = pd.DataFrame({\n",
    "        'DATE': all_dates,\n",
    "        'START_LCC_EVAL': all_start_times,\n",
    "        'END_LCC_EVAL': all_end_times,\n",
    "        'TEMPERATURE': all_temps,\n",
    "        'CLOUD_COVER': all_cloud_cover,\n",
    "        'WIND_SPEED_10M': all_wind_speed_10m,\n",
    "        'WIND_SPEED_100M': all_wind_speed_100m,\n",
    "        'WIND_DIRECTION_10M': all_wind_dir_10m,\n",
    "        'WIND_DIRECTION_100M': all_wind_dir_100m,\n",
    "        'WIND_GUSTS': all_wind_gusts,\n",
    "        'PRESSURE': all_pressure,\n",
    "        'RAIN': all_rain,\n",
    "        'HUMIDITY': all_humidity,\n",
    "        'WEATHER_CODE': all_weather_code\n",
    "    })\n",
    "\n",
    "    return weather_arrays_df\n",
    "\n",
    "# Create the weather arrays dataframe\n",
    "weather_arrays = create_weather_arrays(merged_df, weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_field_mill_arrays(merged_df, field_mill_df):\n",
    "    '''\n",
    "    Create a dataframe of field mill arrays for each launch by:\n",
    "    - Converting END_LCC_EVAL and DATETIME to datetime\n",
    "    - Aggregating field mill data across sensors for each hour\n",
    "    - Looping through each launch and appending the field mill data for the 8 hours before the launch\n",
    "    - Creating a dataframe from the lists\n",
    "    '''\n",
    "    # Convert END_LCC_EVAL to datetime if it's not already\n",
    "    merged_df['END_LCC_EVAL'] = pd.to_datetime(merged_df['END_LCC_EVAL'])\n",
    "    field_mill_df['DATETIME'] = pd.to_datetime(field_mill_df['DATETIME'])\n",
    "\n",
    "    # Initialize lists to store our results\n",
    "    all_dates = []\n",
    "    all_start_times = []\n",
    "    all_end_times = []\n",
    "    all_mean_values = []\n",
    "    all_median_values = []\n",
    "    all_max_values = []\n",
    "    all_min_values = []\n",
    "\n",
    "    # For each launch\n",
    "    for _, row in merged_df.iterrows():\n",
    "        end_time = row['END_LCC_EVAL']\n",
    "        if pd.isna(end_time):\n",
    "            continue\n",
    "\n",
    "        # Get the 8 hours before end time\n",
    "        start_time = end_time - pd.Timedelta(hours=8)\n",
    "\n",
    "        # Filter field mill data for this time window\n",
    "        mask = (field_mill_df['DATETIME'] >= start_time) & (field_mill_df['DATETIME'] <= end_time)\n",
    "        relevant_data = field_mill_df[mask].copy()\n",
    "\n",
    "        # Group by datetime and aggregate across sensors\n",
    "        hourly_stats = relevant_data.groupby('DATETIME').agg({\n",
    "            'MEAN': 'mean',\n",
    "            'MEDIAN': 'mean',\n",
    "            'MAX': 'max',\n",
    "            'MIN': 'min'\n",
    "        }).sort_values('DATETIME', ascending=False)\n",
    "\n",
    "        # Pad with zeros if less than 8 hours\n",
    "        pad_length = 8 - len(hourly_stats)\n",
    "\n",
    "        means = hourly_stats['MEAN'].tolist() + [0] * pad_length\n",
    "        medians = hourly_stats['MEDIAN'].tolist() + [0] * pad_length\n",
    "        maxes = hourly_stats['MAX'].tolist() + [0] * pad_length\n",
    "        mins = hourly_stats['MIN'].tolist() + [0] * pad_length\n",
    "\n",
    "        # Append to our lists\n",
    "        all_dates.append(row['DATE'])\n",
    "        all_start_times.append(row['START_LCC_EVAL'])\n",
    "        all_end_times.append(row['END_LCC_EVAL'])\n",
    "        all_mean_values.append(means[:8])  # Ensure we only take first 8 values\n",
    "        all_median_values.append(medians[:8])\n",
    "        all_max_values.append(maxes[:8])\n",
    "        all_min_values.append(mins[:8])\n",
    "\n",
    "    # Create the final dataframe\n",
    "    field_mill_arrays_df = pd.DataFrame({\n",
    "        'DATE': all_dates,\n",
    "        'START_LCC_EVAL': all_start_times,\n",
    "        'END_LCC_EVAL': all_end_times,\n",
    "        'FIELD_MILL_MEAN': all_mean_values,\n",
    "        'FIELD_MILL_MEDIAN': all_median_values,\n",
    "        'FIELD_MILL_MAX': all_max_values,\n",
    "        'FIELD_MILL_MIN': all_min_values\n",
    "    })\n",
    "\n",
    "    return field_mill_arrays_df\n",
    "\n",
    "# Create the field mill arrays dataframe\n",
    "field_mill_arrays = create_field_mill_arrays(merged_df, field_mill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wind_profiler_arrays(merged_df, binned_wind_profiles_df):\n",
    "    '''\n",
    "    Create a dataframe of wind profiler arrays for each launch by:\n",
    "    - Converting END_LCC_EVAL and DATETIME to datetime\n",
    "    - Creating separate arrays for each atmospheric layer\n",
    "    - Looping through each launch and appending the wind profiler data for the 8 hours before launch\n",
    "    - Creating a dataframe from the lists\n",
    "    '''\n",
    "    # Convert END_LCC_EVAL to datetime if it's not already\n",
    "    merged_df['END_LCC_EVAL'] = pd.to_datetime(merged_df['END_LCC_EVAL'])\n",
    "    binned_wind_profiles_df['DATETIME'] = pd.to_datetime(binned_wind_profiles_df['DATETIME'])\n",
    "\n",
    "    # Initialize lists for dates and each atmospheric layer's measurements\n",
    "    all_dates = []\n",
    "    all_start_times = []\n",
    "    all_end_times = []\n",
    "    layers = ['surface_layer_1000hpa', 'boundary_layer_900hpa', 'low_troposphere_700hpa', 'mid_low_troposphere_500hpa', 'mid_troposphere_400hpa',\n",
    "            'high_mid_troposphere_300hpa', 'upper_troposphere_200hpa', 'lower_stratosphere_150hpa', 'mid_stratosphere_100hpa',\n",
    "            'upper_stratosphere_50hpa', 'top_stratosphere_10hpa'\n",
    "            ]\n",
    "\n",
    "    # Dictionary to store arrays for each measurement type and layer\n",
    "    measurements = {\n",
    "        'SPEED_AVG': {layer: [] for layer in layers},\n",
    "        'SPEED_MAX_PEAK': {layer: [] for layer in layers},\n",
    "        'SHEAR_AVG': {layer: [] for layer in layers},\n",
    "        'SHEAR_MAX_PEAK': {layer: [] for layer in layers}\n",
    "    }\n",
    "\n",
    "    # For each launch\n",
    "    for _, row in merged_df.iterrows():\n",
    "        end_time = row['END_LCC_EVAL']\n",
    "        if pd.isna(end_time):\n",
    "            continue\n",
    "\n",
    "        # Get the 8 hours before end time\n",
    "        start_time = end_time - pd.Timedelta(hours=8)\n",
    "\n",
    "        # Filter wind profiler data for this time window\n",
    "        mask = (binned_wind_profiles_df['DATETIME'] >= start_time) & (binned_wind_profiles_df['DATETIME'] <= end_time)\n",
    "        relevant_data = binned_wind_profiles_df[mask].copy()\n",
    "\n",
    "        # Add date and times to our lists\n",
    "        all_dates.append(row['DATE'])\n",
    "        all_start_times.append(row['START_LCC_EVAL'])\n",
    "        all_end_times.append(row['END_LCC_EVAL'])\n",
    "\n",
    "        # Process each atmospheric layer\n",
    "        for layer in layers:\n",
    "            layer_data = relevant_data[relevant_data['ATMOSPHERIC_LAYER'] == layer].sort_values('DATETIME', ascending=False)\n",
    "\n",
    "            # Pad with zeros if less than 8 hours\n",
    "            pad_length = 8 - len(layer_data)\n",
    "\n",
    "            # Process each measurement type for this layer\n",
    "            for measure in measurements.keys():\n",
    "                values = layer_data[measure].tolist() + [0] * pad_length\n",
    "                measurements[measure][layer].append(values[:8])  # Ensure we only take first 8 values\n",
    "\n",
    "    # Create the final dataframe\n",
    "    wind_profiler_arrays_df = pd.DataFrame({\n",
    "        'DATE': all_dates,\n",
    "        'START_LCC_EVAL': all_start_times,\n",
    "        'END_LCC_EVAL': all_end_times\n",
    "    })\n",
    "\n",
    "    # Add columns for each measurement and layer combination\n",
    "    for measure in measurements.keys():\n",
    "        for layer in layers:\n",
    "            col_name = f'WIND_{measure}_{layer.upper()}'\n",
    "            wind_profiler_arrays_df[col_name] = measurements[measure][layer]\n",
    "\n",
    "    return wind_profiler_arrays_df\n",
    "\n",
    "# Create the wind profiler arrays dataframe\n",
    "wind_profiler_arrays = create_wind_profiler_arrays(merged_df, binned_wind_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | DATE                | START_LCC_EVAL      | END_LCC_EVAL        | WIND_SPEED_AVG_SURFACE_LAYER_1000HPA     | WIND_SPEED_AVG_BOUNDARY_LAYER_900HPA                                                                                                                           | WIND_SPEED_AVG_LOW_TROPOSPHERE_700HPA                                                                                                                           | WIND_SPEED_AVG_MID_LOW_TROPOSPHERE_500HPA                                                                                                                       | WIND_SPEED_AVG_MID_TROPOSPHERE_400HPA                                                                                                                       | WIND_SPEED_AVG_HIGH_MID_TROPOSPHERE_300HPA                                                                                                                     | WIND_SPEED_AVG_UPPER_TROPOSPHERE_200HPA                                                                                                                          | WIND_SPEED_AVG_LOWER_STRATOSPHERE_150HPA                                                                                                                  | WIND_SPEED_AVG_MID_STRATOSPHERE_100HPA                                                                                                                    | WIND_SPEED_AVG_UPPER_STRATOSPHERE_50HPA                                                                                                                     | WIND_SPEED_AVG_TOP_STRATOSPHERE_10HPA                                                                                                                          | WIND_SPEED_MAX_PEAK_SURFACE_LAYER_1000HPA   | WIND_SPEED_MAX_PEAK_BOUNDARY_LAYER_900HPA                                                                                          | WIND_SPEED_MAX_PEAK_LOW_TROPOSPHERE_700HPA                                                                                   | WIND_SPEED_MAX_PEAK_MID_LOW_TROPOSPHERE_500HPA                                                                                             | WIND_SPEED_MAX_PEAK_MID_TROPOSPHERE_400HPA                                                                                                       | WIND_SPEED_MAX_PEAK_HIGH_MID_TROPOSPHERE_300HPA                                                                                                          | WIND_SPEED_MAX_PEAK_UPPER_TROPOSPHERE_200HPA                                                                                                             | WIND_SPEED_MAX_PEAK_LOWER_STRATOSPHERE_150HPA                                                                                                    | WIND_SPEED_MAX_PEAK_MID_STRATOSPHERE_100HPA                                                                                                  | WIND_SPEED_MAX_PEAK_UPPER_STRATOSPHERE_50HPA                                                                             | WIND_SPEED_MAX_PEAK_TOP_STRATOSPHERE_10HPA                                                                                           | WIND_SHEAR_AVG_SURFACE_LAYER_1000HPA     | WIND_SHEAR_AVG_BOUNDARY_LAYER_900HPA                                                                                                                                               | WIND_SHEAR_AVG_LOW_TROPOSPHERE_700HPA                                                                                                                                             | WIND_SHEAR_AVG_MID_LOW_TROPOSPHERE_500HPA                                                                                                                                           | WIND_SHEAR_AVG_MID_TROPOSPHERE_400HPA                                                                                                                                             | WIND_SHEAR_AVG_HIGH_MID_TROPOSPHERE_300HPA                                                                                                                                       | WIND_SHEAR_AVG_UPPER_TROPOSPHERE_200HPA                                                                                                                                          | WIND_SHEAR_AVG_LOWER_STRATOSPHERE_150HPA                                                                                                                                         | WIND_SHEAR_AVG_MID_STRATOSPHERE_100HPA                                                                                                                                           | WIND_SHEAR_AVG_UPPER_STRATOSPHERE_50HPA                                                                                                                                         | WIND_SHEAR_AVG_TOP_STRATOSPHERE_10HPA                                                                                                                                       | WIND_SHEAR_MAX_PEAK_SURFACE_LAYER_1000HPA   | WIND_SHEAR_MAX_PEAK_BOUNDARY_LAYER_900HPA                | WIND_SHEAR_MAX_PEAK_LOW_TROPOSPHERE_700HPA               | WIND_SHEAR_MAX_PEAK_MID_LOW_TROPOSPHERE_500HPA           | WIND_SHEAR_MAX_PEAK_MID_TROPOSPHERE_400HPA               | WIND_SHEAR_MAX_PEAK_HIGH_MID_TROPOSPHERE_300HPA          | WIND_SHEAR_MAX_PEAK_UPPER_TROPOSPHERE_200HPA             | WIND_SHEAR_MAX_PEAK_LOWER_STRATOSPHERE_150HPA            | WIND_SHEAR_MAX_PEAK_MID_STRATOSPHERE_100HPA              | WIND_SHEAR_MAX_PEAK_UPPER_STRATOSPHERE_50HPA             | WIND_SHEAR_MAX_PEAK_TOP_STRATOSPHERE_10HPA               |\n",
      "|---:|:--------------------|:--------------------|:--------------------|:-----------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------|\n",
      "|  0 | 2018-01-07 00:00:00 | 2018-01-07 22:00:00 | 2018-01-08 01:00:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [5.124249592592593, 4.625080574074074, 4.117626592592592, 4.11141287037037, 4.318536944444444, 2.5931934074074077, 2.5641960370370374, 2.9183782037037043]     | [9.93717576923077, 8.923383089743592, 8.279545871794872, 7.135265025641027, 8.65093526923077, 9.092587525641026, 9.512730743589744, 9.74502835897436]           | [32.837132051282055, 30.030919500000003, 28.63713380769231, 29.676737333333335, 27.471343923076923, 23.817675256410254, 22.60313155128205, 20.792070512820512]  | [54.07536147619048, 56.703765976190475, 55.40421035714286, 53.26580223809525, 53.420257619047625, 55.22711927380953, 50.122102630952384, 46.88652870238095] | [66.91557461111113, 67.35364202777778, 70.81157844444445, 72.57627555555557, 76.96316344444445, 80.25954308333334, 67.8880221388889, 63.6098743888889]         | [79.12908947619049, 78.51925702380954, 71.27316923809524, 71.83107272619048, 75.14520585714287, 84.89320451190477, 75.78566308333333, 54.097997178571426]        | [79.82706801666667, 77.48853365833334, 74.1769304, 73.972809625, 75.10060020833335, 73.69319212500001, 70.9557368, 71.89525160000001]                     | [60.250913690476196, 60.32814138095238, 61.94725985714286, 65.03104142857144, 64.43985014285714, 65.66750411904762, 67.69672826190477, 68.61280845238096] | [51.01657135897437, 51.848254179487185, 48.140095948717956, 48.09851180769231, 49.22701935897437, 50.24941565384616, 49.40626134615385, 47.725688474358975] | [35.090132133333334, 36.15827098333334, 34.310931366666665, 33.787114583333334, 33.5093612, 35.1982509, 35.783583533333335, 36.0371034]                        | [nan, nan, nan, nan, nan, nan, nan, nan]    | [9.618842, 8.500372, 7.381902, 8.276678, 10.513618000000001, 5.3686560000000005, 4.47388, 5.3686560000000005]                      | [16.329662, 15.882274, 16.777050000000003, 17.000744, 18.566602000000003, 13.197946000000002, 13.42164, 13.869028000000002]  | [49.883762000000004, 47.64682200000001, 42.278166, 39.14645, 38.922756, 37.356898, 37.580592, 34.001488]                                   | [63.976484000000006, 69.121446, 71.58208, 70.01622200000001, 70.01622200000001, 74.26640800000001, 70.01622200000001, 60.844768]                 | [71.80577400000001, 72.476856, 74.26640800000001, 80.082452, 80.753534, 90.148682, 81.872004, 68.89775200000001]                                         | [90.372376, 93.05670400000001, 80.52984000000001, 78.740288, 87.46435400000001, 100.43860600000001, 100.214912, 70.23991600000001]                       | [95.29364400000001, 98.87274800000002, 92.609316, 92.16192800000002, 95.51733800000001, 94.39886800000001, 86.12219, 91.490846]                  | [69.34514, 65.094954, 69.34514, 72.253162, 71.358386, 75.608572, 78.2929, 76.05596]                                                          | [78.51659400000001, 78.06920600000001, 71.80577400000001, 70.01622200000001, 72.476856, 75.608572, 77.845512, 72.476856] | [47.64682200000001, 51.673314000000005, 48.094210000000004, 48.988986, 48.765292, 47.199434000000004, 46.752046, 49.883762000000004] | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.0061759259259258886, 0.006009259259259245, 0.005009259259259233, 0.004999999999999966, 0.0049537037037036885, 0.003916666666666633, 0.004194444444444422, 0.003861111111111089] | [0.0047692307692307305, 0.0049807692307692, 0.004999999999999962, 0.005455128205128154, 0.005019230769230746, 0.005192307692307654, 0.005621794871794839, 0.004878205128205092]   | [0.008948717948717922, 0.008666666666666623, 0.00812179487179484, 0.008871794871794838, 0.0081858974358974, 0.007564102564102523, 0.0071730769230768845, 0.005602564102564062]      | [0.005970238095238057, 0.007648809523809479, 0.009249999999999965, 0.00888690476190473, 0.011071428571428536, 0.01289880952380947, 0.007982142857142821, 0.008982142857142822]    | [0.004958333333333283, 0.0048333333333333, 0.0056666666666666, 0.006291666666666633, 0.007749999999999966, 0.013208333333333301, 0.012972222222222184, 0.006777777777777733]     | [0.006541666666666614, 0.006553571428571386, 0.008428571428571379, 0.006452380952380922, 0.006303571428571393, 0.01321428571428567, 0.01719047619047615, 0.014232142857142822]   | [0.018612499999999966, 0.017599999999999966, 0.016324999999999958, 0.018329166666666615, 0.019937499999999976, 0.018987499999999966, 0.01439166666666664, 0.015374999999999962]  | [0.014190476190476144, 0.016869047619047586, 0.0172023809523809, 0.016154761904761887, 0.017464285714285672, 0.016892857142857088, 0.02028571428571423, 0.020309523809523784]    | [0.022134615384615346, 0.018967948717948685, 0.018570512820512784, 0.021326923076923045, 0.020891025641025594, 0.016698717948717908, 0.01991666666666663, 0.018653846153846125] | [0.020774999999999967, 0.017558333333333308, 0.01527499999999996, 0.01519999999999995, 0.01855833333333328, 0.01835833333333329, 0.02147499999999996, 0.018491666666666632] | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.02, 0.017, 0.016, 0.022, 0.021, 0.011, 0.017, 0.014]  | [0.013, 0.012, 0.014, 0.018, 0.016, 0.024, 0.025, 0.016] | [0.019, 0.019, 0.02, 0.029, 0.018, 0.017, 0.023, 0.026]  | [0.014, 0.023, 0.018, 0.018, 0.024, 0.023, 0.02, 0.021]  | [0.011, 0.013, 0.016, 0.013, 0.025, 0.033, 0.022, 0.017] | [0.019, 0.028, 0.02, 0.018, 0.03, 0.04, 0.052, 0.035]    | [0.063, 0.055, 0.065, 0.077, 0.069, 0.065, 0.063, 0.063] | [0.052, 0.056, 0.051, 0.039, 0.05, 0.045, 0.052, 0.061]  | [0.115, 0.091, 0.047, 0.057, 0.052, 0.049, 0.066, 0.06]  | [0.071, 0.06, 0.05, 0.045, 0.052, 0.058, 0.08, 0.076]    |\n",
      "|  1 | 2018-01-18 00:00:00 | 2018-01-18 18:52:00 | 2018-01-19 00:52:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [16.29445090740741, 18.489966092592592, 19.60222237037037, 20.79732827777778, 22.36732875925926, 22.317618981481484, 23.241392351851854, 23.270389722222223]   | [23.79473228205129, 24.20770582051282, 25.919825282051285, 27.413986487179486, 27.601832089743592, 29.164822217948718, 30.354989012820514, 32.25208620512821]   | [42.69544134615384, 43.127056051282054, 43.475502474358976, 44.20824371794872, 44.0433410897436, 44.91517411538462, 47.76870655128206, 49.52241015384615]       | [86.36186214285715, 87.65742322619049, 89.93297707142858, 92.33635605952382, 92.07404821428572, 92.46817573809525, 94.79299552380952, 96.19907209523811]    | [107.54399736111111, 108.5444066388889, 109.06635930555557, 110.37434783333333, 110.61046927777778, 113.3538276388889, 114.55307602777778, 112.53361630555555] | [131.07270039285714, 133.54132346428574, 133.59990998809525, 133.65317046428572, 134.2590083809524, 134.15914498809525, 131.27109566666667, 129.5481192619048]   | [113.19755252500002, 112.23939655833333, 112.18440511666668, 111.96443934999999, 111.95977905833334, 111.0305169, 109.61938058333335, 110.69124766666667] | [89.56547978571429, 91.7811155952381, 91.85035421428573, 93.66919947619047, 93.09398633333333, 97.06455483333335, 95.25103561904764, 90.2392248095238]    | [79.01130188461539, 77.06831874358976, 78.98549103846155, 80.12833794871796, 79.21492078205128, 79.13031856410257, 77.0984313974359, 74.35387808974359]     | [41.152239533333336, 38.864968383333334, 41.366612950000004, 42.92874271666667, 45.89455233333334, 49.268603500000005, 51.19610013333333, 50.625680433333336]  | [nan, nan, nan, nan, nan, nan, nan, nan]    | [18.566602000000003, 20.579848, 22.369400000000002, 23.264176000000003, 24.830034, 25.72481, 27.066974000000002, 26.84328]         | [32.43563, 31.540854000000003, 32.883018, 34.448876000000006, 34.225182000000004, 34.448876000000006, 37.804286, 41.159696]  | [60.173686000000004, 61.51585000000001, 64.423872, 66.437118, 63.752790000000005, 65.542342, 72.924244, 72.253162]                         | [101.10968800000002, 102.22815800000001, 104.01771000000001, 106.925732, 106.925732, 107.59681400000001, 109.38636600000001, 109.38636600000001] | [113.41285800000001, 116.54457400000001, 114.53132800000002, 116.76826800000002, 114.755022, 120.79476000000001, 120.12367800000001, 117.66304400000001] | [143.835242, 146.072182, 145.17740600000002, 145.848488, 151.217144, 154.572554, 150.546062, 161.730762]                                                 | [146.29587600000002, 135.782258, 135.782258, 139.585056, 138.91397400000002, 139.585056, 144.953712, 154.12516600000004]                         | [106.478344, 106.925732, 110.72853, 109.162672, 104.688792, 105.58356800000001, 104.01771000000001, 100.6623]                                | [116.54457400000001, 95.06995, 92.83301, 95.51733800000001, 94.39886800000001, 95.51733800000001, 83.437862, 90.819764]  | [58.83152200000001, 58.38413400000001, 63.529096, 61.963238000000004, 66.660812, 67.331894, 67.77928200000001, 69.121446]            | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.005601851851851811, 0.005296296296296267, 0.0044444444444444115, 0.0041296296296296, 0.003648148148148111, 0.0033796296296296, 0.0034351851851851553, 0.003638888888888855]     | [0.006512820512820469, 0.006429487179487131, 0.0056858974358974, 0.005467948717948685, 0.004423076923076877, 0.0049807692307692, 0.005551282051282023, 0.006384615384615338]      | [0.007512820512820469, 0.00780769230769227, 0.008051282051282, 0.008294871794871763, 0.007717948717948669, 0.007878205128205078, 0.008256410256410208, 0.008179487179487131]        | [0.012017857142857092, 0.012511904761904722, 0.013309523809523766, 0.012904761904761872, 0.014107142857142806, 0.014095238095238044, 0.011964285714285686, 0.012124999999999952]  | [0.009597222222222167, 0.00951388888888885, 0.009624999999999967, 0.008805555555555516, 0.0076111111111111, 0.011166666666666632, 0.00912499999999995, 0.0091805555555555]       | [0.01293452380952378, 0.012898809523809484, 0.012666666666666614, 0.012589285714285678, 0.0134583333333333, 0.014696428571428522, 0.013660714285714236, 0.013505952380952329]    | [0.017379166666666616, 0.015120833333333288, 0.012579166666666624, 0.013791666666666624, 0.015070833333333289, 0.015379166666666624, 0.017520833333333298, 0.019899999999999966] | [0.021107142857142817, 0.019357142857142826, 0.018999999999999986, 0.016678571428571386, 0.016309523809523784, 0.020261904761904714, 0.019095238095238058, 0.019107142857142816] | [0.025743589743589694, 0.020121794871794826, 0.0184935897435897, 0.019826923076923044, 0.020333333333333283, 0.01711538461538459, 0.013679487179487138, 0.01885256410256406]    | [0.0219083333333333, 0.01961666666666663, 0.01861666666666664, 0.01719166666666662, 0.02159999999999997, 0.02614166666666662, 0.02269999999999996, 0.017866666666666607]    | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.017, 0.016, 0.013, 0.011, 0.011, 0.01, 0.009, 0.009]  | [0.013, 0.014, 0.013, 0.012, 0.013, 0.016, 0.017, 0.015] | [0.021, 0.026, 0.022, 0.024, 0.021, 0.024, 0.03, 0.028]  | [0.029, 0.023, 0.029, 0.033, 0.03, 0.029, 0.024, 0.024]  | [0.021, 0.021, 0.02, 0.018, 0.016, 0.027, 0.022, 0.02]   | [0.03, 0.032, 0.036, 0.031, 0.031, 0.056, 0.058, 0.069]  | [0.073, 0.049, 0.047, 0.042, 0.044, 0.06, 0.074, 0.094]  | [0.087, 0.056, 0.053, 0.043, 0.046, 0.047, 0.064, 0.043] | [0.107, 0.074, 0.062, 0.077, 0.091, 0.068, 0.047, 0.057] | [0.062, 0.054, 0.051, 0.054, 0.073, 0.088, 0.068, 0.055] |\n",
      "|  2 | 2018-01-19 00:00:00 | 2018-01-19 18:48:00 | 2018-01-20 00:48:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [14.078223314814815, 15.35203637037037, 17.14365961111111, 17.820955333333334, 17.707037092592593, 18.41954390740741, 17.541337833333333, 18.738514981481483]  | [25.164141064102566, 25.131160538461536, 25.8925805, 25.551303756410256, 26.75294203846154, 27.401081064102566, 28.212688782051284, 28.813507923076923]         | [39.193769884615385, 40.17888384615385, 38.750683692307696, 39.299881141025644, 39.337163474358974, 40.58325376923077, 42.11326337179487, 42.911965666666674]   | [59.984611309523814, 60.18833263095239, 61.14302666666667, 64.24411789285715, 63.29608141666667, 62.62499941666666, 61.69959864285715, 61.84606495238096]   | [79.75623158333333, 76.87617133333333, 75.42216033333334, 75.08351247222222, 81.07664755555557, 83.46271688888889, 82.53065855555556, 83.22970230555556]       | [97.23765138095239, 97.15376613095239, 98.93532905952382, 100.25485735714287, 99.61440013095239, 95.08060209523809, 94.46011754761905, 100.32409597619049]       | [103.975767375, 101.12273681666667, 99.57179175000002, 98.92401120833334, 98.33215416666667, 99.38071979166668, 99.34623363333334, 99.37978773333334]     | [79.70962866666666, 84.23943216666667, 83.66954507142857, 82.38330457142857, 81.7814611904762, 82.99047399999999, 83.5150896904762, 84.30867078571428]    | [62.01055788461539, 65.10929335897437, 65.62264241025642, 68.46900516666668, 69.10423876923076, 70.44066702564103, 68.73428330769231, 67.4953626923077]     | [47.19756988333334, 46.97014765, 47.376525083333334, 44.45545426666667, 48.52854918333334, 50.01984251666667, 50.329285883333334, 51.162546033333335]          | [nan, nan, nan, nan, nan, nan, nan, nan]    | [16.553356, 18.790296, 19.685072, 21.474624000000002, 21.474624000000002, 22.145706, 20.803542000000004, 36.014734000000004]       | [32.43563, 31.764548, 33.777794, 34.896264, 37.133204000000006, 38.922756, 38.02798, 39.14645]                               | [46.752046, 48.541598, 52.12070200000001, 52.12070200000001, 52.344396, 52.79178400000001, 52.12070200000001, 52.568090000000005]          | [70.46361, 70.01622200000001, 73.14793800000001, 79.187676, 78.06920600000001, 78.51659400000001, 77.39812400000001, 78.51659400000001]          | [91.93823400000001, 87.68804800000001, 83.88525, 84.10894400000001, 91.93823400000001, 92.609316, 88.135436, 87.911742]                                  | [108.26789600000001, 107.37312, 108.71528400000001, 114.978716, 114.08394000000001, 106.25465000000001, 109.61006, 110.504836]                           | [114.53132800000002, 111.84700000000001, 107.37312, 104.912486, 106.25465000000001, 110.05744800000001, 110.504836, 122.13692400000001]          | [102.89924, 105.58356800000001, 97.083196, 96.859502, 96.63580800000001, 98.87274800000002, 101.333382, 105.807262]                          | [78.51659400000001, 83.88525, 77.17443, 81.872004, 84.556332, 82.990474, 82.543086, 75.608572]                           | [56.818276, 55.252418, 56.147194000000006, 58.83152200000001, 62.410626, 64.423872, 61.068462000000004, 63.081708000000006]          | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.0031111111111110775, 0.004731481481481444, 0.0033425925925925555, 0.005185185185185144, 0.006018518518518489, 0.004962962962962933, 0.0043611111111111, 0.006351851851851833]   | [0.0073461538461538, 0.005641025641025623, 0.0051410256410256, 0.005602564102564077, 0.005641025641025608, 0.006532051282051223, 0.007153846153846116, 0.006512820512820461]      | [0.006570512820512784, 0.007230769230769185, 0.00864743589743586, 0.008621794871794831, 0.007480769230769193, 0.0074935897435896995, 0.007801282051282, 0.007647435897435861]       | [0.007535714285714236, 0.007315476190476157, 0.007434523809523764, 0.007738095238095199, 0.008011904761904722, 0.00816666666666663, 0.008589285714285678, 0.009494047619047579]   | [0.012736111111111066, 0.013472222222222184, 0.0102222222222222, 0.00887499999999995, 0.008666666666666633, 0.009999999999999967, 0.011180555555555518, 0.009638888888888834]    | [0.014059523809523763, 0.014374999999999949, 0.012892857142857107, 0.013803571428571386, 0.012464285714285678, 0.007541666666666621, 0.008636904761904707, 0.008380952380952334] | [0.01426249999999995, 0.015133333333333285, 0.013991666666666616, 0.011704166666666625, 0.012816666666666624, 0.0122583333333333, 0.01289166666666662, 0.0161583333333333]       | [0.02305952380952376, 0.018559523809523755, 0.017416666666666643, 0.01628571428571427, 0.018952380952380898, 0.018023809523809487, 0.017095238095238073, 0.01974999999999996]    | [0.018647435897435862, 0.02054487179487176, 0.019025641025640968, 0.0153461538461538, 0.019410256410256385, 0.019249999999999965, 0.017352564102564062, 0.017942307692307646]   | [0.01235833333333331, 0.01089999999999996, 0.01139166666666662, 0.01902499999999997, 0.017891666666666632, 0.020066666666666618, 0.018049999999999948, 0.01671666666666664] | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.013, 0.014, 0.01, 0.015, 0.019, 0.016, 0.018, 0.033]  | [0.027, 0.02, 0.015, 0.013, 0.015, 0.017, 0.021, 0.016]  | [0.034, 0.024, 0.031, 0.035, 0.037, 0.046, 0.031, 0.021] | [0.02, 0.021, 0.019, 0.03, 0.021, 0.034, 0.037, 0.023]   | [0.027, 0.038, 0.021, 0.016, 0.019, 0.021, 0.023, 0.02]  | [0.036, 0.032, 0.032, 0.053, 0.045, 0.019, 0.032, 0.022] | [0.039, 0.049, 0.052, 0.033, 0.038, 0.046, 0.048, 0.1]   | [0.062, 0.051, 0.049, 0.051, 0.051, 0.058, 0.041, 0.055] | [0.058, 0.054, 0.053, 0.057, 0.076, 0.065, 0.049, 0.046] | [0.041, 0.035, 0.048, 0.084, 0.046, 0.064, 0.057, 0.048] |\n",
      "|  3 | 2018-01-30 00:00:00 | 2018-01-30 15:30:00 | 2018-01-30 18:30:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [40.824155, 43.07352244444444, 45.79927525925926, 44.82372087037037, 40.320843499999995, 41.109986222222226, 39.013890592592595, 40.91736083333333]            | [42.8330991923077, 45.7970446923077, 44.73019638461539, 43.18011167948718, 48.44695823076923, 53.7740300897436, 54.05221365384615, 55.03445974358975]           | [55.21513566666667, 54.00489376923077, 54.592807487179485, 55.339888089743596, 55.07604388461539, 52.61110807692308, 53.22913444871795, 60.083348038461544]     | [67.0349668452381, 64.80202138095238, 64.7740596309524, 68.94169189285715, 76.86285621428571, 80.51785639285715, 85.13420816666667, 88.05288226190477]      | [102.10388355555557, 104.07363350000001, 109.41432775, 114.77987688888891, 123.24918027777778, 126.12302680555557, 123.86123191666667, 120.95010305555557]     | [114.63385441666667, 114.57793091666667, 117.82681996428572, 119.52716066666667, 123.59359802380952, 122.53770908333334, 126.56819561904763, 126.28857811904763] | [99.97164477500002, 99.91478921666666, 99.22506605000001, 98.77861010833334, 100.05180179166668, 101.09197889166668, 100.838459025, 99.14584109166667]    | [91.94356004761906, 94.01272954761906, 90.1806382857143, 86.46039402380951, 84.17285657142858, 89.75988052380953, 92.78507557142858, 92.4735017857143]    | [59.67324237179488, 57.86218133333333, 57.26853187179488, 57.747466461538465, 59.28321180769231, 59.591508025641026, 61.62913093589744, 61.68935624358975]  | [35.68105711666667, 33.91573863333333, 37.06423168333334, 34.018265050000004, 32.06094255, 31.62287513333333, 29.950762483333335, 25.314704333333335]          | [nan, nan, nan, nan, nan, nan, nan, nan]    | [42.50186, 49.43637400000001, 52.344396, 51.00223200000001, 50.778538000000005, 52.568090000000005, 49.212680000000006, 49.660068] | [48.988986, 50.778538000000005, 52.344396, 53.015478, 58.16044, 58.38413400000001, 60.62107400000001, 63.081708000000006]    | [64.647566, 64.423872, 60.844768, 61.963238000000004, 61.73954400000001, 59.27891, 66.213424, 72.253162]                                   | [84.10894400000001, 86.12219, 87.911742, 109.162672, 118.334126, 123.47908800000002, 124.15017, 114.978716]                                      | [113.189164, 112.741776, 119.00520800000001, 121.018454, 129.07143800000003, 132.203154, 131.532072, 127.95296800000001]                                 | [129.07143800000003, 125.71602800000001, 123.92647600000001, 125.04494600000001, 135.55856400000002, 130.637296, 136.90072800000002, 135.55856400000002] | [115.42610400000001, 116.54457400000001, 117.66304400000001, 115.64979800000002, 117.66304400000001, 119.452596, 128.176662, 126.16341600000001] | [103.570322, 103.34662800000001, 106.478344, 104.688792, 102.451852, 102.00446400000001, 100.43860600000001, 99.54383]                       | [91.04345800000002, 87.016966, 76.950736, 70.23991600000001, 75.608572, 96.41211400000002, 96.859502, 81.64831000000001] | [46.752046, 44.515106, 53.68656, 43.172942000000006, 39.14645, 45.409882, 42.278166, 37.804286]                                      | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.011111111111111066, 0.011388888888888855, 0.010166666666666645, 0.00917592592592589, 0.011092592592592545, 0.009999999999999978, 0.0109444444444444, 0.01126851851851849]       | [0.0067435897435897075, 0.006916666666666631, 0.009134615384615338, 0.009378205128205093, 0.008512820512820476, 0.0096410256410256, 0.011499999999999984, 0.009724358974358931]   | [0.007211538461538423, 0.006673076923076877, 0.006051282051282007, 0.007153846153846116, 0.007455128205128161, 0.009570512820512784, 0.009096153846153808, 0.00880769230769227]     | [0.008214285714285672, 0.010952380952380913, 0.009761904761904699, 0.011392857142857099, 0.015160714285714263, 0.015874999999999948, 0.015404761904761864, 0.013499999999999972]  | [0.016666666666666632, 0.014680555555555532, 0.019055555555555534, 0.013819444444444384, 0.0112083333333333, 0.012472222222222199, 0.010833333333333283, 0.013736111111111067]   | [0.01181547619047615, 0.009565476190476157, 0.009130952380952335, 0.00905952380952377, 0.010333333333333286, 0.009154761904761843, 0.009172619047619015, 0.009583333333333301]   | [0.012245833333333305, 0.01408333333333329, 0.014983333333333303, 0.013604166666666626, 0.0145458333333333, 0.014262499999999964, 0.014591666666666624, 0.013733333333333295]    | [0.014833333333333313, 0.01816666666666663, 0.02674999999999999, 0.025321428571428543, 0.022714285714285687, 0.020440476190476144, 0.014428571428571386, 0.01497619047619043]    | [0.016153846153846116, 0.017217948717948676, 0.01769230769230766, 0.017019230769230724, 0.01648076923076919, 0.02170512820512816, 0.019910256410256368, 0.018621794871794814]   | [0.01576666666666664, 0.01652499999999995, 0.0173083333333333, 0.01828333333333329, 0.019849999999999958, 0.02038333333333331, 0.01799166666666662, 0.0187083333333333]     | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.031, 0.029, 0.038, 0.029, 0.049, 0.048, 0.042, 0.04]  | [0.018, 0.018, 0.021, 0.031, 0.021, 0.026, 0.029, 0.022] | [0.013, 0.021, 0.02, 0.044, 0.019, 0.019, 0.021, 0.021]  | [0.023, 0.031, 0.033, 0.042, 0.036, 0.038, 0.035, 0.024] | [0.032, 0.033, 0.035, 0.034, 0.028, 0.024, 0.027, 0.03]  | [0.032, 0.036, 0.031, 0.024, 0.059, 0.029, 0.026, 0.028] | [0.041, 0.062, 0.066, 0.053, 0.045, 0.044, 0.054, 0.045] | [0.039, 0.044, 0.073, 0.092, 0.079, 0.055, 0.034, 0.05]  | [0.054, 0.059, 0.071, 0.068, 0.055, 0.082, 0.083, 0.06]  | [0.042, 0.04, 0.055, 0.078, 0.07, 0.091, 0.049, 0.059]   |\n",
      "|  4 | 2018-01-31 00:00:00 | 2018-01-31 18:30:00 | 2018-01-31 21:25:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [13.479634740740742, 12.649067203703705, 14.70580925925926, 14.591891018518519, 15.43240051111111, 14.64988575925926, 17.276219018518518, 18.55831703703704]   | [27.745225679487177, 27.730886320512823, 28.548229782051287, 28.592681794871798, 30.874934169230773, 31.244029269230772, 31.417535512820514, 32.44710148717949] | [37.39704820512821, 32.61487198717949, 30.95150634615385, 31.410365833333337, 34.18560536923077, 37.53757392307693, 42.511897551282054, 48.58174820512821]      | [42.68560864285714, 45.401892928571435, 45.54436470238095, 40.83214407142857, 37.38565865714286, 38.32623866666667, 42.10107491666667, 49.65341044047619]   | [62.513152416666664, 56.46720069444444, 51.493116055555554, 49.03869577777778, 47.967450066666665, 45.10540961111112, 45.391240833333335, 52.30400680555556]   | [95.69842361904763, 93.55468945238097, 87.27394779761906, 82.4139293452381, 76.79255238571429, 74.43817303571429, 75.00273408333335, 76.77630794047619]          | [90.24281989166667, 92.63820980833334, 93.65881368333334, 97.15776066666668, 99.05729555, 98.12896545000001, 97.77012299166668, 98.24640480000001]        | [70.96958452380953, 72.67125673809524, 73.45684876190477, 74.83363207142858, 74.94068562857144, 74.01075771428573, 70.76719471428572, 66.55429104761905]  | [64.87126, 64.29481776923078, 64.6117176025641, 63.821618923076926, 64.23803390769231, 67.54124864102565, 67.59430426923078, 70.41342224358975]             | [34.6613853, 32.84014331666667, 32.24362598333333, 31.05618366666667, 28.151889900000004, 25.441464266666667, 27.05206106666667, 28.960916533333336]           | [nan, nan, nan, nan, nan, nan, nan, nan]    | [24.830034, 22.593094, 21.922012000000002, 20.356154, 20.356154, 21.474624000000002, 21.922012000000002, 21.027236000000002]       | [33.777794, 33.330406, 35.119958000000004, 36.462122, 39.370144, 41.830778, 45.633576, 46.752046]                            | [41.60708400000001, 41.383390000000006, 40.041226, 37.580592, 39.14645, 44.067718, 48.988986, 55.028724000000004]                          | [50.778538000000005, 52.568090000000005, 53.462866, 51.897008, 52.344396, 46.752046, 50.107456, 59.726298]                                       | [72.70055, 70.01622200000001, 56.818276, 51.897008, 51.897008, 50.33115, 51.897008, 62.858014000000004]                                                  | [106.478344, 106.030956, 110.05744800000001, 109.61006, 109.83375400000001, 109.162672, 110.72853, 109.61006]                                            | [105.807262, 105.807262, 111.399612, 115.64979800000002, 118.78151400000002, 121.465842, 120.34737200000001, 122.36061800000002]                 | [78.2929, 80.082452, 79.187676, 80.306146, 83.21416800000002, 82.990474, 79.63506400000001, 78.740288]                                       | [80.753534, 82.543086, 83.661556, 80.97722800000001, 84.780026, 88.582824, 87.016966, 93.72778600000001]                 | [56.594582, 54.133948000000004, 48.988986, 51.897008, 38.25167400000001, 39.81753200000001, 49.660068, 49.883762000000004]           | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.009120370370370322, 0.008111111111111078, 0.007999999999999957, 0.0068333333333333, 0.006699999999999978, 0.008833333333333299, 0.006851851851851811, 0.007555555555555522]     | [0.005538461538461508, 0.0064102564102563615, 0.007320512820512793, 0.006794871794871761, 0.006661538461538454, 0.006711538461538431, 0.006666666666666623, 0.007211538461538423] | [0.003910256410256377, 0.0051025641025640766, 0.006198717948717915, 0.005762820512820477, 0.0048153846153846155, 0.0061730769230768845, 0.006435897435897399, 0.006884615384615346] | [0.0048809523809523496, 0.004797619047619007, 0.005273809523809493, 0.005797619047619, 0.005585714285714278, 0.0059345238095237715, 0.006285714285714236, 0.007303571428571401]   | [0.010763888888888866, 0.00790277777777775, 0.004402777777777767, 0.004666666666666617, 0.008116666666666666, 0.007069444444444416, 0.008083333333333283, 0.009305555555555534]  | [0.014446428571428529, 0.01534523809523805, 0.015339285714285665, 0.014994047619047593, 0.01582857142857142, 0.0168630952380952, 0.015369047619047577, 0.01322023809523805]      | [0.017183333333333307, 0.01431249999999996, 0.01363749999999996, 0.01557916666666662, 0.015529999999999993, 0.016320833333333278, 0.016349999999999955, 0.016099999999999955]    | [0.019297619047618984, 0.017797619047619, 0.013107142857142814, 0.016857142857142828, 0.014514285714285686, 0.018476190476190445, 0.017547619047619027, 0.019416666666666613]    | [0.020064102564102522, 0.02158974358974356, 0.020096153846153805, 0.020499999999999963, 0.020869230769230758, 0.02101923076923072, 0.023128205128205074, 0.023256410256410214]  | [0.020291666666666618, 0.0195333333333333, 0.01841666666666663, 0.02019166666666663, 0.0201, 0.0247583333333333, 0.0233083333333333, 0.02344166666666663]                   | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.033, 0.022, 0.025, 0.018, 0.025, 0.044, 0.027, 0.02]  | [0.02, 0.018, 0.026, 0.018, 0.016, 0.021, 0.019, 0.016]  | [0.01, 0.013, 0.025, 0.026, 0.015, 0.018, 0.013, 0.016]  | [0.012, 0.011, 0.015, 0.021, 0.013, 0.016, 0.017, 0.028] | [0.021, 0.022, 0.011, 0.013, 0.025, 0.028, 0.033, 0.024] | [0.031, 0.033, 0.042, 0.033, 0.031, 0.045, 0.034, 0.031] | [0.047, 0.049, 0.047, 0.043, 0.049, 0.049, 0.057, 0.052] | [0.059, 0.044, 0.03, 0.061, 0.041, 0.051, 0.042, 0.049]  | [0.058, 0.051, 0.055, 0.062, 0.066, 0.065, 0.079, 0.079] | [0.05, 0.047, 0.058, 0.059, 0.052, 0.06, 0.056, 0.064]   |\n",
      "|  5 | 2018-02-06 00:00:00 | 2018-02-06 15:30:00 | 2018-02-06 20:45:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [6.286215648148148, 4.063774333333334, 2.8189586481481483, 2.3032197037037037, 3.601699353535354, 5.600634962962963, 9.484211351851851, 11.362826703703705]    | [23.804769833333335, 23.867863012820514, 22.97021914102564, 23.097839435897438, 23.492562881118882, 24.90459866666667, 24.15608412820513, 25.47960696153846]    | [34.39151856410256, 35.25474797435898, 35.50855462820513, 34.47755471794872, 33.0582188951049, 32.6191737948718, 31.203879064102566, 32.17895547435898]         | [51.29383310714286, 50.85976022619048, 52.343064488095244, 53.62797347619048, 51.5512990909091, 49.773246511904766, 49.45501516666667, 47.589566988095235]  | [70.80536472222222, 67.26354305555556, 67.87248783333334, 70.32690811111111, 77.23882675757577, 70.99799011111112, 69.60922319444445, 68.08064752777777]       | [87.04759077380952, 87.18074196428573, 88.30853254761905, 89.24325390476191, 89.59816235064936, 90.20726852380952, 82.1303173095238, 72.50881228571428]          | [85.33739688333334, 85.5741397, 85.00278794166667, 84.97575825000001, 86.72921417272728, 83.59444780000001, 79.07396488333333, 77.09800121666667]         | [66.52499778571429, 64.51707783333335, 64.58631645238096, 62.32540923809524, 66.18437283116883, 64.32001407142857, 62.47187554761906, 63.579693452380965] | [62.502397897435905, 61.819844410256415, 61.42837991025642, 59.99874582051282, 57.83193832167832, 60.05323538461539, 59.80946628205129, 64.62605696153847]  | [50.43926876666667, 48.871546650000006, 47.57039321666667, 46.2822886, 46.67476989090909, 49.798012633333336, 46.815425966666666, 47.946944783333336]          | [nan, nan, nan, nan, nan, nan, nan, nan]    | [13.197946000000002, 9.171454, 9.618842, 7.381902, 14.763804, 17.671826000000003, 21.698318, 21.25093]                             | [32.43563, 32.43563, 27.514362000000002, 29.974996000000004, 29.974996000000004, 33.330406, 29.974996000000004, 28.185444]   | [42.50186, 42.949248000000004, 45.409882, 44.96249400000001, 44.738800000000005, 41.830778, 44.96249400000001, 40.936002]                  | [64.647566, 61.068462000000004, 69.56883400000001, 69.34514, 69.792528, 69.121446, 65.542342, 61.068462000000004]                                | [79.187676, 77.39812400000001, 74.042714, 73.81902000000001, 82.319392, 80.97722800000001, 74.490102, 73.371632]                                         | [97.977972, 102.89924, 107.149426, 110.95222400000002, 114.30763400000001, 112.29438800000001, 104.46509800000001, 91.267152]                            | [95.29364400000001, 100.43860600000001, 106.030956, 99.76752400000001, 115.20241000000001, 104.912486, 95.741032, 94.39886800000001]             | [80.082452, 82.990474, 79.41137, 80.753534, 79.63506400000001, 81.872004, 80.52984000000001, 68.89775200000001]                              | [75.832266, 70.68730400000001, 71.134692, 70.910998, 72.02946800000001, 70.910998, 75.161184, 78.51659400000001]         | [69.792528, 68.22667, 71.358386, 66.884506, 65.766036, 61.068462000000004, 64.87126, 64.423872]                                      | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.0052499999999999665, 0.004462962962962922, 0.0038425925925925663, 0.0037222222222222, 0.004989898989898956, 0.005601851851851822, 0.006888888888888867, 0.007481481481481444]   | [0.006320512820512777, 0.007352564102564061, 0.007166666666666646, 0.008089743589743546, 0.007426573426573376, 0.007621794871794838, 0.006762820512820492, 0.005961538461538423]  | [0.006820512820512784, 0.005153846153846107, 0.006121794871794838, 0.007076923076923023, 0.0060349650349649925, 0.005852564102564085, 0.006365384615384584, 0.0057115384615384224]  | [0.007458333333333293, 0.006374999999999964, 0.006916666666666621, 0.007023809523809486, 0.0077207792207791785, 0.0078928571428571, 0.007571428571428535, 0.005952380952380922]   | [0.012361111111111066, 0.009749999999999983, 0.007527777777777733, 0.006694444444444382, 0.00918181818181815, 0.008361111111111083, 0.007888888888888867, 0.0094583333333333]    | [0.007857142857142821, 0.010017857142857106, 0.010714285714285664, 0.012178571428571391, 0.01600649350649346, 0.01610119047619043, 0.014833333333333278, 0.013607142857142821]   | [0.013924999999999955, 0.016137499999999954, 0.017920833333333296, 0.018391666666666626, 0.01832727272727268, 0.017908333333333297, 0.016145833333333283, 0.015429166666666622]  | [0.0157619047619047, 0.019321428571428528, 0.01801190476190471, 0.01957142857142854, 0.01862337662337657, 0.01965476190476184, 0.017071428571428543, 0.01501190476190473]        | [0.01407051282051277, 0.015602564102564062, 0.0153910256410256, 0.017397435897435868, 0.015895104895104862, 0.014596153846153799, 0.01836538461538458, 0.017512820512820486]    | [0.02226666666666663, 0.025383333333333293, 0.02452499999999996, 0.023249999999999972, 0.02147272727272722, 0.02096666666666664, 0.02152499999999996, 0.021541666666666633] | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.025, 0.023, 0.023, 0.016, 0.025, 0.025, 0.032, 0.032] | [0.019, 0.025, 0.028, 0.026, 0.021, 0.02, 0.021, 0.03]   | [0.022, 0.013, 0.015, 0.015, 0.017, 0.022, 0.025, 0.017] | [0.022, 0.015, 0.015, 0.018, 0.022, 0.021, 0.017, 0.015] | [0.035, 0.031, 0.015, 0.014, 0.019, 0.021, 0.015, 0.02]  | [0.018, 0.029, 0.029, 0.033, 0.035, 0.037, 0.047, 0.059] | [0.049, 0.061, 0.072, 0.054, 0.051, 0.06, 0.045, 0.052]  | [0.062, 0.071, 0.053, 0.047, 0.036, 0.053, 0.059, 0.041] | [0.046, 0.039, 0.044, 0.062, 0.05, 0.05, 0.16, 0.055]    | [0.093, 0.055, 0.072, 0.061, 0.072, 0.087, 0.072, 0.056] |\n",
      "|  6 | 2018-03-01 00:00:00 | 2018-03-01 16:02:00 | 2018-03-01 22:02:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [39.53998574074074, 39.58762427777778, 39.85688557407408, 35.416145425925926, 32.75460107407408, 29.726447111111113, 31.35651357407408, 30.55287216666667]     | [48.22899997435897, 47.85761057692308, 47.50629628205128, 45.60776515384616, 43.04532170512822, 39.97526494871795, 38.30616356410257, 35.013846743589745]       | [54.3375668974359, 53.900216448717956, 53.44135696153846, 51.01083561538462, 48.8169136923077, 45.46867337179488, 43.78523262820514, 41.77915630769231]         | [56.90482427380953, 56.82227053571429, 57.87682796428572, 58.41609028571429, 58.57720322619048, 56.39485521428572, 54.82500267857143, 53.79441246428571]    | [58.29092816666667, 58.62957602777778, 59.90338908333334, 62.38577111111112, 64.64445913888889, 63.867743861111116, 61.317010888888895, 61.57798722222223]     | [68.87644780952381, 67.8738193452381, 69.39041140476192, 70.47958814285714, 74.34496720238096, 73.12530229761906, 71.2918104047619, 69.22397241666667]           | [83.58978750833333, 81.891577225, 81.21956316666667, 81.06111325, 79.51389641666667, 79.99577057500001, 78.76638563333334, 82.85812171666667]             | [64.7913692857143, 64.9724549047619, 62.90861145238096, 63.10567521428572, 61.129711547619046, 66.75668085714287, 70.0029068809524, 72.09870661904763]    | [53.06709969230769, 53.29222762820513, 52.427564282051286, 49.81063126923077, 51.45105393589744, 52.728690820512824, 50.90759223076923, 47.443203102564105] | [20.01688476666667, 21.398195216666668, 19.854706616666668, 22.268737700000003, 21.174501216666666, 18.726916033333335, 17.860101783333334, 18.63557431666667] | [nan, nan, nan, nan, nan, nan, nan, nan]    | [47.870516, 49.883762000000004, 62.63432, 44.291412, 42.72555400000001, 38.02798, 36.685816, 36.014734000000004]                   | [55.252418, 54.581336, 51.673314000000005, 49.43637400000001, 48.765292, 45.85727, 43.844024000000005, 40.488614000000005]   | [59.726298, 60.62107400000001, 64.423872, 61.73954400000001, 58.38413400000001, 54.357642000000006, 47.64682200000001, 46.528352000000005] | [61.068462000000004, 61.51585000000001, 69.121446, 64.20017800000001, 64.423872, 63.529096, 60.844768, 60.62107400000001]                        | [61.51585000000001, 61.068462000000004, 64.87126, 66.213424, 68.89775200000001, 68.89775200000001, 63.976484000000006, 63.976484000000006]               | [82.319392, 82.319392, 81.64831000000001, 80.753534, 83.21416800000002, 82.09569800000001, 79.85875800000001, 77.39812400000001]                         | [103.34662800000001, 103.794016, 100.88599400000001, 104.688792, 98.42536000000001, 98.42536000000001, 105.807262, 113.189164]                   | [73.81902000000001, 72.70055, 74.713796, 73.81902000000001, 76.27965400000001, 78.51659400000001, 79.41137, 88.582824]                       | [76.05596, 68.45036400000001, 68.002976, 65.318648, 66.213424, 70.23991600000001, 72.70055, 72.253162]                   | [42.50186, 48.094210000000004, 48.541598, 47.199434000000004, 45.85727, 40.488614000000005, 40.936002, 40.264920000000004]           | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.009749999999999957, 0.008092592592592556, 0.009592592592592578, 0.0063333333333333115, 0.006749999999999977, 0.008055555555555522, 0.00947222222222219, 0.008601851851851822]   | [0.008083333333333307, 0.007801282051282, 0.007269230769230731, 0.006224358974358923, 0.005769230769230747, 0.006935897435897393, 0.007782051282051238, 0.005589743589743546]     | [0.009121794871794845, 0.008839743589743562, 0.008032051282051245, 0.007294871794871746, 0.007371794871794831, 0.006275641025640984, 0.0060833333333333, 0.007076923076923031]      | [0.008964285714285687, 0.008964285714285664, 0.009898809523809478, 0.00624999999999995, 0.007029761904761886, 0.006386904761904728, 0.0056785714285714, 0.005249999999999957]     | [0.008902777777777716, 0.0072638888888888675, 0.007430555555555533, 0.0068333333333333, 0.0045277777777777495, 0.00602777777777775, 0.0040277777777777335, 0.003805555555555517] | [0.010499999999999942, 0.010071428571428537, 0.009791666666666629, 0.009297619047618999, 0.008023809523809478, 0.009202380952380906, 0.009827380952380916, 0.008285714285714242] | [0.016183333333333293, 0.01660416666666663, 0.01490833333333329, 0.015516666666666628, 0.015074999999999969, 0.016295833333333308, 0.018204166666666625, 0.019608333333333294]   | [0.02063095238095234, 0.014964285714285671, 0.013726190476190444, 0.014309523809523772, 0.0156428571428571, 0.014928571428571387, 0.025940476190476156, 0.0221785714285714]      | [0.022179487179487145, 0.0158910256410256, 0.018384615384615347, 0.0200833333333333, 0.018275641025641002, 0.020326923076923038, 0.022044871794871766, 0.024275641025640983]    | [0.017924999999999948, 0.015683333333333292, 0.0171666666666666, 0.017783333333333297, 0.0169333333333333, 0.0181083333333333, 0.01991666666666663, 0.02309999999999997]    | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.02, 0.03, 0.046, 0.022, 0.03, 0.03, 0.03, 0.024]      | [0.022, 0.021, 0.02, 0.019, 0.016, 0.019, 0.022, 0.018]  | [0.033, 0.022, 0.03, 0.021, 0.024, 0.018, 0.014, 0.014]  | [0.022, 0.025, 0.047, 0.026, 0.025, 0.019, 0.015, 0.016] | [0.02, 0.026, 0.021, 0.019, 0.011, 0.016, 0.01, 0.008]   | [0.022, 0.024, 0.03, 0.026, 0.027, 0.034, 0.037, 0.02]   | [0.046, 0.043, 0.041, 0.048, 0.054, 0.054, 0.055, 0.054] | [0.056, 0.032, 0.039, 0.038, 0.047, 0.038, 0.079, 0.095] | [0.085, 0.048, 0.05, 0.061, 0.059, 0.055, 0.081, 0.068]  | [0.056, 0.061, 0.067, 0.043, 0.044, 0.039, 0.052, 0.093] |\n",
      "|  7 | 2018-03-06 00:00:00 | 2018-03-06 02:30:00 | 2018-03-06 05:33:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [19.413739462962965, 21.851589814814815, 22.597236481481485, 22.60759268518519, 22.28862161111111, 22.827144203703703, 23.612144444444443, 22.547526703703703] | [30.630304705128204, 32.02695826923077, 35.398141564102566, 38.41227482051282, 41.087999205128206, 42.02866115384616, 42.18065835897436, 41.8867015]            | [46.26020598717949, 49.113738423076924, 47.061776153846154, 45.64791535897436, 46.99868297435898, 46.35914756410257, 46.86532693589744, 45.57478462820513]      | [58.35484073809524, 58.26429792857143, 57.7689755, 55.34029778571429, 55.80632695238096, 57.518651261904765, 57.270990047619044, 55.58529597619048]         | [67.9719073888889, 72.04189544444445, 75.81051797222223, 77.33287991666667, 77.52861216666668, 73.98057677777778, 74.82253613888889, 76.5002411388889]         | [82.79607326190477, 88.05953982142857, 92.56138157142857, 95.08060209523809, 93.504092, 90.14069292857143, 93.42952733333334, 96.37882620238096]                 | [90.71723758333334, 95.24704108333334, 96.16418648333334, 98.5819458, 98.83546566666668, 99.60720996666667, 100.04620944166668, 102.18714743333334]       | [77.9680110952381, 78.6630603095238, 80.19962504761905, 81.16896571428572, 78.62311495238096, 83.93584745238095, 85.08627373809524, 79.51789095238095]    | [60.209534397435895, 57.91380302564103, 60.62394187179488, 61.48860521794872, 62.9655591923077, 64.87986361538462, 62.1984034871795, 60.95374712820513]     | [31.438327583333336, 30.13717415, 28.170531066666666, 28.998198866666666, 31.544582233333337, 33.0507885, 30.797071449999997, 31.613554550000003]              | [nan, nan, nan, nan, nan, nan, nan, nan]    | [29.974996000000004, 34.448876000000006, 36.685816, 32.659324, 34.67257, 33.330406, 32.211936, 30.198690000000003]                 | [45.186188, 40.936002, 39.81753200000001, 43.396636, 47.423128, 47.423128, 47.870516, 49.212680000000006]                    | [53.91025400000001, 55.923500000000004, 54.80503, 51.225926, 50.778538000000005, 49.660068, 51.897008, 52.12070200000001]                  | [70.68730400000001, 65.98973000000001, 70.01622200000001, 71.134692, 68.22667, 68.22667, 69.34514, 69.34514]                                     | [71.80577400000001, 86.12219, 84.556332, 85.45110800000002, 85.674802, 83.661556, 84.10894400000001, 86.569578]                                          | [93.95148, 102.22815800000001, 108.71528400000001, 111.84700000000001, 112.518082, 112.96547000000001, 117.88673800000001, 113.41285800000001]           | [102.451852, 108.93897800000002, 109.83375400000001, 114.08394000000001, 116.54457400000001, 119.89998400000002, 125.71602800000001, 125.939722] | [92.609316, 93.72778600000001, 93.95148, 93.504092, 87.016966, 96.63580800000001, 97.75427800000001, 91.490846]                              | [77.845512, 78.51659400000001, 80.082452, 78.963982, 92.609316, 83.88525, 81.200922, 79.63506400000001]                  | [45.633576, 42.50186, 43.172942000000006, 41.830778, 42.50186, 48.988986, 42.50186, 42.949248000000004]                              | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.008101851851851844, 0.009314814814814778, 0.011101851851851823, 0.011768518518518477, 0.009907407407407356, 0.010703703703703679, 0.010712962962962922, 0.011212962962962933]   | [0.008730769230769193, 0.007339743589743546, 0.006205128205128154, 0.007262820512820469, 0.006602564102564061, 0.007705128205128161, 0.007115384615384577, 0.008012820512820477]  | [0.005826923076923039, 0.007608974358974315, 0.006467948717948684, 0.0073782051282050695, 0.006435897435897407, 0.006371794871794839, 0.00587179487179483, 0.005141025641025607]    | [0.005916666666666635, 0.0047559523809523425, 0.006238095238095186, 0.007672619047619007, 0.005678571428571386, 0.006428571428571393, 0.006851190476190428, 0.007244047619047572] | [0.00627777777777775, 0.008930555555555499, 0.0088194444444444, 0.007986111111111083, 0.009833333333333283, 0.010972222222222184, 0.011722222222222198, 0.011152777777777734]    | [0.010755952380952328, 0.010517857142857084, 0.011392857142857107, 0.010869047619047579, 0.011071428571428527, 0.011589285714285665, 0.0118988095238095, 0.015428571428571392]   | [0.014962499999999952, 0.016441666666666625, 0.01627499999999995, 0.016441666666666625, 0.018504166666666624, 0.0194833333333333, 0.022341666666666628, 0.02160416666666663]     | [0.014749999999999958, 0.016035714285714257, 0.017583333333333315, 0.014999999999999958, 0.014428571428571386, 0.015142857142857085, 0.015857142857142813, 0.017238095238095184] | [0.01622435897435894, 0.0159807692307692, 0.0136538461538461, 0.013314102564102525, 0.016243589743589706, 0.01623717948717944, 0.01562820512820509, 0.017794871794871752]       | [0.0175833333333333, 0.01759999999999997, 0.018499999999999968, 0.0137833333333333, 0.01255833333333331, 0.01876666666666662, 0.02031666666666665, 0.01828333333333329]     | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.029, 0.027, 0.025, 0.028, 0.034, 0.038, 0.043, 0.047] | [0.029, 0.017, 0.017, 0.023, 0.028, 0.03, 0.03, 0.036]   | [0.024, 0.024, 0.018, 0.029, 0.023, 0.016, 0.016, 0.014] | [0.014, 0.011, 0.017, 0.017, 0.017, 0.022, 0.016, 0.018] | [0.015, 0.022, 0.02, 0.017, 0.022, 0.049, 0.03, 0.027]   | [0.023, 0.025, 0.028, 0.031, 0.032, 0.034, 0.049, 0.04]  | [0.038, 0.031, 0.045, 0.052, 0.05, 0.067, 0.076, 0.07]   | [0.054, 0.04, 0.045, 0.045, 0.042, 0.05, 0.047, 0.068]   | [0.05, 0.063, 0.042, 0.046, 0.057, 0.044, 0.045, 0.05]   | [0.056, 0.049, 0.045, 0.038, 0.039, 0.052, 0.046, 0.05]  |\n",
      "|  8 | 2018-04-02 00:00:00 | 2018-04-02 13:30:00 | 2018-04-02 16:30:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [4.995832666666667, 8.960187444444445, nan, nan, nan, nan, nan, nan]                                                                                           | [9.496957448717948, 12.526864, nan, nan, nan, nan, nan, nan]                                                                                                    | [22.210233115384618, 20.132460000000002, nan, nan, nan, nan, nan, nan]                                                                                          | [31.92299791666667, 31.652701000000004, nan, nan, nan, nan, nan, nan]                                                                                       | [37.15495202777778, 39.18373233333333, nan, nan, nan, nan, nan, nan]                                                                                           | [59.57450564285715, 54.58133600000001, nan, nan, nan, nan, nan, nan]                                                                                             | [76.01215325833334, 60.5539658, nan, nan, nan, nan, nan, nan]                                                                                             | [56.41349638095239, 48.46170728571429, nan, nan, nan, nan, nan, nan]                                                                                      | [38.83958771794872, 33.81220846153847, nan, nan, nan, nan, nan, nan]                                                                                        | [20.44003925, 24.047105000000002, nan, nan, nan, nan, nan, nan]                                                                                                | [nan, nan, nan, nan, nan, nan, nan, nan]    | [9.395148, 19.685072, nan, nan, nan, nan, nan, nan]                                                                                | [16.777050000000003, 16.777050000000003, nan, nan, nan, nan, nan, nan]                                                       | [30.422384, 26.395892000000003, nan, nan, nan, nan, nan, nan]                                                                              | [38.475368, 36.014734000000004, nan, nan, nan, nan, nan, nan]                                                                                    | [44.515106, 42.949248000000004, nan, nan, nan, nan, nan, nan]                                                                                            | [83.21416800000002, 79.187676, nan, nan, nan, nan, nan, nan]                                                                                             | [96.41211400000002, 93.504092, nan, nan, nan, nan, nan, nan]                                                                                     | [69.792528, 57.713052000000005, nan, nan, nan, nan, nan, nan]                                                                                | [52.79178400000001, 42.72555400000001, nan, nan, nan, nan, nan, nan]                                                     | [34.67257, 32.659324, nan, nan, nan, nan, nan, nan]                                                                                  | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.0030277777777777664, 0.004999999999999989, nan, nan, nan, nan, nan, nan]                                                                                                        | [0.00460897435897433, 0.004807692307692308, nan, nan, nan, nan, nan, nan]                                                                                                         | [0.007089743589743569, 0.006999999999999992, nan, nan, nan, nan, nan, nan]                                                                                                          | [0.005505952380952342, 0.006321428571428557, nan, nan, nan, nan, nan, nan]                                                                                                        | [0.006958333333333284, 0.0095, nan, nan, nan, nan, nan, nan]                                                                                                                     | [0.010898809523809472, 0.010749999999999994, nan, nan, nan, nan, nan, nan]                                                                                                       | [0.015841666666666625, 0.019449999999999988, nan, nan, nan, nan, nan, nan]                                                                                                       | [0.023166666666666603, 0.02164285714285714, nan, nan, nan, nan, nan, nan]                                                                                                        | [0.016391025641025608, 0.017153846153846145, nan, nan, nan, nan, nan, nan]                                                                                                      | [0.01939166666666662, 0.01654999999999998, nan, nan, nan, nan, nan, nan]                                                                                                    | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.009, 0.012, nan, nan, nan, nan, nan, nan]             | [0.018, 0.012, nan, nan, nan, nan, nan, nan]             | [0.018, 0.015, nan, nan, nan, nan, nan, nan]             | [0.017, 0.016, nan, nan, nan, nan, nan, nan]             | [0.016, 0.025, nan, nan, nan, nan, nan, nan]             | [0.028, 0.024, nan, nan, nan, nan, nan, nan]             | [0.045, 0.077, nan, nan, nan, nan, nan, nan]             | [0.059, 0.056, nan, nan, nan, nan, nan, nan]             | [0.05, 0.048, nan, nan, nan, nan, nan, nan]              | [0.075, 0.024, nan, nan, nan, nan, nan, nan]             |\n",
      "|  9 | 2018-04-14 00:00:00 | 2018-04-14 17:13:00 | 2018-04-14 23:13:00 | [nan, nan, nan, nan, nan, nan, nan, nan] | [14.179714111111112, 15.832564222222222, 15.813923055555556, 19.047129851851853, 20.020613, 17.92244612962963, 16.677630444444443, 15.855347870370373]         | [18.173703564102563, 20.47517067948718, 21.179233205128206, 20.57267832051282, 18.481999782051282, 18.43037808974359, 18.07476198717949, 17.769333641025643]    | [19.45851012820513, 20.604224910256413, 19.302211115384615, 15.820614756410258, 14.990365871794873, 13.846085025641027, 11.774047653846155, 10.173775192307694] | [27.433139773809526, 25.09367335714286, 23.847378214285715, 21.758236035714287, 19.01399, 17.441474440476192, 15.23116467857143, 11.951650857142857]        | [42.09796805555556, 41.61951144444445, 41.32746650000001, 40.17792788888889, 35.80346744444444, 32.768064138888896, 26.37103711111111, 24.183806888888892]     | [51.010221071428575, 54.79437790476191, 54.572015416666666, 55.34162929761904, 52.01684407142858, 46.981066047619045, 43.63897116666667, 40.76689998809524]      | [65.67189810833334, 68.66660153333333, 69.21185565833335, 72.30069697500001, 70.55588377500001, 70.03393110833335, 69.74312890833333, 67.39713808333333]  | [40.14242090476191, 41.287521142857145, 41.73757216666667, 44.331357357142856, 43.29277807142858, 42.437947428571434, 43.43924438095239, 45.409882]       | [31.783189166666673, 30.02518375641026, 28.94112821794872, 28.520985, 31.244029269230772, 30.111219910256413, 28.12378475641026, 29.014258948717952]        | [10.215359333333335, 10.168756416666668, 8.716609533333335, 7.596275416666667, 8.563751966666668, 10.729855533333334, 10.739176116666668, 11.074717116666667]  | [nan, nan, nan, nan, nan, nan, nan, nan]    | [19.237684, 20.803542000000004, 19.237684, 23.48787, 24.606340000000003, 24.606340000000003, 19.461378, 18.790296]                 | [25.501116000000003, 25.948504, 25.053728, 26.172198, 25.948504, 25.501116000000003, 23.040482000000004, 22.369400000000002] | [24.382646, 26.84328, 26.395892000000003, 20.579848, 19.908766000000004, 17.000744, 15.65858, 16.553356]                                   | [41.383390000000006, 37.804286, 38.922756, 34.001488, 29.08022, 28.185444, 27.066974000000002, 26.619586]                                        | [45.409882, 46.304658, 44.515106, 47.64682200000001, 43.844024000000005, 40.264920000000004, 38.25167400000001, 31.764548]                               | [61.73954400000001, 67.10820000000001, 67.555588, 70.23991600000001, 71.58208, 68.45036400000001, 67.77928200000001, 63.30540200000001]                  | [83.88525, 88.135436, 89.701294, 87.46435400000001, 93.72778600000001, 85.89849600000001, 89.701294, 85.89849600000001]                          | [49.212680000000006, 52.568090000000005, 47.64682200000001, 53.462866, 51.897008, 55.028724000000004, 55.028724000000004, 63.30540200000001] | [66.660812, 47.870516, 48.094210000000004, 54.581336, 55.252418, 50.778538000000005, 49.883762000000004, 50.107456]      | [23.48787, 28.409138, 21.027236000000002, 14.987498000000002, 17.89552, 19.685072, 23.264176000000003, 24.830034]                    | [nan, nan, nan, nan, nan, nan, nan, nan] | [0.006453703703703689, 0.006638888888888856, 0.0053981481481481216, 0.006055555555555512, 0.006537037037037, 0.005416666666666633, 0.004925925925925889, 0.005342592592592555]     | [0.006391025641025622, 0.006480769230769192, 0.005506410256410208, 0.004942307692307654, 0.006326923076923046, 0.005564102564102531, 0.005647435897435869, 0.006051282051282024]  | [0.006249999999999954, 0.005903846153846122, 0.005480769230769193, 0.004826923076923031, 0.0037692307692307305, 0.004705128205128161, 0.0045833333333332995, 0.003980769230769192]  | [0.010035714285714257, 0.010047619047619, 0.010666666666666621, 0.010071428571428535, 0.008827380952380915, 0.008744047619047578, 0.007916666666666622, 0.007172619047619021]     | [0.006194444444444399, 0.006374999999999967, 0.0069583333333333, 0.0099444444444444, 0.014472222222222183, 0.010902777777777749, 0.010402777777777717, 0.009722222222222167]     | [0.0087678571428571, 0.0087023809523809, 0.008773809523809486, 0.011874999999999965, 0.011428571428571394, 0.009083333333333292, 0.01182738095238092, 0.010291666666666628]      | [0.01245833333333329, 0.014845833333333294, 0.01497499999999996, 0.013145833333333301, 0.013591666666666618, 0.014316666666666625, 0.01643749999999995, 0.014833333333333296]    | [0.015297619047618985, 0.015095238095238042, 0.011309523809523757, 0.016857142857142814, 0.016821428571428515, 0.016571428571428515, 0.016821428571428543, 0.017654761904761857] | [0.024371794871794823, 0.018775641025640992, 0.01467948717948713, 0.01720512820512816, 0.01733974358974356, 0.014807692307692253, 0.015749999999999962, 0.01710256410256407]    | [0.01721666666666663, 0.018183333333333298, 0.0155083333333333, 0.01646666666666662, 0.01563333333333329, 0.01852499999999995, 0.020791666666666632, 0.02151666666666664]   | [nan, nan, nan, nan, nan, nan, nan, nan]    | [0.015, 0.021, 0.02, 0.021, 0.025, 0.029, 0.023, 0.018]  | [0.02, 0.022, 0.021, 0.021, 0.024, 0.017, 0.023, 0.024]  | [0.019, 0.019, 0.017, 0.013, 0.016, 0.02, 0.027, 0.022]  | [0.028, 0.036, 0.042, 0.038, 0.027, 0.031, 0.029, 0.028] | [0.017, 0.02, 0.016, 0.032, 0.034, 0.024, 0.02, 0.021]   | [0.035, 0.022, 0.022, 0.041, 0.053, 0.019, 0.047, 0.023] | [0.038, 0.055, 0.035, 0.035, 0.044, 0.061, 0.053, 0.048] | [0.046, 0.05, 0.03, 0.047, 0.047, 0.053, 0.044, 0.047]   | [0.096, 0.051, 0.046, 0.042, 0.043, 0.061, 0.056, 0.049] | [0.05, 0.068, 0.06, 0.049, 0.04, 0.054, 0.05, 0.049]     |\n"
     ]
    }
   ],
   "source": [
    "print(wind_profiler_arrays.head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge launch data with weather, field mill, and wind profiler data based on the DATE column using inner joins, creating a unified dataframe, launch_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging weather and field mill arrays...\n",
      "Successfully merged with 232 matching records\n",
      "\n",
      "Merging with wind profiler arrays...\n",
      "Successfully merged with 232 matching records\n",
      "\n",
      "Merging with launch data...\n",
      "Successfully merged with 232 matching records\n"
     ]
    }
   ],
   "source": [
    "# First merge all the weather/sensor data together\n",
    "print(\"Merging weather and field mill arrays...\")\n",
    "weather_combined = pd.merge(weather_arrays, field_mill_arrays, \n",
    "                          on=['DATE', 'START_LCC_EVAL', 'END_LCC_EVAL'], \n",
    "                          how='inner')\n",
    "print(f\"Successfully merged with {len(weather_combined)} matching records\")\n",
    "\n",
    "print(\"\\nMerging with wind profiler arrays...\")\n",
    "weather_combined = pd.merge(weather_combined, wind_profiler_arrays,\n",
    "                          on=['DATE', 'START_LCC_EVAL', 'END_LCC_EVAL'], \n",
    "                          how='inner')\n",
    "print(f\"Successfully merged with {len(weather_combined)} matching records\")\n",
    "\n",
    "# Then merge with launch data\n",
    "print(\"\\nMerging with launch data...\")\n",
    "launch_corpus = pd.merge(merged_df, weather_combined, \n",
    "                        on=['DATE', 'START_LCC_EVAL', 'END_LCC_EVAL'], \n",
    "                        how='inner')\n",
    "print(f\"Successfully merged with {len(launch_corpus)} matching records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain statistics by hour before launch:\n",
      "\n",
      "Successful launches (n=174):\n",
      "\n",
      "T-2:\n",
      "Min (0th): 0.000 inches\n",
      "25th: 0.000 inches\n",
      "Median (50th): 0.000 inches\n",
      "75th: 0.000 inches\n",
      "Max (100th): 0.303 inches\n",
      "\n",
      "T-1:\n",
      "Min (0th): 0.000 inches\n",
      "25th: 0.000 inches\n",
      "Median (50th): 0.000 inches\n",
      "75th: 0.000 inches\n",
      "Max (100th): 0.272 inches\n",
      "\n",
      "T-0:\n",
      "Min (0th): 0.000 inches\n",
      "25th: 0.000 inches\n",
      "Median (50th): 0.000 inches\n",
      "75th: 0.000 inches\n",
      "Max (100th): 0.165 inches\n",
      "\n",
      "Weather scrubbed launches (n=23):\n",
      "\n",
      "T-2:\n",
      "Min (0th): 0.000 inches\n",
      "25th: 0.000 inches\n",
      "Median (50th): 0.000 inches\n",
      "75th: 0.024 inches\n",
      "Max (100th): 0.390 inches\n",
      "\n",
      "T-1:\n",
      "Min (0th): 0.000 inches\n",
      "25th: 0.000 inches\n",
      "Median (50th): 0.004 inches\n",
      "75th: 0.012 inches\n",
      "Max (100th): 0.165 inches\n",
      "\n",
      "T-0:\n",
      "Min (0th): 0.000 inches\n",
      "25th: 0.000 inches\n",
      "Median (50th): 0.008 inches\n",
      "75th: 0.014 inches\n",
      "Max (100th): 0.043 inches\n"
     ]
    }
   ],
   "source": [
    "# Filter for successful and weather scrubbed launches\n",
    "successful_launches = launch_corpus[launch_corpus['LAUNCHED'] == 1].copy()\n",
    "wx_scrub_launches = launch_corpus[launch_corpus['WX_SCRUB'] == 1].copy()\n",
    "\n",
    "# Calculate rain arrays for T-2 to T-0 for both groups\n",
    "success_rain_arrays = np.vstack(successful_launches['RAIN'].apply(lambda x: x[-3:]))\n",
    "scrub_rain_arrays = np.vstack(wx_scrub_launches['RAIN'].apply(lambda x: x[-3:]))\n",
    "\n",
    "# Calculate rain statistics for T-2 to T-0 for both groups\n",
    "success_percentiles = np.percentile(success_rain_arrays, [0, 25, 50, 75, 100], axis=0)\n",
    "scrub_percentiles = np.percentile(scrub_rain_arrays, [0, 25, 50, 75, 100], axis=0)\n",
    "\n",
    "print(\"Rain statistics by hour before launch:\")\n",
    "print(f\"\\nSuccessful launches (n={len(successful_launches)}):\")\n",
    "for i, hour in enumerate(['T-2', 'T-1', 'T-0']):\n",
    "    print(f\"\\n{hour}:\")\n",
    "    print(f\"Min (0th): {success_percentiles[0,i]:.3f} inches\")\n",
    "    print(f\"25th: {success_percentiles[1,i]:.3f} inches\")\n",
    "    print(f\"Median (50th): {success_percentiles[2,i]:.3f} inches\")\n",
    "    print(f\"75th: {success_percentiles[3,i]:.3f} inches\")\n",
    "    print(f\"Max (100th): {success_percentiles[4,i]:.3f} inches\")\n",
    "\n",
    "print(f\"\\nWeather scrubbed launches (n={len(wx_scrub_launches)}):\")\n",
    "for i, hour in enumerate(['T-2', 'T-1', 'T-0']):\n",
    "    print(f\"\\n{hour}:\")\n",
    "    print(f\"Min (0th): {scrub_percentiles[0,i]:.3f} inches\")\n",
    "    print(f\"25th: {scrub_percentiles[1,i]:.3f} inches\")\n",
    "    print(f\"Median (50th): {scrub_percentiles[2,i]:.3f} inches\")\n",
    "    print(f\"75th: {scrub_percentiles[3,i]:.3f} inches\")\n",
    "    print(f\"Max (100th): {scrub_percentiles[4,i]:.3f} inches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum observed wind shear by layer and hour before launch:\n",
      "\n",
      "Boundary Layer 900Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.071\n",
      "T-1: 0.074\n",
      "T-0: 0.073\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.070\n",
      "T-1: 0.064\n",
      "T-0: 0.049\n",
      "\n",
      "Low Troposphere 700Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.099\n",
      "T-1: 0.089\n",
      "T-0: 0.100\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.091\n",
      "T-1: 0.035\n",
      "T-0: 0.058\n",
      "\n",
      "Mid Low Troposphere 500Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.106\n",
      "T-1: 0.060\n",
      "T-0: 0.056\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.080\n",
      "T-1: 0.045\n",
      "T-0: 0.041\n",
      "\n",
      "Mid Troposphere 400Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.113\n",
      "T-1: 0.055\n",
      "T-0: 0.066\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.080\n",
      "T-1: 0.040\n",
      "T-0: 0.037\n",
      "\n",
      "High Mid Troposphere 300Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.149\n",
      "T-1: 0.095\n",
      "T-0: 0.075\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.056\n",
      "T-1: 0.044\n",
      "T-0: 0.035\n",
      "\n",
      "Upper Troposphere 200Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.118\n",
      "T-1: 0.102\n",
      "T-0: 0.144\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.109\n",
      "T-1: 0.092\n",
      "T-0: 0.067\n",
      "\n",
      "Lower Stratosphere 150Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.123\n",
      "T-1: 0.105\n",
      "T-0: 0.519\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.122\n",
      "T-1: 0.076\n",
      "T-0: 0.088\n",
      "\n",
      "Mid Stratosphere 100Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.110\n",
      "T-1: 0.099\n",
      "T-0: 0.390\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.110\n",
      "T-1: 0.090\n",
      "T-0: 0.066\n",
      "\n",
      "Upper Stratosphere 50Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.129\n",
      "T-1: 0.160\n",
      "T-0: 0.370\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.163\n",
      "T-1: 0.140\n",
      "T-0: 0.117\n",
      "\n",
      "Top Stratosphere 10Hpa:\n",
      "Successful launches:\n",
      "T-2: 0.243\n",
      "T-1: 0.117\n",
      "T-0: 0.374\n",
      "Weather scrubbed launches:\n",
      "T-2: 0.107\n",
      "T-1: 0.104\n",
      "T-0: 0.094\n"
     ]
    }
   ],
   "source": [
    "wind_shear_layers = [\n",
    "    # 'WIND_SHEAR_MAX_PEAK_SURFACE_LAYER_1000HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_BOUNDARY_LAYER_900HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_LOW_TROPOSPHERE_700HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_MID_LOW_TROPOSPHERE_500HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_MID_TROPOSPHERE_400HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_HIGH_MID_TROPOSPHERE_300HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_UPPER_TROPOSPHERE_200HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_LOWER_STRATOSPHERE_150HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_MID_STRATOSPHERE_100HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_UPPER_STRATOSPHERE_50HPA',\n",
    "    'WIND_SHEAR_MAX_PEAK_TOP_STRATOSPHERE_10HPA'\n",
    "]\n",
    "\n",
    "# Filter for successful and weather scrubbed launches\n",
    "successful_launches = launch_corpus[launch_corpus['LAUNCHED'] == 1].copy()\n",
    "wx_scrub_launches = launch_corpus[launch_corpus['WX_SCRUB'] == 1].copy()\n",
    "\n",
    "print(\"Maximum observed wind shear by layer and hour before launch:\")\n",
    "for layer in wind_shear_layers:\n",
    "    layer_name = layer.replace('WIND_SHEAR_MAX_PEAK_', '').replace('_', ' ').title()\n",
    "    \n",
    "    # Successful launches\n",
    "    success_t2 = successful_launches[layer].apply(lambda x: x[-3]).max()\n",
    "    success_t1 = successful_launches[layer].apply(lambda x: x[-2]).max()\n",
    "    success_t0 = successful_launches[layer].apply(lambda x: x[-1]).max()\n",
    "    \n",
    "    # Weather scrubbed launches  \n",
    "    scrub_t2 = wx_scrub_launches[layer].apply(lambda x: x[-3]).max()\n",
    "    scrub_t1 = wx_scrub_launches[layer].apply(lambda x: x[-2]).max()\n",
    "    scrub_t0 = wx_scrub_launches[layer].apply(lambda x: x[-1]).max()\n",
    "    \n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(\"Successful launches:\")\n",
    "    print(f\"T-2: {success_t2:.3f}\")\n",
    "    print(f\"T-1: {success_t1:.3f}\")\n",
    "    print(f\"T-0: {success_t0:.3f}\")\n",
    "    print(\"Weather scrubbed launches:\")\n",
    "    print(f\"T-2: {scrub_t2:.3f}\")\n",
    "    print(f\"T-1: {scrub_t1:.3f}\")\n",
    "    print(f\"T-0: {scrub_t0:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum observed wind speed by layer and hour before launch:\n",
      "\n",
      "Boundary Layer 900Hpa:\n",
      "Successful launches:\n",
      "T-2: 59.055\n",
      "T-1: 55.924\n",
      "T-0: 50.779\n",
      "Weather scrubbed launches:\n",
      "T-2: 68.227\n",
      "T-1: 68.003\n",
      "T-0: 62.411\n",
      "\n",
      "Low Troposphere 700Hpa:\n",
      "Successful launches:\n",
      "T-2: 76.280\n",
      "T-1: 71.358\n",
      "T-0: 74.490\n",
      "Weather scrubbed launches:\n",
      "T-2: 76.056\n",
      "T-1: 88.135\n",
      "T-0: 100.886\n",
      "\n",
      "Mid Low Troposphere 500Hpa:\n",
      "Successful launches:\n",
      "T-2: 93.280\n",
      "T-1: 94.623\n",
      "T-0: 95.965\n",
      "Weather scrubbed launches:\n",
      "T-2: 103.794\n",
      "T-1: 105.807\n",
      "T-0: 110.729\n",
      "\n",
      "Mid Troposphere 400Hpa:\n",
      "Successful launches:\n",
      "T-2: 112.742\n",
      "T-1: 112.071\n",
      "T-0: 111.847\n",
      "Weather scrubbed launches:\n",
      "T-2: 141.151\n",
      "T-1: 140.704\n",
      "T-0: 132.427\n",
      "\n",
      "High Mid Troposphere 300Hpa:\n",
      "Successful launches:\n",
      "T-2: 131.979\n",
      "T-1: 130.414\n",
      "T-0: 131.085\n",
      "Weather scrubbed launches:\n",
      "T-2: 148.085\n",
      "T-1: 144.730\n",
      "T-0: 144.730\n",
      "\n",
      "Upper Troposphere 200Hpa:\n",
      "Successful launches:\n",
      "T-2: 155.467\n",
      "T-1: 152.112\n",
      "T-0: 149.428\n",
      "Weather scrubbed launches:\n",
      "T-2: 157.481\n",
      "T-1: 156.809\n",
      "T-0: 155.020\n",
      "\n",
      "Lower Stratosphere 150Hpa:\n",
      "Successful launches:\n",
      "T-2: 160.612\n",
      "T-1: 159.046\n",
      "T-0: 171.797\n",
      "Weather scrubbed launches:\n",
      "T-2: 161.060\n",
      "T-1: 161.283\n",
      "T-0: 163.744\n",
      "\n",
      "Mid Stratosphere 100Hpa:\n",
      "Successful launches:\n",
      "T-2: 128.848\n",
      "T-1: 128.848\n",
      "T-0: 154.125\n",
      "Weather scrubbed launches:\n",
      "T-2: 123.255\n",
      "T-1: 125.045\n",
      "T-0: 123.703\n",
      "\n",
      "Upper Stratosphere 50Hpa:\n",
      "Successful launches:\n",
      "T-2: 119.676\n",
      "T-1: 122.808\n",
      "T-0: 122.361\n",
      "Weather scrubbed launches:\n",
      "T-2: 106.031\n",
      "T-1: 107.597\n",
      "T-0: 106.255\n",
      "\n",
      "Top Stratosphere 10Hpa:\n",
      "Successful launches:\n",
      "T-2: 80.754\n",
      "T-1: 83.885\n",
      "T-0: 81.648\n",
      "Weather scrubbed launches:\n",
      "T-2: 74.490\n",
      "T-1: 74.490\n",
      "T-0: 71.135\n"
     ]
    }
   ],
   "source": [
    "wind_speed_layers = [\n",
    "    'WIND_SPEED_MAX_PEAK_BOUNDARY_LAYER_900HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_LOW_TROPOSPHERE_700HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_MID_LOW_TROPOSPHERE_500HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_MID_TROPOSPHERE_400HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_HIGH_MID_TROPOSPHERE_300HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_UPPER_TROPOSPHERE_200HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_LOWER_STRATOSPHERE_150HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_MID_STRATOSPHERE_100HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_UPPER_STRATOSPHERE_50HPA',\n",
    "    'WIND_SPEED_MAX_PEAK_TOP_STRATOSPHERE_10HPA'\n",
    "]\n",
    "\n",
    "# Filter for successful and weather scrubbed launches\n",
    "successful_launches = launch_corpus[launch_corpus['LAUNCHED'] == 1].copy()\n",
    "wx_scrub_launches = launch_corpus[launch_corpus['WX_SCRUB'] == 1].copy()\n",
    "\n",
    "print(\"Maximum observed wind speed by layer and hour before launch:\")\n",
    "for layer in wind_speed_layers:\n",
    "    layer_name = layer.replace('WIND_SPEED_MAX_PEAK_', '').replace('_', ' ').title()\n",
    "    \n",
    "    # Successful launches\n",
    "    success_t2 = successful_launches[layer].apply(lambda x: x[-3]).max()\n",
    "    success_t1 = successful_launches[layer].apply(lambda x: x[-2]).max()\n",
    "    success_t0 = successful_launches[layer].apply(lambda x: x[-1]).max()\n",
    "    \n",
    "    # Weather scrubbed launches\n",
    "    scrub_t2 = wx_scrub_launches[layer].apply(lambda x: x[-3]).max()\n",
    "    scrub_t1 = wx_scrub_launches[layer].apply(lambda x: x[-2]).max() \n",
    "    scrub_t0 = wx_scrub_launches[layer].apply(lambda x: x[-1]).max()\n",
    "    \n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(\"Successful launches:\")\n",
    "    print(f\"T-2: {success_t2:.3f}\")\n",
    "    print(f\"T-1: {success_t1:.3f}\")\n",
    "    print(f\"T-0: {success_t0:.3f}\")\n",
    "    print(\"Weather scrubbed launches:\")\n",
    "    print(f\"T-2: {scrub_t2:.3f}\")\n",
    "    print(f\"T-1: {scrub_t1:.3f}\")\n",
    "    print(f\"T-0: {scrub_t0:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     | START_LCC_EVAL      | END_LCC_EVAL        | RAIN                                                   | WEATHER_CODE                                     |\n",
      "|----:|:--------------------|:--------------------|:-------------------------------------------------------|:-------------------------------------------------|\n",
      "|   0 | 2018-01-07 22:00:00 | 2018-01-08 01:00:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '2', '0', '1', '1', '2', '1', '3']         |\n",
      "|   2 | 2018-01-19 18:48:00 | 2018-01-20 00:48:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '1', '1', '1', '1', '1']         |\n",
      "|   4 | 2018-01-31 18:30:00 | 2018-01-31 21:25:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['3', '3', '3', '3', '3', '2', '2', '2']         |\n",
      "|   5 | 2018-02-06 15:30:00 | 2018-02-06 20:45:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '0', '0', '0', '1', '2', '2', '0']         |\n",
      "|   6 | 2018-03-01 16:02:00 | 2018-03-01 22:02:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '2', '2', '1', '1', '1']         |\n",
      "|   7 | 2018-03-06 02:30:00 | 2018-03-06 05:33:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '1', '0', '1']         |\n",
      "|   8 | 2018-04-02 13:30:00 | 2018-04-02 16:30:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '1', '1', '2', '2', '2', '1']         |\n",
      "|   9 | 2018-04-14 17:13:00 | 2018-04-14 23:13:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.008]           | ['0', '1', '1', '0', '1', '3', '51', '51']       |\n",
      "|  11 | 2018-04-18 20:00:00 | 2018-04-18 22:51:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  13 | 2018-05-11 17:42:00 | 2018-05-11 20:14:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '1', '1']         |\n",
      "|  14 | 2018-06-04 01:30:00 | 2018-06-04 04:45:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.0]             | ['1', '2', '1', '1', '0', '51', '1', '1']        |\n",
      "|  15 | 2018-06-29 06:42:00 | 2018-06-29 09:42:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '1', '1', '1', '1', '1', '1']         |\n",
      "|  16 | 2018-07-22 03:50:00 | 2018-07-22 05:50:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '1', '1']         |\n",
      "|  17 | 2018-08-07 03:18:00 | 2018-08-07 05:18:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  19 | 2018-08-12 02:00:00 | 2018-08-12 08:00:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '1', '1', '1', '0', '0', '0']         |\n",
      "|  20 | 2018-09-10 01:28:00 | 2018-09-10 04:45:00 | [0.016, 0.063, 0.142, 0.0, 0.0, 0.012, 0.0, 0.0]       | ['51', '61', '63', '2', '1', '51', '1', '1']     |\n",
      "|  21 | 2018-10-16 22:15:00 | 2018-10-17 04:15:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  23 | 2018-11-15 18:46:00 | 2018-11-15 20:46:00 | [0.0, 0.012, 0.024, 0.0, 0.0, 0.0, 0.0, 0.004]         | ['2', '51', '53', '2', '1', '2', '1', '51']      |\n",
      "|  24 | 2018-12-05 16:15:00 | 2018-12-05 18:15:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '1', '1', '1', '1', '0', '1']         |\n",
      "|  28 | 2018-12-23 11:51:00 | 2018-12-23 13:51:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  29 | 2019-02-21 22:45:00 | 2019-02-22 01:45:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '1', '1']         |\n",
      "|  30 | 2019-03-02 05:48:00 | 2019-03-02 07:48:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '1', '1', '1', '1', '3']         |\n",
      "|  31 | 2019-03-15 17:56:00 | 2019-03-16 00:26:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '1', '0', '0', '1', '1', '1']         |\n",
      "|  33 | 2019-04-11 20:35:00 | 2019-04-11 22:35:00 | [0.0, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0]             | ['0', '0', '0', '51', '1', '1', '0', '1']        |\n",
      "|  35 | 2019-05-04 04:48:00 | 2019-05-04 06:48:00 | [0.004, 0.008, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0]         | ['51', '51', '51', '1', '1', '1', '1', '1']      |\n",
      "|  37 | 2019-06-25 04:30:00 | 2019-06-25 06:30:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  38 | 2019-07-02 05:00:00 | 2019-07-02 11:00:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '1', '1', '1', '1', '1']         |\n",
      "|  40 | 2019-07-25 20:01:00 | 2019-07-25 22:02:00 | [0.008, 0.004, 0.02, 0.047, 0.075, 0.11, 0.126, 0.165] | ['51', '51', '53', '55', '61', '63', '63', '63'] |\n",
      "|  41 | 2019-08-06 20:53:00 | 2019-08-06 23:23:00 | [0.008, 0.075, 0.067, 0.035, 0.024, 0.016, 0.012, 0.0] | ['51', '61', '61', '53', '53', '51', '51', '1']  |\n",
      "|  42 | 2019-08-08 03:44:00 | 2019-08-08 10:13:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '1']         |\n",
      "|  43 | 2019-08-22 07:00:00 | 2019-08-22 13:06:00 | [0.008, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]           | ['51', '51', '0', '0', '0', '0', '0', '0']       |\n",
      "|  44 | 2019-10-11 19:25:00 | 2019-10-12 02:00:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.0]             | ['0', '0', '1', '1', '1', '51', '2', '1']        |\n",
      "|  45 | 2019-11-11 12:56:00 | 2019-11-11 14:56:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004]             | ['1', '1', '2', '2', '1', '2', '1', '51']        |\n",
      "|  47 | 2019-12-05 15:29:00 | 2019-12-05 17:29:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '1', '1', '1', '1', '1']         |\n",
      "|  48 | 2019-12-16 22:10:00 | 2019-12-17 00:10:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '1', '1', '1', '2', '2', '1']         |\n",
      "|  49 | 2019-12-20 08:36:00 | 2019-12-20 11:36:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '2', '2', '2', '1', '1', '2', '3']         |\n",
      "|  50 | 2020-01-07 00:19:00 | 2020-01-07 02:19:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  51 | 2020-01-19 13:00:00 | 2020-01-19 15:30:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '0', '1', '1', '1']         |\n",
      "|  53 | 2020-01-29 12:06:00 | 2020-01-29 14:06:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['3', '2', '1', '0', '0', '0', '0', '0']         |\n",
      "|  54 | 2020-02-10 01:03:00 | 2020-02-10 04:03:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '1', '2', '0', '2', '0']         |\n",
      "|  55 | 2020-02-17 13:05:00 | 2020-02-17 15:05:00 | [0.004, 0.004, 0.008, 0.0, 0.0, 0.0, 0.0, 0.0]         | ['51', '51', '51', '3', '1', '1', '2', '0']      |\n",
      "|  56 | 2020-03-07 02:50:00 | 2020-03-07 04:50:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '0', '0', '0']         |\n",
      "|  58 | 2020-03-18 10:11:00 | 2020-03-18 12:11:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '0', '1']         |\n",
      "|  59 | 2020-03-26 15:57:00 | 2020-03-26 20:18:00 | [0.004, 0.004, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0]         | ['51', '51', '51', '0', '0', '1', '2', '1']      |\n",
      "|  60 | 2020-04-22 17:30:00 | 2020-04-22 19:30:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '0', '0', '0', '0', '1']         |\n",
      "|  62 | 2020-05-17 10:14:00 | 2020-05-17 13:14:00 | [0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             | ['51', '3', '0', '0', '1', '1', '1', '0']        |\n",
      "|  64 | 2020-05-30 17:22:00 | 2020-05-30 19:22:00 | [0.004, 0.008, 0.016, 0.004, 0.0, 0.0, 0.0, 0.0]       | ['51', '51', '51', '51', '1', '1', '1', '1']     |\n",
      "|  65 | 2020-06-03 23:25:00 | 2020-06-04 01:25:00 | [0.02, 0.02, 0.236, 0.307, 0.433, 0.303, 0.102, 0.008] | ['53', '53', '63', '65', '65', '65', '63', '51'] |\n",
      "|  66 | 2020-06-13 07:21:00 | 2020-06-13 09:21:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  67 | 2020-06-30 17:58:00 | 2020-06-30 20:11:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '0', '1', '0', '0', '0', '0']         |\n",
      "|  68 | 2020-07-20 19:00:00 | 2020-07-20 21:30:00 | [0.008, 0.004, 0.008, 0.012, 0.004, 0.0, 0.004, 0.004] | ['51', '51', '51', '51', '51', '1', '51', '51']  |\n",
      "|  70 | 2020-08-07 03:12:00 | 2020-08-07 05:12:00 | [0.0, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0]             | ['1', '1', '1', '51', '1', '1', '1', '1']        |\n",
      "|  71 | 2020-07-20 08:30:00 | 2020-07-20 11:50:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  72 | 2020-08-18 12:31:00 | 2020-08-18 14:31:00 | [0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             | ['51', '1', '1', '1', '1', '0', '0', '1']        |\n",
      "|  73 | 2020-08-30 21:19:00 | 2020-08-30 23:19:00 | [0.02, 0.0, 0.0, 0.0, 0.008, 0.039, 0.272, 0.087]      | ['53', '2', '2', '3', '51', '55', '63', '61']    |\n",
      "|  74 | 2020-09-03 10:46:00 | 2020-09-03 12:46:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['2', '1', '1', '0', '0', '0', '0', '0']         |\n",
      "|  79 | 2020-10-06 09:29:00 | 2020-10-06 11:29:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.004]           | ['0', '1', '1', '2', '1', '51', '2', '51']       |\n",
      "|  80 | 2020-10-18 10:26:00 | 2020-10-18 12:26:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '0', '0']         |\n",
      "|  82 | 2020-10-24 13:31:00 | 2020-10-24 15:31:00 | [0.012, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]           | ['51', '51', '1', '1', '1', '2', '2', '2']       |\n",
      "|  84 | 2020-11-05 21:24:00 | 2020-11-05 23:24:00 | [0.0, 0.0, 0.0, 0.0, 0.008, 0.016, 0.008, 0.0]         | ['0', '1', '1', '1', '51', '51', '51', '1']      |\n",
      "|  85 | 2020-11-15 22:25:00 | 2020-11-16 00:27:00 | [0.0, 0.0, 0.0, 0.004, 0.004, 0.008, 0.004, 0.004]     | ['1', '0', '0', '51', '51', '51', '51', '51']    |\n",
      "|  87 | 2020-11-13 19:13:00 | 2020-11-13 22:32:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.004, 0.0]           | ['0', '0', '3', '1', '2', '51', '51', '3']       |\n",
      "|  89 | 2020-11-25 00:13:00 | 2020-11-25 02:13:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '2', '3', '3', '2', '3', '3', '1']         |\n",
      "|  90 | 2020-12-06 14:17:00 | 2020-12-06 16:17:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '2', '2', '0', '1', '0', '0', '0']         |\n",
      "|  93 | 2020-12-09 22:09:00 | 2020-12-10 01:09:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "|  95 | 2020-12-13 14:22:00 | 2020-12-13 17:30:00 | [0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             | ['51', '2', '3', '3', '2', '2', '3', '2']        |\n",
      "|  97 | 2020-12-19 12:00:00 | 2020-12-19 14:00:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['3', '3', '3', '3', '3', '3', '3', '3']         |\n",
      "|  98 | 2021-01-07 23:28:00 | 2021-01-08 02:15:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['2', '1', '1', '2', '2', '1', '3', '2']         |\n",
      "|  99 | 2021-01-20 11:02:00 | 2021-01-20 13:02:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '0', '0', '0', '0', '0', '0']         |\n",
      "| 101 | 2021-01-24 13:00:00 | 2021-01-24 15:00:00 | [0.0, 0.008, 0.008, 0.0, 0.0, 0.0, 0.0, 0.004]         | ['2', '51', '51', '1', '3', '3', '3', '51']      |\n",
      "| 102 | 2021-02-04 04:19:00 | 2021-02-04 06:19:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "| 104 | 2021-02-16 01:59:00 | 2021-02-16 03:59:00 | [0.0, 0.094, 0.126, 0.0, 0.0, 0.0, 0.0, 0.0]           | ['0', '61', '63', '1', '1', '2', '1', '1']       |\n",
      "| 106 | 2021-03-04 06:24:00 | 2021-03-04 08:24:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['3', '3', '3', '3', '3', '3', '2', '0']         |\n",
      "| 107 | 2021-03-11 06:13:00 | 2021-03-11 08:13:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['2', '2', '3', '3', '3', '3', '3', '3']         |\n",
      "| 108 | 2021-03-14 08:01:00 | 2021-03-14 10:01:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '0', '0', '0', '0', '1', '1']         |\n",
      "| 109 | 2021-03-24 06:28:00 | 2021-03-24 08:28:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '0', '1', '1', '1', '1', '1', '0']         |\n",
      "| 110 | 2021-04-07 14:34:00 | 2021-04-07 16:34:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '0', '0', '1', '0', '0', '0']         |\n",
      "| 111 | 2021-04-23 07:49:00 | 2021-04-23 09:49:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['2', '1', '2', '2', '1', '1', '0', '0']         |\n",
      "| 112 | 2021-04-29 01:44:00 | 2021-04-29 03:44:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '2', '1', '3']         |\n",
      "| 113 | 2021-05-04 17:01:00 | 2021-05-04 19:01:00 | [0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0]             | ['2', '2', '51', '0', '0', '1', '1', '1']        |\n",
      "| 114 | 2021-05-09 04:42:00 | 2021-05-09 06:42:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '1', '0', '0', '0', '0', '0']         |\n",
      "| 115 | 2021-05-15 20:56:00 | 2021-05-15 22:56:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012, 0.004]           | ['2', '2', '2', '0', '1', '1', '51', '51']       |\n",
      "| 117 | 2021-05-18 14:31:00 | 2021-05-18 17:37:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '1', '0', '1', '2', '1', '1']         |\n",
      "| 118 | 2021-05-26 16:59:00 | 2021-05-26 18:59:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '1', '2']         |\n",
      "| 119 | 2021-06-03 15:29:00 | 2021-06-03 17:29:00 | [0.0, 0.0, 0.004, 0.016, 0.004, 0.0, 0.008, 0.008]     | ['1', '1', '51', '51', '51', '2', '51', '51']    |\n",
      "| 120 | 2021-06-06 02:26:00 | 2021-06-06 04:26:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '1', '1']         |\n",
      "| 121 | 2021-06-17 14:09:00 | 2021-06-17 16:09:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '2', '1', '1', '1', '1']         |\n",
      "| 123 | 2021-06-30 16:56:00 | 2021-06-30 19:31:00 | [0.008, 0.004, 0.016, 0.0, 0.102, 0.008, 0.008, 0.024] | ['51', '51', '51', '2', '63', '51', '51', '53']  |\n",
      "| 125 | 2021-08-29 05:14:00 | 2021-08-29 07:14:00 | [0.0, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0]             | ['1', '1', '2', '51', '1', '2', '1', '3']        |\n",
      "| 126 | 2021-09-15 22:02:00 | 2021-09-16 00:02:00 | [0.0, 0.0, 0.004, 0.0, 0.0, 0.004, 0.004, 0.004]       | ['1', '1', '51', '0', '0', '51', '51', '51']     |\n",
      "| 127 | 2021-10-16 06:34:00 | 2021-10-16 09:34:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "| 128 | 2021-11-11 00:03:00 | 2021-11-11 02:03:00 | [0.0, 0.012, 0.0, 0.012, 0.012, 0.047, 0.012, 0.004]   | ['1', '51', '2', '51', '51', '55', '51', '51']   |\n",
      "| 130 | 2021-11-13 10:19:00 | 2021-11-13 12:19:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['3', '3', '3', '2', '0', '0', '0', '1']         |\n",
      "| 131 | 2021-12-02 21:12:00 | 2021-12-02 23:12:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '0', '0', '0', '0', '0', '0', '0']         |\n",
      "| 132 | 2021-12-07 06:04:00 | 2021-12-07 10:19:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '0', '0', '0', '0', '1', '0']         |\n",
      "| 133 | 2021-12-09 04:00:00 | 2021-12-09 06:00:00 | [0.0, 0.0, 0.0, 0.0, 0.024, 0.0, 0.0, 0.0]             | ['0', '0', '0', '0', '53', '1', '1', '2']        |\n",
      "| 134 | 2021-12-19 01:58:00 | 2021-12-19 03:58:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['1', '1', '1', '1', '1', '1', '1', '1']         |\n",
      "| 135 | 2021-12-21 08:07:00 | 2021-12-21 10:07:00 | [0.0, 0.008, 0.0, 0.008, 0.106, 0.035, 0.146, 0.142]   | ['2', '51', '3', '51', '63', '53', '63', '63']   |\n",
      "| 136 | 2022-01-06 19:49:00 | 2022-01-06 21:49:00 | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['0', '1', '1', '1', '1', '1', '1', '0']         |\n",
      "| 137 | 2022-01-13 13:25:00 | 2022-01-13 15:25:00 | [0.0, 0.0, 0.0, 0.0, 0.008, 0.004, 0.0, 0.0]           | ['0', '0', '0', '1', '51', '51', '2', '2']       |\n"
     ]
    }
   ],
   "source": [
    "print(launch_corpus[launch_corpus['LAUNCHED'] == 1].head(100)[['START_LCC_EVAL', 'END_LCC_EVAL', 'RAIN', 'WEATHER_CODE']].to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     | START_LCC_EVAL      | END_LCC_EVAL        | REMARKS                                                       | RAIN                                                     | WEATHER_CODE                                     |\n",
      "|----:|:--------------------|:--------------------|:--------------------------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------|\n",
      "|  26 | 2018-12-20 12:03:00 | 2018-12-20 13:20:00 | Wx Scrub, multiple violations                                 | [0.055, 0.012, 0.043, 0.031, 0.031, 0.028, 0.039, 0.031] | ['61', '51', '55', '53', '53', '53', '55', '53'] |\n",
      "|  27 | 2018-12-22 11:55:00 | 2018-12-22 14:20:00 | Upper Level Winds                                             | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                 | ['0', '0', '0', '0', '0', '0', '0', '3']         |\n",
      "|  32 | 2019-04-10 20:35:00 | 2019-04-10 22:24:00 | Upper Level Winds                                             | [0.0, 0.031, 0.039, 0.004, 0.063, 0.055, 0.008, 0.012]   | ['1', '53', '55', '51', '61', '61', '51', '51']  |\n",
      "|  36 | 2019-05-16 00:30:00 | 2019-05-16 02:47:00 | Upper Level Winds                                             | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                 | ['2', '1', '1', '3', '3', '0', '3', '3']         |\n",
      "|  39 | 2019-07-24 20:24:00 | 2019-07-24 22:24:00 | Field Mill/Detached Anvil                                     | [0.004, 0.028, 0.0, 0.004, 0.098, 0.008, 0.004, 0.004]   | ['51', '53', '1', '51', '63', '51', '51', '51']  |\n",
      "|  46 | 2019-12-04 15:52:00 | 2019-12-04 17:00:00 | Upper level winds                                             | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                 | ['0', '0', '0', '0', '0', '2', '1', '2']         |\n",
      "|  52 | 2020-01-27 12:49:00 | 2020-01-27 14:11:00 | Scrubbed during count due to upper level winds                | [0.0, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0]               | ['3', '2', '2', '51', '3', '3', '3', '3']        |\n",
      "|  61 | 2020-05-16 09:24:00 | 2020-05-16 14:22:00 | Scrub due to ground winds & cumulus cloud rule                | [0.004, 0.004, 0.0, 0.004, 0.012, 0.02, 0.016, 0.008]    | ['51', '51', '1', '51', '51', '53', '51', '51']  |\n",
      "|  63 | 2020-05-27 18:33:00 | 2020-05-27 20:17:00 | Scrub due to attached anvil, lightning, field mill violations | [0.024, 0.008, 0.004, 0.004, 0.004, 0.0, 0.0, 0.0]       | ['53', '51', '51', '51', '51', '2', '1', '1']    |\n",
      "|  69 | 2020-07-08 13:59:00 | 2020-07-08 15:58:00 | Scrubbed due to Surface Electric Fields Rule violation        | [0.012, 0.02, 0.008, 0.0, 0.0, 0.0, 0.0, 0.0]            | ['51', '53', '51', '1', '1', '1', '1', '1']      |\n",
      "|  76 | 2020-09-28 12:22:00 | 2020-09-28 14:21:00 | Thick Cloud Layers Rule violation                             | [0.008, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             | ['51', '51', '2', '1', '1', '1', '1', '1']       |\n",
      "|  78 | 2020-10-05 09:51:00 | 2020-10-05 11:14:00 | Cu Cloud violation; balloon icing preventing accurate winds   | [0.012, 0.012, 0.028, 0.004, 0.0, 0.0, 0.004, 0.02]      | ['51', '51', '53', '51', '2', '2', '51', '53']   |\n",
      "| 100 | 2021-01-23 12:40:00 | 2021-01-23 14:40:00 | NONE                                                          | [0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]               | ['51', '3', '3', '1', '1', '1', '1', '0']        |\n",
      "| 124 | 2021-08-28 05:37:00 | 2021-08-28 07:37:00 | Three concurrent LLCC violations at T-0                       | [0.0, 0.0, 0.02, 0.0, 0.0, 0.016, 0.165, 0.008]          | ['1', '2', '53', '2', '2', '51', '63', '51']     |\n",
      "| 129 | 2021-11-12 10:41:00 | 2021-11-12 12:03:00 | Three LLCC violations at T-0                                  | [0.0, 0.0, 0.0, 0.0, 0.016, 0.39, 0.15, 0.043]           | ['1', '2', '1', '1', '51', '65', '63', '55']     |\n",
      "| 140 | 2022-01-27 21:11:00 | 2022-01-27 22:35:00 | Rain/Cu cloud violation entire count                          | [0.012, 0.008, 0.0, 0.004, 0.024, 0.028, 0.0, 0.0]       | ['51', '51', '2', '51', '53', '53', '1', '1']    |\n",
      "| 141 | 2022-01-28 21:11:00 | 2022-01-28 23:03:00 | Thick Cloud Layers, cleared 12m after T-0                     | [0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.024, 0.008]           | ['3', '3', '2', '2', '3', '51', '53', '51']      |\n",
      "| 168 | 2022-06-30 19:00:00 | 2022-06-30 23:34:00 | NONE                                                          | [0.0, 0.004, 0.008, 0.008, 0.012, 0.008, 0.004, 0.004]   | ['2', '51', '51', '51', '51', '51', '51', '51']  |\n",
      "| 183 | 2022-09-14 00:10:00 | 2022-09-14 01:31:00 | NONE                                                          | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.016]             | ['2', '2', '1', '1', '1', '1', '51', '51']       |\n",
      "| 184 | 2022-09-14 23:48:00 | 2022-09-15 00:22:00 | NONE                                                          | [0.004, 0.012, 0.024, 0.043, 0.102, 0.224, 0.008, 0.008] | ['51', '51', '53', '55', '63', '63', '51', '51'] |\n",
      "| 185 | 2022-09-15 23:27:00 | 2022-09-16 01:48:00 | NONE                                                          | [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.024]            | ['53', '3', '3', '1', '1', '2', '61', '53']      |\n",
      "| 186 | 2022-09-16 23:03:00 | 2022-09-17 01:14:00 | NONE                                                          | [0.138, 0.008, 0.004, 0.024, 0.063, 0.071, 0.004, 0.008] | ['63', '51', '51', '53', '61', '61', '51', '51'] |\n",
      "| 201 | 2022-11-22 18:54:00 | 2022-11-22 20:53:00 | NONE                                                          | [0.004, 0.0, 0.008, 0.008, 0.0, 0.0, 0.004, 0.016]       | ['51', '3', '51', '51', '2', '1', '51', '51']    |\n"
     ]
    }
   ],
   "source": [
    "print(launch_corpus[launch_corpus['WX_SCRUB'] == 1].head(100)[['START_LCC_EVAL', 'END_LCC_EVAL', 'REMARKS', 'RAIN', 'WEATHER_CODE']].to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Launches with Weather Code > 60:\n",
      "|     | DATE                | WEATHER_CODE                                     | REMARKS                                                                                            |   WX_SCRUB |   LAUNCHED |\n",
      "|----:|:--------------------|:-------------------------------------------------|:---------------------------------------------------------------------------------------------------|-----------:|-----------:|\n",
      "|  20 | 2018-09-10 00:00:00 | ['51', '61', '63', '2', '1', '51', '1', '1']     | NONE                                                                                               |          0 |          1 |\n",
      "|  26 | 2018-12-20 00:00:00 | ['61', '51', '55', '53', '53', '53', '55', '53'] | Wx Scrub, multiple violations                                                                      |          1 |          0 |\n",
      "|  32 | 2019-04-10 00:00:00 | ['1', '53', '55', '51', '61', '61', '51', '51']  | Upper Level Winds                                                                                  |          1 |          0 |\n",
      "|  34 | 2019-05-03 00:00:00 | ['51', '51', '51', '63', '51', '63', '63', '51'] | Droneship generator malfunction                                                                    |          0 |          0 |\n",
      "|  39 | 2019-07-24 00:00:00 | ['51', '53', '1', '51', '63', '51', '51', '51']  | Field Mill/Detached Anvil                                                                          |          1 |          0 |\n",
      "|  40 | 2019-07-25 00:00:00 | ['51', '51', '53', '55', '61', '63', '63', '63'] | NO/GO until L-:33 for DW/Anvil/FM/Thickcloud                                                       |          0 |          1 |\n",
      "|  41 | 2019-08-06 00:00:00 | ['51', '61', '61', '53', '53', '51', '51', '1']  | T-0 delayed 30 minutes due to weather                                                              |          0 |          1 |\n",
      "|  65 | 2020-06-04 00:00:00 | ['53', '53', '63', '65', '65', '65', '63', '51'] | NONE                                                                                               |          0 |          1 |\n",
      "|  73 | 2020-08-30 00:00:00 | ['53', '2', '2', '3', '51', '55', '63', '61']    | NONE                                                                                               |          0 |          1 |\n",
      "|  92 | 2020-09-30 00:00:00 | ['51', '53', '53', '61', '61', '63', '53', '61'] | Abort                                                                                              |          0 |          0 |\n",
      "| 104 | 2021-02-16 00:00:00 | ['0', '61', '63', '1', '1', '2', '1', '1']       | NONE                                                                                               |          0 |          1 |\n",
      "| 122 | 2021-06-29 00:00:00 | ['53', '51', '1', '51', '1', '61', '2', '1']     | Scrub at L-11s due to helicopter breaching airspace                                                |          0 |          0 |\n",
      "| 123 | 2021-06-30 00:00:00 | ['51', '51', '51', '2', '63', '51', '51', '53']  | NONE                                                                                               |          0 |          1 |\n",
      "| 124 | 2021-08-28 00:00:00 | ['1', '2', '53', '2', '2', '51', '63', '51']     | Three concurrent LLCC violations at T-0                                                            |          1 |          0 |\n",
      "| 129 | 2021-11-12 00:00:00 | ['1', '2', '1', '1', '51', '65', '63', '55']     | Three LLCC violations at T-0                                                                       |          1 |          0 |\n",
      "| 135 | 2021-12-21 00:00:00 | ['2', '51', '3', '51', '63', '53', '63', '63']   | NONE                                                                                               |          0 |          1 |\n",
      "| 152 | 2022-03-19 00:00:00 | ['63', '1', '0', '0', '0', '2', '1', '1']        | Two T-0s, targeted 2nd due to weather threat                                                       |          0 |          1 |\n",
      "| 153 | 2022-04-01 00:00:00 | ['3', '3', '51', '65', '63', '61', '61', '51']   | NONE                                                                                               |          0 |          1 |\n",
      "| 171 | 2022-07-15 00:00:00 | ['0', '1', '61', '1', '1', '1', '1', '1']        | NONE                                                                                               |          0 |          1 |\n",
      "| 172 | 2022-07-17 00:00:00 | ['63', '63', '1', '51', '63', '2', '0', '1']     | NONE                                                                                               |          0 |          1 |\n",
      "| 178 | 2022-08-28 00:00:00 | ['63', '51', '63', '3', '51', '51', '55', '55']  | NONE                                                                                               |          0 |          1 |\n",
      "| 179 | 2022-08-29 00:00:00 | ['61', '2', '1', '1', '1', '1', '1', '51']       | Tanking delayed by approx 1h due to lightning risk; multiple leak/valve issues upon entering count |          0 |          0 |\n",
      "| 184 | 2022-09-15 00:00:00 | ['51', '51', '53', '55', '63', '63', '51', '51'] | NONE                                                                                               |          1 |          0 |\n",
      "| 185 | 2022-09-16 00:00:00 | ['53', '3', '3', '1', '1', '2', '61', '53']      | NONE                                                                                               |          1 |          0 |\n",
      "| 186 | 2022-09-17 00:00:00 | ['63', '51', '51', '53', '61', '61', '51', '51'] | NONE                                                                                               |          1 |          0 |\n",
      "| 213 | 2023-01-26 00:00:00 | ['3', '3', '61', '61', '51', '2', '0', '0']      | NONE                                                                                               |          0 |          1 |\n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where any weather code in the list is above 60\n",
    "high_weather_code = launch_corpus[launch_corpus['WEATHER_CODE'].apply(lambda x: any(int(code) > 60 for code in x))]\n",
    "print(\"\\nLaunches with Weather Code > 60:\")\n",
    "print(high_weather_code[['DATE', 'WEATHER_CODE', 'REMARKS', 'WX_SCRUB', 'LAUNCHED']].to_markdown())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates a launch report visualization for a specified date, highlighting weather and launch conditions with flexible subplots, labeled axes, status indicators, and formatted time labels. It also includes color-coded statuses, launch window highlighting, and threshold lines for conditions like wind speed and shear, providing a weather and status overview for launch decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_launch_report(launch_corpus, target_date):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive visualization report for a specific launch date with enhanced formatting\n",
    "    and visual elements.\n",
    "\n",
    "    Args:\n",
    "        launch_corpus (pd.DataFrame): The main dataframe containing all launch data\n",
    "        target_date: The date to analyze (can be string 'YYYY-MM-DD' or datetime.date)\n",
    "    \"\"\"\n",
    "    # Convert target_date to datetime.date if it's a string\n",
    "    if isinstance(target_date, str):\n",
    "        target_date = pd.to_datetime(target_date).date()\n",
    "\n",
    "    # Get launch data for target date - match on just the date portion of DATETIME\n",
    "    launch_data = launch_corpus[launch_corpus['DATE'].dt.date == target_date].iloc[0]\n",
    "\n",
    "    # Create figure with GridSpec for flexible subplot layout\n",
    "    fig = plt.figure(figsize=(15, 20))\n",
    "    gs = fig.add_gridspec(8, 2, hspace=1.0, wspace=0.8)  # Increased spacing between columns\n",
    "\n",
    "    # Enhanced title formatting\n",
    "    fig.text(0.5, 0.98, f\"Launch Report for {target_date}\", \n",
    "             fontsize=16, weight='bold', ha='center')\n",
    "    fig.text(0.5, 0.96, f\"Vehicle: {launch_data['LAUNCH_VEHICLE']}\", \n",
    "             fontsize=12, ha='center')\n",
    "\n",
    "    # Status with color coding\n",
    "    status_text = 'Weather Scrub' if launch_data['WX_SCRUB'] else 'Weather Delay' if launch_data['WX DELAY'] else 'Nominal'\n",
    "    status_color = 'red' if launch_data['WX_SCRUB'] else 'orange' if launch_data['WX DELAY'] else 'green'\n",
    "    fig.text(0.5, 0.95, f\"Status: {status_text}\", \n",
    "             fontsize=12, ha='center', color=status_color)\n",
    "\n",
    "    # Remarks in italics\n",
    "    fig.text(0.5, 0.94, f\"Remarks: {launch_data['REMARKS']}\", \n",
    "             fontsize=10, style='italic', ha='center')\n",
    "\n",
    "    def add_launch_window_highlight(ax):\n",
    "        \"\"\"Helper function to add launch window highlighting\"\"\"\n",
    "        start_time = pd.to_datetime(launch_data['START_LCC_EVAL'])\n",
    "        end_time = pd.to_datetime(launch_data['END_LCC_EVAL'])\n",
    "\n",
    "        # Convert times to x-axis positions\n",
    "        start_idx = 7 - (end_time - start_time).seconds // 3600\n",
    "        end_idx = 7\n",
    "\n",
    "        # Add transparent highlight\n",
    "        ylims = ax.get_ylim()\n",
    "        ax.axvspan(start_idx, end_idx, color='yellow', alpha=0.2, label='Launch Window')\n",
    "        ax.set_ylim(ylims)  # Restore y-limits after adding span\n",
    "\n",
    "    def setup_axis(ax, title, ylabel=None):\n",
    "        \"\"\"Helper function to setup consistent axis formatting\"\"\"\n",
    "        ax.set_title(title, pad=10, fontsize=10, weight='bold')\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        add_launch_window_highlight(ax)\n",
    "        # Add left padding for y-axis labels\n",
    "        ax.yaxis.set_label_coords(-0.15, 0.5)\n",
    "\n",
    "    # Plot weather parameters\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(range(8), launch_data['TEMPERATURE'], marker='o')\n",
    "    setup_axis(ax1, 'Temperature', '°F')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(range(8), launch_data['CLOUD_COVER'], marker='o')\n",
    "    setup_axis(ax2, 'Cloud Cover', '%')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Wind speeds at different heights\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3.plot(range(8), launch_data['WIND_SPEED_10M'], label='10m', marker='o')\n",
    "    ax3.plot(range(8), launch_data['WIND_SPEED_100M'], label='100m', marker='o')\n",
    "    setup_axis(ax3, 'Surface Wind Speed', 'MP/H')\n",
    "    ax3.set_ylim(0, 30)\n",
    "    ax3.legend(bbox_to_anchor=(1.12, 1))\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Field Mill readings\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.plot(range(8), launch_data['FIELD_MILL_MAX'], label='Max', marker='o')\n",
    "    ax4.plot(range(8), launch_data['FIELD_MILL_MEAN'], label='Mean', marker='o')\n",
    "    ax4.axhline(y=1500, color='r', linestyle='--', label='Critical (1500)')\n",
    "    ax4.axhline(y=500, color='y', linestyle='--', label='Warning (500)')\n",
    "    setup_axis(ax4, 'Field Mill Readings')\n",
    "    # Set y-axis limit based on max value with padding\n",
    "    max_value = max(max(launch_data['FIELD_MILL_MAX']), 1500)  # At least show critical line\n",
    "    y_max = max_value * 1.2  # Add 20% padding above max value\n",
    "    ax4.set_ylim(0, y_max)\n",
    "    ax4.legend(bbox_to_anchor=(1.12, 1))\n",
    "    ax4.grid(True)\n",
    "\n",
    "    # Wind profiles for each atmospheric layer\n",
    "    layers = ['BOUNDARY_LAYER', 'LOW_TROPOSPHERE', 'MID_TROPOSPHERE', \n",
    "              'UPPER_TROPOSPHERE', 'STRATOSPHERE']\n",
    "\n",
    "    # Speed plots\n",
    "    ax5 = fig.add_subplot(gs[2:4, 0])\n",
    "    max_speed = 0\n",
    "    for layer in layers:\n",
    "        speed_data = launch_data[f'WIND_SPEED_MAX_PEAK_{layer}']\n",
    "        max_speed = max(max_speed, max(speed_data))\n",
    "        ax5.plot(range(8), speed_data, \n",
    "                label=layer.replace('_', ' ').title(), marker='o')\n",
    "    setup_axis(ax5, 'Wind Speed by Atmospheric Layer', 'MP/H')\n",
    "    ax5.set_ylim(0, max_speed * 1.2)  # Add 20% padding above max value\n",
    "    ax5.legend(bbox_to_anchor=(1.12, 1))\n",
    "    ax5.grid(True)\n",
    "\n",
    "    # Shear plots\n",
    "    ax6 = fig.add_subplot(gs[2:4, 1])\n",
    "    max_shear = 0\n",
    "    for layer in layers:\n",
    "        shear_data = launch_data[f'WIND_SHEAR_MAX_PEAK_{layer}']\n",
    "        max_shear = max(max_shear, max(shear_data))\n",
    "        ax6.plot(range(8), shear_data, \n",
    "                label=layer.replace('_', ' ').title(), marker='o')\n",
    "    setup_axis(ax6, 'Wind Shear by Atmospheric Layer')\n",
    "    ax6.set_ylim(0, max_shear * 1.2)  # Add 20% padding above max value\n",
    "\n",
    "    # Add unacceptable threshold lines\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']  # Standard matplotlib colors\n",
    "    ax6.axhline(y=0.08, color=colors[0], linestyle=':', label='BL Unacceptable (>0.08)', linewidth=0.8)\n",
    "    ax6.axhline(y=0.09, color=colors[1], linestyle=':', label='LT Unacceptable (>0.09)', linewidth=0.8)\n",
    "    ax6.axhline(y=0.12, color=colors[2], linestyle=':', label='MT Unacceptable (>0.12)', linewidth=0.8)\n",
    "    ax6.axhline(y=0.14, color=colors[3], linestyle=':', label='UT Unacceptable (>0.14)', linewidth=0.8)\n",
    "    ax6.axhline(y=0.16, color=colors[4], linestyle=':', label='S Unacceptable (>0.16)', linewidth=0.8)\n",
    "    ax6.set_ylim(0, max(max_shear * 1.2, 0.18))  # Ensure threshold lines are visible\n",
    "    ax6.legend(bbox_to_anchor=(1.12, 1), fontsize='x-small')\n",
    "    ax6.grid(True)\n",
    "\n",
    "    # Additional parameters\n",
    "    ax7 = fig.add_subplot(gs[4, 0])\n",
    "    ax7.plot(range(8), launch_data['PRESSURE'], marker='o')\n",
    "    setup_axis(ax7, 'Pressure', 'hPa')\n",
    "    ax7.set_ylim(900, 1100)\n",
    "    ax7.grid(True)\n",
    "\n",
    "    ax8 = fig.add_subplot(gs[4, 1])\n",
    "    ax8.plot(range(8), launch_data['HUMIDITY'], marker='o')\n",
    "    setup_axis(ax8, 'Humidity', '%')\n",
    "    ax8.set_ylim(0, 100)\n",
    "    ax8.grid(True)\n",
    "\n",
    "    # Format x-axes for all subplots\n",
    "    eval_end = pd.to_datetime(launch_data['END_LCC_EVAL'])\n",
    "    time_labels = [(eval_end - pd.Timedelta(hours=x)).strftime('%H:%M') for x in range(7, -1, -1)]\n",
    "\n",
    "    for ax in [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]:\n",
    "        ax.set_xticks(range(8))\n",
    "        ax.set_xticklabels(time_labels, rotation=45)\n",
    "        ax.set_xlabel('Time (GMT+0)')\n",
    "        ax.grid(True, alpha=0.3)  # Lighter grid\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
    "    return fig\n",
    "\n",
    "# Usage:\n",
    "fig = plot_launch_report(launch_corpus, '2019-07-24')  # Field Mill Scrub: '2019-07-24', Wind Scrub: 2018-12-22, Cloud Scrub: '2022-01-28'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wind speed density and violin plots for each atmospheric layer\n",
    "\n",
    "# Define atmospheric layers\n",
    "layers = ['BOUNDARY_LAYER', 'LOW_TROPOSPHERE', 'MID_TROPOSPHERE', \n",
    "          'UPPER_TROPOSPHERE', 'STRATOSPHERE']\n",
    "\n",
    "# Create figure with subplots - 2 columns, one for density plots and one for violin plots\n",
    "fig = plt.figure(figsize=(24, 20))\n",
    "gs = gridspec.GridSpec(len(layers), 2, width_ratios=[1, 1.5])\n",
    "\n",
    "# Find global max wind speed across all layers and launches\n",
    "max_wind_speed = 0\n",
    "for layer in layers:\n",
    "    wind_speeds = launch_corpus[f'WIND_SPEED_MAX_PEAK_{layer}']\n",
    "    layer_max = max(max(wind_speeds))\n",
    "    max_wind_speed = max(max_wind_speed, layer_max)\n",
    "\n",
    "# Plot wind speed density and violin plots for each layer\n",
    "for layer_idx, layer in enumerate(layers):\n",
    "    # Density plot\n",
    "    ax_density = fig.add_subplot(gs[layer_idx, 0])\n",
    "\n",
    "    # Separate wind issue and no wind issue data for density\n",
    "    wind_issue_data = []\n",
    "    no_wind_issue_data = []\n",
    "\n",
    "    for idx, row in launch_corpus.iterrows():\n",
    "        data = row[f'WIND_SPEED_MAX_PEAK_{layer}']\n",
    "        if row['REMARKS_CATEGORY'] == 'wind':\n",
    "            wind_issue_data.extend(data)\n",
    "        else:\n",
    "            no_wind_issue_data.extend(data)\n",
    "\n",
    "    # Create density plots\n",
    "    if wind_issue_data:\n",
    "        sns.kdeplot(data=wind_issue_data, ax=ax_density, color='red', label='Wind Issue', fill=True, alpha=0.3)\n",
    "    if no_wind_issue_data:\n",
    "        sns.kdeplot(data=no_wind_issue_data, ax=ax_density, color='blue', label='No Wind Issue', fill=True, alpha=0.3)\n",
    "\n",
    "    # Set density plot formatting\n",
    "    ax_density.set_title(f'{layer.replace(\"_\", \" \").title()} - Distribution')\n",
    "    ax_density.set_xlabel('Wind Speed (mph)')\n",
    "    ax_density.set_ylabel('Density')\n",
    "    ax_density.grid(True)\n",
    "    ax_density.legend()\n",
    "    ax_density.set_xlim(0, max_wind_speed * 1.1)\n",
    "\n",
    "    # Violin plot\n",
    "    ax_violin = fig.add_subplot(gs[layer_idx, 1])\n",
    "\n",
    "    # Prepare data for violin plot\n",
    "    violin_data = []\n",
    "    violin_categories = []\n",
    "    violin_times = []\n",
    "\n",
    "    for idx, row in launch_corpus.iterrows():\n",
    "        data = row[f'WIND_SPEED_MAX_PEAK_{layer}']\n",
    "        category = 'Wind Issue' if row['REMARKS_CATEGORY'] == 'wind' else 'No Wind Issue'\n",
    "        for hour, value in enumerate(data):\n",
    "            violin_data.append(value)\n",
    "            violin_categories.append(category)\n",
    "            violin_times.append(f'T-{7-hour}')\n",
    "\n",
    "    # Create violin plot\n",
    "    violin_df = pd.DataFrame({\n",
    "        'Wind Speed': violin_data,\n",
    "        'Category': violin_categories,\n",
    "        'Time': violin_times\n",
    "    })\n",
    "\n",
    "    sns.violinplot(data=violin_df, x='Time', y='Wind Speed', hue='Category',\n",
    "                  ax=ax_violin, split=True, inner='quartile',\n",
    "                  palette={'Wind Issue': 'red', 'No Wind Issue': 'blue'})\n",
    "\n",
    "    # Set violin plot formatting\n",
    "    ax_violin.set_title(f'{layer.replace(\"_\", \" \").title()} - Time Series Distribution')\n",
    "    ax_violin.set_xlabel('Hours Before Launch')\n",
    "    ax_violin.set_ylabel('Wind Speed (mph)')\n",
    "    ax_violin.grid(True)\n",
    "    ax_violin.set_ylim(0, max_wind_speed * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Atmospheric Layer | Wind Speed Threshold | Key Observations |\n",
    "|------------------|---------------------|------------------|\n",
    "| Boundary Layer | 25-30 MPH | - Sharp cutoff in safe launches beyond 30 MPH<br>- Most restrictive layer overall<br>- Very clear distinction between safe/unsafe distributions |\n",
    "| Low Troposphere | 50-75 MPH | - Wider tolerance than boundary layer<br>- Gradual increase in issues rather than sharp cutoff<br>- Shows bimodal distribution in wind issues |\n",
    "| Mid Troposphere | 75-100 MPH | - Significantly higher tolerance<br>- Broad distribution of safe launches<br>- Wind issues spread across wider range |\n",
    "| Upper Troposphere | 100-125 MPH | - Highest wind speed tolerance of all layers<br>- Very wide distribution<br>- Notable overlap between safe/unsafe regions |\n",
    "| Stratosphere | 40-50 MPH | - More restrictive than middle layers<br>- Tighter distribution overall<br>- Clear peak in safe launch window |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot field mill density and violin plots\n",
    "\n",
    "# Create figure with subplots - 2 columns, one for density plots and one for violin plots\n",
    "fig = plt.figure(figsize=(24, 10))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1.5])\n",
    "\n",
    "# Find global max field mill value\n",
    "max_field_mill = max(max(row['FIELD_MILL_MAX']) for _, row in launch_corpus.iterrows())\n",
    "\n",
    "# Density plot\n",
    "ax_density = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Separate lightning issue and no lightning issue data for density\n",
    "lightning_issue_data = []\n",
    "no_lightning_issue_data = []\n",
    "\n",
    "for idx, row in launch_corpus.iterrows():\n",
    "    data = row['FIELD_MILL_MAX']\n",
    "    if row['REMARKS_CATEGORY'] == 'lightning':\n",
    "        lightning_issue_data.extend(data)\n",
    "    else:\n",
    "        no_lightning_issue_data.extend(data)\n",
    "\n",
    "# Create density plots\n",
    "if lightning_issue_data:\n",
    "    sns.kdeplot(data=lightning_issue_data, ax=ax_density, color='red', label='Lightning Issue', fill=True, alpha=0.3)\n",
    "if no_lightning_issue_data:\n",
    "    sns.kdeplot(data=no_lightning_issue_data, ax=ax_density, color='blue', label='No Lightning Issue', fill=True, alpha=0.3)\n",
    "\n",
    "# Set density plot formatting\n",
    "ax_density.set_title('Field Mill Readings - Distribution')\n",
    "ax_density.set_xlabel('Field Mill Reading (V/m)')\n",
    "ax_density.set_ylabel('Density')\n",
    "ax_density.grid(True)\n",
    "ax_density.legend()\n",
    "ax_density.set_xlim(0, max_field_mill * 1.1)\n",
    "\n",
    "# x-axis ticks\n",
    "ax_density.xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "\n",
    "# Violin plot\n",
    "ax_violin = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "# Prepare data for violin plot\n",
    "violin_data = []\n",
    "violin_categories = []\n",
    "violin_times = []\n",
    "\n",
    "for idx, row in launch_corpus.iterrows():\n",
    "    data = row['FIELD_MILL_MAX']\n",
    "    category = 'Lightning Issue' if row['REMARKS_CATEGORY'] == 'lightning' else 'No Lightning Issue'\n",
    "    for hour, value in enumerate(data):\n",
    "        violin_data.append(value)\n",
    "        violin_categories.append(category)\n",
    "        violin_times.append(f'T-{7-hour}')\n",
    "\n",
    "# Create violin plot\n",
    "violin_df = pd.DataFrame({\n",
    "    'Field Mill': violin_data,\n",
    "    'Category': violin_categories,\n",
    "    'Time': violin_times\n",
    "})\n",
    "\n",
    "sns.violinplot(data=violin_df, x='Time', y='Field Mill', hue='Category',\n",
    "              ax=ax_violin, split=True, inner='quartile',\n",
    "              palette={'Lightning Issue': 'red', 'No Lightning Issue': 'blue'})\n",
    "\n",
    "# Set violin plot formatting\n",
    "ax_violin.set_title('Field Mill Readings - Time Series Distribution')\n",
    "ax_violin.set_xlabel('Hours Before Launch')\n",
    "ax_violin.set_ylabel('Field Mill Reading (V/m)')\n",
    "ax_violin.grid(True)\n",
    "ax_violin.set_ylim(0, max_field_mill * 1.1)\n",
    "\n",
    "# Add more y-axis ticks\n",
    "ax_violin.yaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field Mill Reading Range | Risk Assessment | Observations |\n",
    "|-------------------------|-----------------|--------------|\n",
    "| 0-600 V/m | Generally Safe | - Majority of successful launches<br>- Optimal range for launch<br>- High confidence zone |\n",
    "| 600-3000 V/m | Increased Risk | - Transition zone<br>- Requires careful monitoring<br>- May indicate developing conditions |\n",
    "| 3000+ V/m | High Risk | - Strong indicator of lightning risk<br>- Rare in successful launches<br>- Likely launch scrub condition |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cloud cover density and violin plots\n",
    "\n",
    "# Create figure with subplots - 2 columns, one for density plots and one for violin plots\n",
    "fig = plt.figure(figsize=(24, 10))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1.5])\n",
    "\n",
    "# Find global max cloud cover value\n",
    "max_cloud_cover = max(max(row['CLOUD_COVER']) for _, row in launch_corpus.iterrows())\n",
    "\n",
    "# Density plot\n",
    "ax_density = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Separate cloud issue and no cloud issue data for density\n",
    "cloud_issue_data = []\n",
    "no_cloud_issue_data = []\n",
    "\n",
    "for idx, row in launch_corpus.iterrows():\n",
    "    data = row['CLOUD_COVER']\n",
    "    if row['REMARKS_CATEGORY'] == 'clouds':\n",
    "        cloud_issue_data.extend(data)\n",
    "    else:\n",
    "        no_cloud_issue_data.extend(data)\n",
    "\n",
    "# Create density plots\n",
    "if cloud_issue_data:\n",
    "    sns.kdeplot(data=cloud_issue_data, ax=ax_density, color='red', label='Cloud Issue', fill=True, alpha=0.3)\n",
    "if no_cloud_issue_data:\n",
    "    sns.kdeplot(data=no_cloud_issue_data, ax=ax_density, color='blue', label='No Cloud Issue', fill=True, alpha=0.3)\n",
    "\n",
    "# Set density plot formatting\n",
    "ax_density.set_title('Cloud Cover - Distribution')\n",
    "ax_density.set_xlabel('Cloud Cover (%)')\n",
    "ax_density.set_ylabel('Density')\n",
    "ax_density.grid(True)\n",
    "ax_density.legend()\n",
    "ax_density.set_xlim(0, max_cloud_cover * 1.1)\n",
    "\n",
    "# x-axis ticks\n",
    "ax_density.xaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "\n",
    "# Violin plot\n",
    "ax_violin = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "# Prepare data for violin plot\n",
    "violin_data = []\n",
    "violin_categories = []\n",
    "violin_times = []\n",
    "\n",
    "for idx, row in launch_corpus.iterrows():\n",
    "    data = row['CLOUD_COVER']\n",
    "    category = 'Cloud Issue' if row['REMARKS_CATEGORY'] == 'clouds' else 'No Cloud Issue'\n",
    "    for hour, value in enumerate(data):\n",
    "        violin_data.append(value)\n",
    "        violin_categories.append(category)\n",
    "        violin_times.append(f'T-{7-hour}')\n",
    "\n",
    "# Create violin plot\n",
    "violin_df = pd.DataFrame({\n",
    "    'Cloud Cover': violin_data,\n",
    "    'Category': violin_categories,\n",
    "    'Time': violin_times\n",
    "})\n",
    "\n",
    "sns.violinplot(data=violin_df, x='Time', y='Cloud Cover', hue='Category',\n",
    "              ax=ax_violin, split=True, inner='quartile',\n",
    "              palette={'Cloud Issue': 'red', 'No Cloud Issue': 'blue'})\n",
    "\n",
    "# Set violin plot formatting\n",
    "ax_violin.set_title('Cloud Cover - Time Series Distribution')\n",
    "ax_violin.set_xlabel('Hours Before Launch')\n",
    "ax_violin.set_ylabel('Cloud Cover (%)')\n",
    "ax_violin.grid(True)\n",
    "ax_violin.set_ylim(0, max_cloud_cover * 1.1)\n",
    "\n",
    "# Add more y-axis ticks\n",
    "ax_violin.yaxis.set_major_locator(plt.MaxNLocator(20))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Cloud Cover Range | Risk Assessment | Observations |\n",
    "|------------------|-----------------|--------------|\n",
    "| 0-15% | Optimal | - First safe zone peak<br>- Highly favorable for launch<br>- Clearest viewing conditions |\n",
    "| 15-35% | Generally Safe | - Second safe zone peak<br>- Still favorable conditions<br>- Acceptable launch window |\n",
    "| 35-50% | Increased Risk | - Transition zone<br>- Beginning of cloud issues<br>- Requires careful evaluation |\n",
    "| >50% | High Risk | - Dominated by cloud issues<br>- Secondary risk peak at 85-90%<br>- Likely launch constraint |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(launch_corpus.sample(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 means no-go (scrubbed), 0 means go (not scrubbed)\n",
    "launch_corpus['NOGO_FLAG'] = launch_corpus['WX_SCRUB'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "temp_features = ['TEMPERATURE']\n",
    "precip_features = ['RAIN']\n",
    "cloud_features = ['CLOUD_COVER']\n",
    "field_mill_features = ['FIELD_MILL_MAX']\n",
    "wind_speed_features = ['WIND_SPEED_10M', 'WIND_SPEED_100M', 'WIND_SPEED_MAX_PEAK_BOUNDARY_LAYER', 'WIND_SPEED_MAX_PEAK_LOW_TROPOSPHERE', 'WIND_SPEED_MAX_PEAK_MID_TROPOSPHERE', 'WIND_SPEED_MAX_PEAK_UPPER_TROPOSPHERE', 'WIND_SPEED_MAX_PEAK_STRATOSPHERE']\n",
    "wind_shear_features = ['WIND_SHEAR_MAX_PEAK_BOUNDARY_LAYER', 'WIND_SHEAR_MAX_PEAK_LOW_TROPOSPHERE', 'WIND_SHEAR_MAX_PEAK_MID_TROPOSPHERE', 'WIND_SHEAR_MAX_PEAK_UPPER_TROPOSPHERE', 'WIND_SHEAR_MAX_PEAK_STRATOSPHERE']\n",
    "weather_code_features = ['WEATHER_CODE']\n",
    "pressure_features = ['PRESSURE']\n",
    "humidity_features = ['HUMIDITY']\n",
    "\n",
    "# Combine features\n",
    "features = temp_features + precip_features + cloud_features + field_mill_features + wind_speed_features + wind_shear_features + weather_code_features + pressure_features + humidity_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure selected features are consistent in time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values in the features\n",
    "missing_values = launch_corpus[features].isnull().sum()\n",
    "print(\"\\nMissing values in features:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check for infinite values\n",
    "# First convert numeric columns to float to handle inf values\n",
    "numeric_features = launch_corpus[features].select_dtypes(include=['int64', 'float64']).columns\n",
    "infinite_values = pd.Series(0, index=features)  # Initialize with zeros for all features\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    infinite_values[numeric_features] = np.isinf(launch_corpus[numeric_features].replace([np.inf, -np.inf], np.nan)).sum()\n",
    "\n",
    "print(\"\\nInfinite values in features:\")\n",
    "print(infinite_values[infinite_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan launch_corpus for array-like entries with NaNs, convert infinities to NaNs, fill NaNs in arrays using edge fill and middle interpolation, log rows with filled NaNs, drop features if entirely NaN, ensure no NaNs remain, and record the new dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_array_nans_in_features(launch_corpus, features):\n",
    "    \"\"\"\n",
    "    Handle NaN values in array-like features by:\n",
    "    1. Checking for NaN values\n",
    "    2. Converting infinities to NaNs\n",
    "    3. Filling NaNs using interpolation and edge filling\n",
    "    4. Dropping rows with all NaN features\n",
    "    5. Verifying results\n",
    "\n",
    "    Args:\n",
    "        launch_corpus: DataFrame containing the launch data\n",
    "        features: List of feature column names to process\n",
    "\n",
    "    Returns:\n",
    "        Processed DataFrame with NaNs handled\n",
    "    \"\"\"\n",
    "    # 1. First check for NaN values\n",
    "    array_nan_data = []\n",
    "    for idx, row in launch_corpus[features].iterrows():\n",
    "        nan_features = {}\n",
    "        for feature in features:\n",
    "            value = row[feature]\n",
    "            if isinstance(value, (list, np.ndarray)):\n",
    "                if any(pd.isna(x) for x in value):\n",
    "                    nan_count = sum(pd.isna(x) for x in value)\n",
    "                    nan_features[feature] = f\"{nan_count} NaN values\"\n",
    "        if nan_features:\n",
    "            nan_features['Row'] = idx\n",
    "            array_nan_data.append(nan_features)\n",
    "\n",
    "    if array_nan_data:\n",
    "        print(\"\\nInitial NaN values in array features:\")\n",
    "        print(pd.DataFrame(array_nan_data).set_index('Row').to_markdown())\n",
    "    else:\n",
    "        print(\"\\nNo initial NaN values found in array features\")\n",
    "\n",
    "    # 2. Replace infinities with NaN\n",
    "    def replace_inf_with_nan(arr):\n",
    "        if isinstance(arr, (list, np.ndarray)):\n",
    "            arr = np.array(arr, dtype=float)\n",
    "            arr[np.isinf(arr)] = np.nan\n",
    "            return arr\n",
    "        return arr\n",
    "\n",
    "    for feature in features:\n",
    "        launch_corpus[feature] = launch_corpus[feature].apply(replace_inf_with_nan)\n",
    "\n",
    "    # 3. Fill NaN values\n",
    "    def fill_array_nans(arr):\n",
    "        if not isinstance(arr, (list, np.ndarray)):\n",
    "            return arr\n",
    "\n",
    "        try:\n",
    "            arr = np.array(arr, dtype=float)\n",
    "        except (ValueError, TypeError):\n",
    "            return arr\n",
    "\n",
    "        if np.all(np.isnan(arr)):\n",
    "            return None\n",
    "\n",
    "        series = pd.Series(arr)\n",
    "        filled = series.interpolate(method='linear')\n",
    "        filled = filled.fillna(method='ffill').fillna(method='bfill')\n",
    "        return filled.values\n",
    "\n",
    "    initial_length = len(launch_corpus)\n",
    "\n",
    "    for feature in features:\n",
    "        launch_corpus[feature] = launch_corpus[feature].apply(fill_array_nans)\n",
    "\n",
    "    # 4. Drop rows where any feature returned None\n",
    "    launch_corpus = launch_corpus.dropna(subset=features)\n",
    "\n",
    "    # 5. Verify results\n",
    "    array_nan_data = []\n",
    "    for idx, row in launch_corpus[features].iterrows():\n",
    "        nan_features = {}\n",
    "        for feature in features:\n",
    "            value = row[feature]\n",
    "            if isinstance(value, (list, np.ndarray)):\n",
    "                if any(pd.isna(x) for x in value):\n",
    "                    nan_count = sum(pd.isna(x) for x in value)\n",
    "                    nan_features[feature] = f\"{nan_count} NaN values\"\n",
    "        if nan_features:\n",
    "            nan_features['Row'] = idx\n",
    "            array_nan_data.append(nan_features)\n",
    "\n",
    "    if array_nan_data:\n",
    "        print(\"\\nRemaining NaN values after filling:\")\n",
    "        print(pd.DataFrame(array_nan_data).set_index('Row').to_markdown())\n",
    "    else:\n",
    "        print(\"\\nAll NaN values have been successfully handled\")\n",
    "\n",
    "    print(f\"\\nDataset length: {len(launch_corpus)} rows (removed {initial_length - len(launch_corpus)} rows)\")\n",
    "\n",
    "    return launch_corpus\n",
    "\n",
    "launch_corpus = handle_array_nans_in_features(launch_corpus, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter weather dataset to include only data from 2018 onwards and rename specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to simplified names and filter for dates after 2018\n",
    "weather_filtered = weather[weather['TIME'].dt.year > 2017].rename(columns={\n",
    "    'TEMPERATURE_2M (°F)': 'TEMPERATURE',\n",
    "    'PRECIPITATION (INCH)': 'PRECIPITATION', \n",
    "    'RAIN (INCH)': 'RAIN',\n",
    "    'CLOUD_COVER (%)': 'CLOUD_COVER',\n",
    "    'WEATHER_CODE (WMO CODE)': 'WEATHER_CODE',\n",
    "    'PRESSURE_MSL (HPA)': 'PRESSURE',\n",
    "    'RELATIVE_HUMIDITY_2M (%)': 'HUMIDITY',\n",
    "    'WIND_SPEED_10M (MP/H)': 'WIND_SPEED_10M',\n",
    "    'WIND_SPEED_100M (MP/H)': 'WIND_SPEED_100M'\n",
    "})\n",
    "weather_filtered = weather_filtered[['TIME', 'TEMPERATURE', 'RAIN', 'CLOUD_COVER', 'WEATHER_CODE', 'PRESSURE', 'HUMIDITY', 'WIND_SPEED_10M', 'WIND_SPEED_100M']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregates hourly maximum values from multiple field mill sensors into a single maximum value per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate field mill data by hour, taking max across sensors\n",
    "field_mill_agg = field_mill.groupby('DATETIME')['MAX'].max().reset_index().rename(columns={'MAX': 'FIELD_MILL_MAX'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms wind profile data by pivoting it to create separate columns for each atmospheric layer, focusing on maximum wind speed and shear, then flattens the resulting multi-index column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot wind profiles to get layer-specific columns\n",
    "wind_profiles_wide = binned_wind_profiles.pivot(\n",
    "    index='DATETIME',\n",
    "    columns='ATMOSPHERIC_LAYER',\n",
    "    values=['SPEED_MAX_PEAK', 'SHEAR_MAX_PEAK']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "wind_profiles_wide.columns = [\n",
    "    f'WIND_{col[0]}_{col[1].upper()}'.replace(' ', '_') \n",
    "    if col[1] else 'DATETIME' \n",
    "    for col in wind_profiles_wide.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge weather data, field mill readings, and wind profiles into a unified dataframe based on datetime, ensuring all numerical columns are converted to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weather_data(weather_df, field_mill_df, wind_profiles_df):\n",
    "    \"\"\"\n",
    "    Merge weather, field mill, and wind profile data into a single dataframe.\n",
    "\n",
    "    Args:\n",
    "        weather_df: Weather dataframe with hourly measurements\n",
    "        field_mill_df: Aggregated field mill readings\n",
    "        wind_profiles_df: Wide-format wind profile data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all weather measurements merged on datetime\n",
    "    \"\"\"\n",
    "    # Merge all dataframes on TIME/DATETIME columns\n",
    "    merged_data = (weather_df\n",
    "                  .merge(field_mill_df, left_on='TIME', right_on='DATETIME')\n",
    "                  .drop(columns=['DATETIME'])\n",
    "                  .merge(wind_profiles_df, left_on='TIME', right_on='DATETIME', how='left')\n",
    "                  .drop(columns=['DATETIME']))\n",
    "\n",
    "    # Convert all columns except TIME and WEATHER_CODE to float\n",
    "    for col in merged_data.columns:\n",
    "        if col not in ['TIME', 'WEATHER_CODE']:\n",
    "            merged_data[col] = merged_data[col].astype(float)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "all_weather_data = merge_weather_data(weather_filtered, field_mill_agg, wind_profiles_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a WeatherViolationChecker class that checks for weather-related violations (like lightning, wind, and cloud cover) within specified time windows, analyzes patterns of these violations, and generates synthetic samples of weather data while ensuring balance across different categories and window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherViolationChecker:\n",
    "    def __init__(self):\n",
    "        # Thresholds\n",
    "        self.THRESHOLDS = {\n",
    "            'field_mill': 600,\n",
    "            'cloud_cover': 50,\n",
    "            'wind': {\n",
    "                'boundary_layer': 30,\n",
    "                'low_troposphere': 75,\n",
    "                'mid_troposphere': 100,\n",
    "                'upper_troposphere': 125,\n",
    "                'stratosphere': 50\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Required window sizes\n",
    "        self.WINDOW_SIZES = [2, 3, 4, 6]\n",
    "\n",
    "        # Column mappings for wind layers\n",
    "        self.WIND_COLUMNS = {\n",
    "            'boundary_layer': 'WIND_SPEED_MAX_PEAK_BOUNDARY_LAYER',\n",
    "            'low_troposphere': 'WIND_SPEED_MAX_PEAK_LOW_TROPOSPHERE',\n",
    "            'mid_troposphere': 'WIND_SPEED_MAX_PEAK_MID_TROPOSPHERE',\n",
    "            'upper_troposphere': 'WIND_SPEED_MAX_PEAK_UPPER_TROPOSPHERE',\n",
    "            'stratosphere': 'WIND_SPEED_MAX_PEAK_STRATOSPHERE'\n",
    "        }\n",
    "\n",
    "    def check_wind_violations(self, row):\n",
    "        \"\"\"Check wind violations across all atmospheric layers.\"\"\"\n",
    "        violated_layers = []\n",
    "\n",
    "        for layer, column in self.WIND_COLUMNS.items():\n",
    "            # Add null check for wind values\n",
    "            if pd.notna(row[column]) and row[column] > self.THRESHOLDS['wind'][layer]:\n",
    "                violated_layers.append(layer)\n",
    "\n",
    "        return violated_layers if len(violated_layers) >= 2 else []\n",
    "\n",
    "    def analyze_violation_pattern(self, violation_hours, window_size):\n",
    "        \"\"\"Analyze if violations are continuous or intermittent.\"\"\"\n",
    "        if not violation_hours:\n",
    "            return {'type': 'none', 'gaps': []}\n",
    "\n",
    "        sorted_hours = sorted(violation_hours)\n",
    "        gaps = []\n",
    "\n",
    "        for i in range(1, len(sorted_hours)):\n",
    "            gap = (sorted_hours[i] - sorted_hours[i-1]).total_seconds() / 3600\n",
    "            gaps.append(gap)\n",
    "\n",
    "        is_continuous = all(gap == 1 for gap in gaps)\n",
    "\n",
    "        return {\n",
    "            'type': 'continuous' if is_continuous else 'intermittent',\n",
    "            'gaps': gaps,\n",
    "            'duration': (sorted_hours[-1] - sorted_hours[0]).total_seconds() / 3600 + 1 if sorted_hours else 0\n",
    "        }\n",
    "\n",
    "    def check_window_violations(self, window_data):\n",
    "        \"\"\"\n",
    "        Comprehensive check for violations in a time window.\n",
    "        Returns detailed violation information.\n",
    "        \"\"\"\n",
    "        if len(window_data) == 0:\n",
    "            return None\n",
    "\n",
    "        violations = {\n",
    "            'lightning': False,\n",
    "            'wind': False,\n",
    "            'cloud': False,\n",
    "            'details': {\n",
    "                'lightning_hours': [],\n",
    "                'wind_layers': {},\n",
    "                'cloud_hours': [],\n",
    "                'patterns': {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Check each hour in the window\n",
    "        for idx, row in window_data.iterrows():\n",
    "            # Lightning check (with null check)\n",
    "            if pd.notna(row['FIELD_MILL_MAX']) and row['FIELD_MILL_MAX'] > self.THRESHOLDS['field_mill']:\n",
    "                violations['lightning'] = True\n",
    "                violations['details']['lightning_hours'].append(row['TIME'])\n",
    "\n",
    "            # Wind check\n",
    "            wind_violations = self.check_wind_violations(row)\n",
    "            if wind_violations:\n",
    "                violations['wind'] = True\n",
    "                violations['details']['wind_layers'][row['TIME']] = wind_violations\n",
    "\n",
    "            # Cloud cover check (with null check)\n",
    "            if pd.notna(row['CLOUD_COVER']) and row['CLOUD_COVER'] > self.THRESHOLDS['cloud_cover']:\n",
    "                violations['cloud'] = True\n",
    "                violations['details']['cloud_hours'].append(row['TIME'])\n",
    "\n",
    "        # Verify final hour has violation(s)\n",
    "        final_time = window_data['TIME'].iloc[-1]\n",
    "        has_final_violation = any([\n",
    "            final_time in violations['details']['lightning_hours'],\n",
    "            final_time in violations['details']['wind_layers'],\n",
    "            final_time in violations['details']['cloud_hours']\n",
    "        ])\n",
    "\n",
    "        if not has_final_violation:\n",
    "            return None\n",
    "\n",
    "        # Analyze patterns\n",
    "        window_size = len(window_data)\n",
    "        violations['details']['patterns'] = {\n",
    "            'lightning': self.analyze_violation_pattern(\n",
    "                violations['details']['lightning_hours'], \n",
    "                window_size\n",
    "            ),\n",
    "            'wind': self.analyze_violation_pattern(\n",
    "                list(violations['details']['wind_layers'].keys()),\n",
    "                window_size\n",
    "            ),\n",
    "            'cloud': self.analyze_violation_pattern(\n",
    "                violations['details']['cloud_hours'],\n",
    "                window_size\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Generate category and subcategory\n",
    "        active_violations = sorted([\n",
    "            k for k, v in violations.items() \n",
    "            if v is True and k != 'details'\n",
    "        ])\n",
    "\n",
    "        violations['category'] = '_'.join(active_violations)\n",
    "\n",
    "        # Add pattern type to subcategory\n",
    "        pattern_types = []\n",
    "        for violation_type in active_violations:\n",
    "            pattern = violations['details']['patterns'][violation_type]['type']\n",
    "            pattern_types.append(f\"{violation_type}_{pattern}\")\n",
    "        violations['subcategory'] = '_'.join(pattern_types)\n",
    "\n",
    "        return violations\n",
    "\n",
    "    def generate_synthetic_samples(self, merged_data, exclude_dates=None, \n",
    "                                min_samples_per_category=100, \n",
    "                                min_samples_per_window=50):\n",
    "        \"\"\"\n",
    "        Generate synthetic samples with balanced categories and window sizes.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting synthetic sample generation...\")\n",
    "        samples = []\n",
    "        category_counts = defaultdict(int)\n",
    "        window_size_counts = defaultdict(int)\n",
    "        subcategory_counts = defaultdict(int)\n",
    "\n",
    "        # Convert exclude_dates to set of datetime.date objects\n",
    "        if exclude_dates is not None:\n",
    "            print(f\"Processing {len(exclude_dates)} excluded dates...\")\n",
    "            if isinstance(next(iter(exclude_dates)), pd.Timestamp):\n",
    "                exclude_dates = {d.date() for d in exclude_dates}\n",
    "            else:\n",
    "                exclude_dates = {pd.to_datetime(d).date() for d in exclude_dates}\n",
    "\n",
    "        total_rows = len(merged_data)\n",
    "        print(f\"\\nProcessing {total_rows} total rows of weather data...\")\n",
    "        progress_interval = max(1, total_rows // 20)  # Show progress every 5%\n",
    "\n",
    "        for i in range(8, len(merged_data)):\n",
    "            if i % progress_interval == 0:\n",
    "                print(f\"Progress: {i}/{total_rows} rows ({(i/total_rows*100):.1f}%)\")\n",
    "                print(f\"Current samples found: {len(samples)}\")\n",
    "\n",
    "            end_time = merged_data.iloc[i]['TIME']\n",
    "\n",
    "            if exclude_dates and end_time.date() in exclude_dates:\n",
    "                continue\n",
    "\n",
    "            full_window = merged_data.iloc[i-8:i].copy()\n",
    "\n",
    "            for window_size in self.WINDOW_SIZES:\n",
    "                window = merged_data.iloc[i-window_size:i].copy()\n",
    "                violations = self.check_window_violations(window)\n",
    "\n",
    "                if violations:\n",
    "                    category = violations['category']\n",
    "                    subcategory = violations['subcategory']\n",
    "\n",
    "                    # Update counts\n",
    "                    category_counts[category] += 1\n",
    "                    window_size_counts[window_size] += 1\n",
    "                    subcategory_counts[subcategory] += 1\n",
    "\n",
    "                    # Create sample with array data\n",
    "                    sample = {\n",
    "                        'START_TIME': full_window['TIME'].iloc[0],\n",
    "                        'END_TIME': end_time,\n",
    "                        'WINDOW_SIZE': window_size,\n",
    "                        'CATEGORY': category,\n",
    "                        'SUBCATEGORY': subcategory,\n",
    "                        'SYNTHETIC': True,\n",
    "                        'HAS_LIGHTNING': violations['lightning'],\n",
    "                        'HAS_WIND': violations['wind'],\n",
    "                        'HAS_CLOUD': violations['cloud'],\n",
    "                        'LIGHTNING_PATTERN': violations['details']['patterns']['lightning']['type'],\n",
    "                        'WIND_PATTERN': violations['details']['patterns']['wind']['type'],\n",
    "                        'CLOUD_PATTERN': violations['details']['patterns']['cloud']['type']\n",
    "                    }\n",
    "\n",
    "                    # Add weather data as arrays\n",
    "                    for col in merged_data.columns:\n",
    "                        if col != 'TIME':  # Skip TIME column\n",
    "                            sample[col] = full_window[col].values\n",
    "\n",
    "                    samples.append(sample)\n",
    "\n",
    "                # Check if we have enough samples\n",
    "                if (all(count >= min_samples_per_category \n",
    "                    for count in category_counts.values()) and\n",
    "                    all(count >= min_samples_per_window \n",
    "                        for count in window_size_counts.values())):\n",
    "                    break\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        print(\"\\nConverting samples to DataFrame...\")\n",
    "        samples_df = pd.DataFrame(samples)\n",
    "\n",
    "        if len(samples_df) > 0:\n",
    "            # Print summary statistics\n",
    "            print(\"\\nSynthetic Samples Generated:\")\n",
    "            print(f\"Total samples: {len(samples_df)}\")\n",
    "            print(\"\\nSamples per category:\")\n",
    "            print(samples_df['CATEGORY'].value_counts().to_markdown())\n",
    "            print(\"\\nSamples per window size:\")\n",
    "            print(samples_df['WINDOW_SIZE'].value_counts().to_markdown())\n",
    "            print(\"\\nSamples per subcategory:\")\n",
    "            print(samples_df['SUBCATEGORY'].value_counts().to_markdown())\n",
    "        else:\n",
    "            print(\"\\nNo samples generated!\")\n",
    "\n",
    "        return samples_df\n",
    "\n",
    "# Usage:\n",
    "checker = WeatherViolationChecker()\n",
    "exclude_dates = set(launch_corpus['DATE'].unique())\n",
    "bad_synthetic_samples = checker.generate_synthetic_samples(\n",
    "    all_weather_data,\n",
    "    exclude_dates=exclude_dates,\n",
    "    min_samples_per_category=100,\n",
    "    min_samples_per_window=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_synthetic_samples.head(2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a GoodWeatherChecker class that evaluates weather conditions to determine if they are favorable across various parameters like lightning activity, wind speeds, and cloud cover within specified time windows. It then generates synthetic samples of good weather conditions, ensuring a balance across different categories and window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodWeatherChecker:\n",
    "    def __init__(self):\n",
    "        # Thresholds for good conditions\n",
    "        self.THRESHOLDS = {\n",
    "            'field_mill': {\n",
    "                'max': 600,  # Must be below this\n",
    "                'preferred_max': 300  # Ideal range\n",
    "            },\n",
    "            'cloud_cover': {\n",
    "                'optimal': 15,  # 0-15%\n",
    "                'acceptable': 35  # 15-35%\n",
    "            },\n",
    "            'wind': {\n",
    "                # Setting conservative thresholds well below the danger zones\n",
    "                'boundary_layer': 20,  # Safe below 25-30\n",
    "                'low_troposphere': 45,  # Safe below 50-75\n",
    "                'mid_troposphere': 65,  # Safe below 75-100\n",
    "                'upper_troposphere': 90,  # Safe below 100-125\n",
    "                'stratosphere': 35   # Safe below 40-50\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Required window sizes\n",
    "        self.WINDOW_SIZES = [2, 3, 4, 6]\n",
    "\n",
    "        # Column mappings for wind layers\n",
    "        self.WIND_COLUMNS = {\n",
    "            'boundary_layer': 'WIND_SPEED_MAX_PEAK_BOUNDARY_LAYER',\n",
    "            'low_troposphere': 'WIND_SPEED_MAX_PEAK_LOW_TROPOSPHERE',\n",
    "            'mid_troposphere': 'WIND_SPEED_MAX_PEAK_MID_TROPOSPHERE',\n",
    "            'upper_troposphere': 'WIND_SPEED_MAX_PEAK_UPPER_TROPOSPHERE',\n",
    "            'stratosphere': 'WIND_SPEED_MAX_PEAK_STRATOSPHERE'\n",
    "        }\n",
    "\n",
    "    def check_good_wind_conditions(self, row):\n",
    "        \"\"\"Check if wind conditions are favorable across all layers.\"\"\"\n",
    "        good_layers = []\n",
    "\n",
    "        for layer, column in self.WIND_COLUMNS.items():\n",
    "            if pd.notna(row[column]) and row[column] <= self.THRESHOLDS['wind'][layer]:\n",
    "                good_layers.append(layer)\n",
    "\n",
    "        # Return layers only if ALL are good (inverse of violation logic)\n",
    "        return good_layers if len(good_layers) == len(self.WIND_COLUMNS) else []\n",
    "\n",
    "    def analyze_condition_pattern(self, good_hours, window_size):\n",
    "        \"\"\"Analyze if good conditions are continuous or intermittent.\"\"\"\n",
    "        if not good_hours:\n",
    "            return {'type': 'none', 'gaps': []}\n",
    "\n",
    "        sorted_hours = sorted(good_hours)\n",
    "        gaps = []\n",
    "\n",
    "        for i in range(1, len(sorted_hours)):\n",
    "            gap = (sorted_hours[i] - sorted_hours[i-1]).total_seconds() / 3600\n",
    "            gaps.append(gap)\n",
    "\n",
    "        is_continuous = all(gap == 1 for gap in gaps)\n",
    "\n",
    "        return {\n",
    "            'type': 'continuous' if is_continuous else 'intermittent',\n",
    "            'gaps': gaps,\n",
    "            'duration': (sorted_hours[-1] - sorted_hours[0]).total_seconds() / 3600 + 1 if sorted_hours else 0\n",
    "        }\n",
    "\n",
    "    def check_window_conditions(self, window_data):\n",
    "        \"\"\"\n",
    "        Check for good weather conditions in a time window.\n",
    "        \"\"\"\n",
    "        if len(window_data) == 0:\n",
    "            return None\n",
    "\n",
    "        conditions = {\n",
    "            'good_lightning': False,\n",
    "            'good_wind': False,\n",
    "            'good_cloud': False,\n",
    "            'details': {\n",
    "                'good_lightning_hours': [],\n",
    "                'good_wind_hours': {},\n",
    "                'good_cloud_hours': [],\n",
    "                'patterns': {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Check each hour in the window\n",
    "        for idx, row in window_data.iterrows():\n",
    "            # Lightning check (with null check)\n",
    "            if pd.notna(row['FIELD_MILL_MAX']) and row['FIELD_MILL_MAX'] <= self.THRESHOLDS['field_mill']['max']:\n",
    "                conditions['good_lightning'] = True\n",
    "                conditions['details']['good_lightning_hours'].append(row['TIME'])\n",
    "\n",
    "            # Wind check\n",
    "            good_wind_layers = self.check_good_wind_conditions(row)\n",
    "            if good_wind_layers:\n",
    "                conditions['good_wind'] = True\n",
    "                conditions['details']['good_wind_hours'][row['TIME']] = good_wind_layers\n",
    "\n",
    "            # Cloud cover check (with null check)\n",
    "            if pd.notna(row['CLOUD_COVER']) and row['CLOUD_COVER'] <= self.THRESHOLDS['cloud_cover']['acceptable']:\n",
    "                conditions['good_cloud'] = True\n",
    "                conditions['details']['good_cloud_hours'].append(row['TIME'])\n",
    "\n",
    "        # Verify final hour has good conditions\n",
    "        final_time = window_data['TIME'].iloc[-1]\n",
    "        has_final_good = all([\n",
    "            final_time in conditions['details']['good_lightning_hours'],\n",
    "            final_time in conditions['details']['good_wind_hours'],\n",
    "            final_time in conditions['details']['good_cloud_hours']\n",
    "        ])\n",
    "\n",
    "        if not has_final_good:\n",
    "            return None\n",
    "\n",
    "        # Analyze patterns\n",
    "        window_size = len(window_data)\n",
    "        conditions['details']['patterns'] = {\n",
    "            'lightning': self.analyze_condition_pattern(\n",
    "                conditions['details']['good_lightning_hours'], \n",
    "                window_size\n",
    "            ),\n",
    "            'wind': self.analyze_condition_pattern(\n",
    "                list(conditions['details']['good_wind_hours'].keys()),\n",
    "                window_size\n",
    "            ),\n",
    "            'cloud': self.analyze_condition_pattern(\n",
    "                conditions['details']['good_cloud_hours'],\n",
    "                window_size\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Generate category and subcategory based on good conditions\n",
    "        active_conditions = sorted([\n",
    "            k.replace('good_', '') for k, v in conditions.items() \n",
    "            if k.startswith('good_') and v is True and k != 'details'\n",
    "        ])\n",
    "\n",
    "        conditions['category'] = 'good_' + '_'.join(active_conditions)\n",
    "\n",
    "        # Add pattern type to subcategory\n",
    "        pattern_types = []\n",
    "        for condition_type in active_conditions:\n",
    "            pattern = conditions['details']['patterns'][condition_type]['type']\n",
    "            pattern_types.append(f\"{condition_type}_{pattern}\")\n",
    "        conditions['subcategory'] = 'good_' + '_'.join(pattern_types)\n",
    "\n",
    "        return conditions\n",
    "\n",
    "    def generate_synthetic_samples(self, merged_data, exclude_dates=None, \n",
    "                                min_samples_per_category=100, \n",
    "                                min_samples_per_window=50):\n",
    "        \"\"\"\n",
    "        Generate synthetic samples with balanced categories and window sizes.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting good weather synthetic sample generation...\")\n",
    "        samples = []\n",
    "        category_counts = defaultdict(int)\n",
    "        window_size_counts = defaultdict(int)\n",
    "        subcategory_counts = defaultdict(int)\n",
    "\n",
    "        # Convert exclude_dates to set of datetime.date objects\n",
    "        if exclude_dates is not None:\n",
    "            print(f\"\\nProcessing {len(exclude_dates)} excluded dates...\")\n",
    "            if isinstance(next(iter(exclude_dates)), pd.Timestamp):\n",
    "                exclude_dates = {d.date() for d in exclude_dates}\n",
    "            else:\n",
    "                exclude_dates = {pd.to_datetime(d).date() for d in exclude_dates}\n",
    "\n",
    "        print(f\"\\nProcessing {len(merged_data)} total rows of weather data...\")\n",
    "        progress_interval = len(merged_data) // 20  # Show progress every 5%\n",
    "\n",
    "        for i in range(8, len(merged_data)):\n",
    "            if i % progress_interval == 0:\n",
    "                print(f\"Progress: {i}/{len(merged_data)} rows ({(i/len(merged_data)*100):.1f}%)\")\n",
    "                print(f\"Current samples found: {len(samples)}\")\n",
    "\n",
    "            end_time = merged_data.iloc[i]['TIME']\n",
    "\n",
    "            if exclude_dates and end_time.date() in exclude_dates:\n",
    "                continue\n",
    "\n",
    "            full_window = merged_data.iloc[i-8:i].copy()\n",
    "\n",
    "            for window_size in self.WINDOW_SIZES:\n",
    "                window = merged_data.iloc[i-window_size:i].copy()\n",
    "                conditions = self.check_window_conditions(window)\n",
    "\n",
    "                if conditions:\n",
    "                    category = conditions['category']\n",
    "                    subcategory = conditions['subcategory']\n",
    "\n",
    "                    # Update counts\n",
    "                    category_counts[category] += 1\n",
    "                    window_size_counts[window_size] += 1\n",
    "                    subcategory_counts[subcategory] += 1\n",
    "\n",
    "                    # Create sample with array data\n",
    "                    sample = {\n",
    "                        'START_TIME': full_window['TIME'].iloc[0],\n",
    "                        'END_TIME': end_time,\n",
    "                        'WINDOW_SIZE': window_size,\n",
    "                        'CATEGORY': category,\n",
    "                        'SUBCATEGORY': subcategory,\n",
    "                        'SYNTHETIC': True,\n",
    "                        'HAS_GOOD_LIGHTNING': conditions['good_lightning'],\n",
    "                        'HAS_GOOD_WIND': conditions['good_wind'],\n",
    "                        'HAS_GOOD_CLOUD': conditions['good_cloud'],\n",
    "                        'LIGHTNING_PATTERN': conditions['details']['patterns']['lightning']['type'],\n",
    "                        'WIND_PATTERN': conditions['details']['patterns']['wind']['type'],\n",
    "                        'CLOUD_PATTERN': conditions['details']['patterns']['cloud']['type']\n",
    "                    }\n",
    "\n",
    "                    # Add weather data as arrays\n",
    "                    for col in merged_data.columns:\n",
    "                        if col != 'TIME':  # Skip TIME column\n",
    "                            sample[col] = full_window[col].values\n",
    "\n",
    "                    samples.append(sample)\n",
    "\n",
    "                # Check if we have enough samples\n",
    "                if (all(count >= min_samples_per_category \n",
    "                    for count in category_counts.values()) and\n",
    "                    all(count >= min_samples_per_window \n",
    "                        for count in window_size_counts.values())):\n",
    "                    break\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        print(\"\\nConverting samples to DataFrame...\")\n",
    "        samples_df = pd.DataFrame(samples)\n",
    "\n",
    "        if len(samples_df) > 0:\n",
    "            # Print summary statistics\n",
    "            print(\"\\nGood Weather Synthetic Samples Generated:\")\n",
    "            print(f\"Total samples: {len(samples_df)}\")\n",
    "            print(\"\\nSamples per category:\")\n",
    "            print(samples_df['CATEGORY'].value_counts().to_markdown())\n",
    "            print(\"\\nSamples per window size:\")\n",
    "            print(samples_df['WINDOW_SIZE'].value_counts().to_markdown())\n",
    "            print(\"\\nSamples per subcategory:\")\n",
    "            print(samples_df['SUBCATEGORY'].value_counts().to_markdown())\n",
    "        else:\n",
    "            print(\"\\nNo samples generated!\")\n",
    "\n",
    "        return samples_df\n",
    "\n",
    "# Usage:\n",
    "checker = GoodWeatherChecker()\n",
    "exclude_dates = set(launch_corpus['DATE'].unique())\n",
    "good_synthetic_samples = checker.generate_synthetic_samples(\n",
    "   all_weather_data,\n",
    "   exclude_dates=exclude_dates,\n",
    "   min_samples_per_category=100,\n",
    "   min_samples_per_window=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle array nans for both synthetic datasets\n",
    "bad_synthetic_samples = handle_array_nans_in_features(bad_synthetic_samples, features)\n",
    "good_synthetic_samples = handle_array_nans_in_features(good_synthetic_samples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter bad weather samples to exclude cloud-only violations and balance with good weather samples.\n",
    "def filter_bad_weather_samples(bad_samples):\n",
    "    \"\"\"\n",
    "    Filter bad weather samples to exclude cloud-only violations and balance with good weather samples.\n",
    "    Returns filtered dataframe with selected categories.\n",
    "    \"\"\"\n",
    "    # Get counts of good weather samples for reference\n",
    "    good_weather_count = len(good_synthetic_samples)\n",
    "    print(f\"Good weather samples available: {good_weather_count}\")\n",
    "\n",
    "    # List categories we want to keep (excluding cloud-only)\n",
    "    desired_categories = [\n",
    "        'wind',\n",
    "        'cloud_lightning',\n",
    "        'cloud_wind', \n",
    "        'lightning',\n",
    "        'cloud_lightning_wind',\n",
    "        'lightning_wind'\n",
    "    ]\n",
    "\n",
    "    # Filter for desired categories\n",
    "    filtered_bad = bad_samples[bad_samples['CATEGORY'].isin(desired_categories)]\n",
    "\n",
    "    # Show distribution of categories\n",
    "    print(\"\\nBad weather category distribution before sampling:\")\n",
    "    print(filtered_bad['CATEGORY'].value_counts().to_markdown())\n",
    "\n",
    "    # Calculate samples needed per category to roughly match good weather total\n",
    "    total_categories = len(desired_categories)\n",
    "    samples_per_category = good_weather_count // total_categories\n",
    "\n",
    "    # Sample evenly from each category\n",
    "    sampled_dfs = []\n",
    "    for category in desired_categories:\n",
    "        category_data = filtered_bad[filtered_bad['CATEGORY'] == category]\n",
    "        if len(category_data) > samples_per_category:\n",
    "            sampled = category_data.sample(n=samples_per_category, random_state=42)\n",
    "        else:\n",
    "            sampled = category_data  # Take all if we don't have enough\n",
    "        sampled_dfs.append(sampled)\n",
    "\n",
    "    # Combine sampled categories\n",
    "    balanced_bad = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "    print(\"\\nBad weather category distribution after sampling:\")\n",
    "    print(balanced_bad['CATEGORY'].value_counts().to_markdown())\n",
    "\n",
    "    return balanced_bad\n",
    "\n",
    "# Filter and balance bad weather samples\n",
    "balanced_bad_samples = filter_bad_weather_samples(bad_synthetic_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels (1 for bad weather, 0 for good weather)\n",
    "good_synthetic_samples['NOGO_FLAG'] = 0\n",
    "balanced_bad_samples['NOGO_FLAG'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine synthetic datasets\n",
    "synthetic_data = pd.concat([good_synthetic_samples, balanced_bad_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine synthetic and real data\n",
    "X_combined = pd.concat([synthetic_data[features], launch_corpus[features]], axis=0)\n",
    "y_combined = pd.concat([synthetic_data['NOGO_FLAG'], launch_corpus['NOGO_FLAG']], axis=0)\n",
    "\n",
    "# Then do a regular train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, \n",
    "    y_combined,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_combined  # Maintain class distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print basic information about X_train\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"\\nX_train dtypes:\")\n",
    "print(X_train.dtypes)\n",
    "\n",
    "# Print a sample of the first row in a more readable format\n",
    "print(\"\\nFirst row sample:\")\n",
    "for feature in X_train.columns:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"Type: {type(X_train[feature].iloc[0])}\")\n",
    "    print(f\"Shape: {X_train[feature].iloc[0].shape}\")\n",
    "    print(f\"Values: {X_train[feature].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_data(X_train, X_test, features):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model by:\n",
    "    1. Standardizing features\n",
    "    2. Reshaping arrays into 3D format [samples, time_steps, features]\n",
    "\n",
    "    Args:\n",
    "        X_train: Training data with array features\n",
    "        X_test: Test data with array features\n",
    "        features: List of feature names\n",
    "\n",
    "    Returns:\n",
    "        Scaled and reshaped training and test data\n",
    "    \"\"\"\n",
    "    print(\"Starting data preparation for LSTM...\")\n",
    "\n",
    "    # Initialize lists to store processed data\n",
    "    X_train_processed = []\n",
    "    X_test_processed = []\n",
    "\n",
    "    # Process each feature separately\n",
    "    for feature in features:\n",
    "        print(f\"\\nProcessing feature: {feature}\")\n",
    "\n",
    "        # Extract arrays from series and stack them\n",
    "        train_arrays = np.stack(X_train[feature].values)\n",
    "        test_arrays = np.stack(X_test[feature].values)\n",
    "\n",
    "        # Reshape to 2D for scaling\n",
    "        train_2d = train_arrays.reshape(-1, 1)\n",
    "        test_2d = test_arrays.reshape(-1, 1)\n",
    "\n",
    "        # Fit scaler on training data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_2d)\n",
    "\n",
    "        # Transform both sets\n",
    "        train_scaled = scaler.transform(train_2d)\n",
    "        test_scaled = scaler.transform(test_2d)\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        train_reshaped = train_scaled.reshape(train_arrays.shape)\n",
    "        test_reshaped = test_scaled.reshape(test_arrays.shape)\n",
    "\n",
    "        # Append to processed lists\n",
    "        X_train_processed.append(train_reshaped)\n",
    "        X_test_processed.append(test_reshaped)\n",
    "\n",
    "        # Print feature statistics\n",
    "        print(f\"Training set - Mean: {train_scaled.mean():.3f}, Std: {train_scaled.std():.3f}\")\n",
    "        print(f\"Test set - Mean: {test_scaled.mean():.3f}, Std: {test_scaled.std():.3f}\")\n",
    "\n",
    "    # Stack features to create final 3D arrays\n",
    "    X_train_3d = np.stack(X_train_processed, axis=2)\n",
    "    X_test_3d = np.stack(X_test_processed, axis=2)\n",
    "\n",
    "    print(\"\\nFinal array shapes:\")\n",
    "    print(f\"Training set: {X_train_3d.shape} [samples, time_steps, features]\")\n",
    "    print(f\"Test set: {X_test_3d.shape} [samples, time_steps, features]\")\n",
    "\n",
    "    return X_train_3d, X_test_3d\n",
    "\n",
    "# Prepare the data\n",
    "X_train_scaled, X_test_scaled = prepare_lstm_data(X_train, X_test, features)\n",
    "\n",
    "# Verify the shapes and data\n",
    "print(\"\\nData Statistics:\")\n",
    "print(f\"Training set - Mean: {X_train_scaled.mean():.3f}, Std: {X_train_scaled.std():.3f}\")\n",
    "print(f\"Test set - Mean: {X_test_scaled.mean():.3f}, Std: {X_test_scaled.std():.3f}\")\n",
    "\n",
    "# Check for any remaining NaN or infinite values\n",
    "print(\"\\nQuality Check:\")\n",
    "print(f\"Training set NaN count: {np.isnan(X_train_scaled).sum()}\")\n",
    "print(f\"Training set Inf count: {np.isinf(X_train_scaled).sum()}\")\n",
    "print(f\"Test set NaN count: {np.isnan(X_test_scaled).sum()}\")\n",
    "print(f\"Test set Inf count: {np.isinf(X_test_scaled).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of the prepared LSTM data\n",
    "print(\"\\nLSTM Data Sample:\")\n",
    "print(\"\\nTraining Data (First 3 time steps, first 5 features of first sample):\")\n",
    "print(X_train_scaled[0, :3, :5])\n",
    "\n",
    "print(\"\\nTest Data (First 3 time steps, first 5 features of first sample):\")\n",
    "print(X_test_scaled[0, :3, :5])\n",
    "\n",
    "# Show feature dimensions\n",
    "print(\"\\nFeature Names:\")\n",
    "for i, feature in enumerate(features):\n",
    "    print(f\"{i}: {feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(launch_corpus.sample(20).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Single Model\n",
    "\n",
    "def create_lstm_model(input_shape, verbose=True):\n",
    "    \"\"\"\n",
    "    Create LSTM model for weather prediction\n",
    "    Args:\n",
    "        input_shape: Tuple of (time_steps, n_features)\n",
    "        verbose: Whether to print model summary\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First LSTM layer\n",
    "        # Return sequences=True because we're stacking LSTM layers\n",
    "        LSTM(units=64, \n",
    "             return_sequences=True,\n",
    "             input_shape=input_shape),\n",
    "        Dropout(0.2),  # Prevent overfitting\n",
    "\n",
    "        # Second LSTM layer\n",
    "        LSTM(units=32, \n",
    "             return_sequences=False),  # False because it's the last LSTM layer\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Dense layers for final predictions\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification (go/no-go)\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get input shape from our preprocessed data\n",
    "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])  # (time_steps, features)\n",
    "\n",
    "# Create and train model\n",
    "model = create_lstm_model(input_shape)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: K-Fold Cross Validation\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    \"\"\"Recreate our LSTM model architecture\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(units=64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# First, ensure data is in the right format for LSTM\n",
    "# Assuming each feature is already a sequence, we need to stack them\n",
    "def prepare_data_for_lstm(X):\n",
    "    # Stack the arrays\n",
    "    arrays = []\n",
    "    for feature in features:\n",
    "        arrays.append(np.stack(X[feature].values))\n",
    "    return np.stack(arrays, axis=2)\n",
    "\n",
    "# Prepare all data\n",
    "X_combined_3d = prepare_data_for_lstm(X_combined)\n",
    "\n",
    "# Set up K-fold cross validation\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "cv_scores = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': []\n",
    "}\n",
    "\n",
    "# Get input shape from prepared data\n",
    "input_shape = (X_combined_3d.shape[1], X_combined_3d.shape[2])  # (time_steps, features)\n",
    "\n",
    "print(f\"Starting {n_splits}-fold cross validation...\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "\n",
    "# Perform k-fold cross validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_combined_3d)):\n",
    "    print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train_fold = X_combined_3d[train_idx]\n",
    "    X_val_fold = X_combined_3d[val_idx]\n",
    "    y_train_fold = y_combined.iloc[train_idx]\n",
    "    y_val_fold = y_combined.iloc[val_idx]\n",
    "\n",
    "    # Create and train model\n",
    "    model = create_lstm_model(input_shape)\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_fold,\n",
    "        y_train_fold,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_val_fold, verbose=0)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred_classes))\n",
    "    cv_scores['precision'].append(precision_score(y_val_fold, y_pred_classes))\n",
    "    cv_scores['recall'].append(recall_score(y_val_fold, y_pred_classes))\n",
    "    cv_scores['f1'].append(f1_score(y_val_fold, y_pred_classes))\n",
    "\n",
    "    print(f\"Fold {fold+1} Accuracy: {cv_scores['accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nCross-validation results:\")\n",
    "for metric in cv_scores:\n",
    "    mean_score = np.mean(cv_scores[metric])\n",
    "    std_score = np.std(cv_scores[metric])\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    print(f\"  Mean: {mean_score:.4f}\")\n",
    "    print(f\"  Std:  {std_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, seed=42):\n",
    "    \"\"\"Create more stable LSTM model\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(units=64, \n",
    "             return_sequences=True,\n",
    "             input_shape=input_shape,\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        LSTM(units=32,\n",
    "             return_sequences=False,\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(16, \n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Prepare the data first\n",
    "def prepare_data_for_lstm(X):\n",
    "    arrays = []\n",
    "    for feature in features:\n",
    "        arrays.append(np.stack(X[feature].values))\n",
    "    return np.stack(arrays, axis=2)\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train_3d = prepare_data_for_lstm(X_train)\n",
    "X_test_3d = prepare_data_for_lstm(X_test)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_np = y_train.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "# Training with more patience\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train multiple times to check stability\n",
    "n_runs = 3\n",
    "results = []\n",
    "\n",
    "print(\"Input shape:\", X_train_3d.shape)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print(f\"\\nRun {i+1}/{n_runs}\")\n",
    "    set_seeds(42 + i)  # Different seed each time\n",
    "\n",
    "    model = create_lstm_model(input_shape=(X_train_3d.shape[1], X_train_3d.shape[2]))\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_3d, \n",
    "        y_train_np,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    loss, acc = model.evaluate(X_test_3d, y_test_np, verbose=0)\n",
    "    results.append(acc)\n",
    "    print(f\"Run {i+1}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "    # Get predictions and print classification report\n",
    "    y_pred = model.predict(X_test_3d, verbose=0)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_np, y_pred_classes))\n",
    "\n",
    "print(f\"\\nMean Accuracy: {np.mean(results):.4f} ± {np.std(results):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "\n",
    "# 1. Plot training history\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    " \n",
    "    # Loss plot\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 3. Generate predictions and confusion matrix\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating CV Results\n",
    "\n",
    "# 1. Plot CV scores\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Box Plot\n",
    "scores_df = pd.DataFrame(cv_scores)\n",
    "sns.boxplot(data=scores_df, ax=ax1)\n",
    "ax1.set_title('Distribution of Metrics Across Folds (Box Plot)')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Violin Plot (shows full distribution)\n",
    "sns.violinplot(data=scores_df, ax=ax2)\n",
    "ax2.set_title('Distribution of Metrics Across Folds (Violin Plot)')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show individual fold scores in a table\n",
    "print(\"\\nDetailed scores per fold:\")\n",
    "fold_df = pd.DataFrame({\n",
    "    'Fold': range(1, n_splits + 1),\n",
    "    'Accuracy': cv_scores['accuracy'],\n",
    "    'Precision': cv_scores['precision'],\n",
    "    'Recall': cv_scores['recall'],\n",
    "    'F1': cv_scores['f1']\n",
    "}).set_index('Fold')\n",
    "\n",
    "print(fold_df.round(4).to_markdown())\n",
    "\n",
    "# Calculate and display statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(fold_df.describe().round(4).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Hoc Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X_test_3d, y_test, features):\n",
    "    \"\"\"Analyze feature importance using mean absolute impact on predictions\"\"\"\n",
    "\n",
    "    # Initialize importance scores\n",
    "    importance_scores = []\n",
    "    importance_stds = []\n",
    "\n",
    "    # Get baseline predictions\n",
    "    baseline_pred = model.predict(X_test_3d)\n",
    "\n",
    "    # For each feature\n",
    "    for feat_idx in range(X_test_3d.shape[2]):  # Loop through features\n",
    "        # Create copies for permutation\n",
    "        importance_per_run = []\n",
    "\n",
    "        # Repeat multiple times for stability\n",
    "        for _ in range(10):  # number of repeats\n",
    "            # Create a copy of the test data\n",
    "            X_permuted = X_test_3d.copy()\n",
    "\n",
    "            # Shuffle the feature across all time steps\n",
    "            for time_step in range(X_test_3d.shape[1]):\n",
    "                X_permuted[:, time_step, feat_idx] = np.random.permutation(\n",
    "                    X_permuted[:, time_step, feat_idx]\n",
    "                )\n",
    "\n",
    "            # Get predictions with permuted feature\n",
    "            permuted_pred = model.predict(X_permuted)\n",
    "\n",
    "            # Calculate impact (mean absolute difference in predictions)\n",
    "            impact = np.mean(np.abs(baseline_pred - permuted_pred))\n",
    "            importance_per_run.append(impact)\n",
    "\n",
    "        # Store mean and std of importance\n",
    "        importance_scores.append(np.mean(importance_per_run))\n",
    "        importance_stds.append(np.std(importance_per_run))\n",
    "\n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance_Mean': importance_scores,\n",
    "        'Importance_Std': importance_stds\n",
    "    }).sort_values('Importance_Mean', ascending=False)\n",
    "\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.errorbar(\n",
    "        importance_df['Importance_Mean'],\n",
    "        range(len(features)),\n",
    "        xerr=importance_df['Importance_Std'],\n",
    "        fmt='o'\n",
    "    )\n",
    "    plt.yticks(range(len(features)), importance_df['Feature'])\n",
    "    plt.xlabel('Feature Importance (Mean Absolute Impact)')\n",
    "    plt.title('Feature Importance Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "analyze_feature_importance(model, X_test_3d, y_test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Predictions with Original Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_launch_corpus(model, launch_corpus, features):\n",
    "    \"\"\"\n",
    "    Make predictions on launch corpus data\n",
    "    \"\"\"\n",
    "    # Prepare data in 3D format for LSTM\n",
    "    arrays = []\n",
    "    for feature in features:\n",
    "        feature_array = np.stack(launch_corpus[feature].values)\n",
    "        arrays.append(feature_array)\n",
    "    X = np.stack(arrays, axis=2)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = launch_corpus.copy()\n",
    "    results_df['Predicted_NOGO'] = y_pred.flatten()  # Flatten predictions\n",
    "    results_df['Prediction_Confidence'] = y_pred_proba.flatten()  # Flatten probabilities\n",
    "\n",
    "    # Add correct/incorrect flag\n",
    "    results_df['Correct_Prediction'] = results_df['Predicted_NOGO'] == results_df['NOGO_FLAG']\n",
    "\n",
    "    # Print analysis\n",
    "    print(\"\\nPrediction Analysis:\")\n",
    "    print(f\"Total cases: {len(results_df)}\")\n",
    "    print(f\"Correct predictions: {results_df['Correct_Prediction'].sum()}\")\n",
    "    print(f\"Accuracy: {results_df['Correct_Prediction'].mean():.2%}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(results_df['NOGO_FLAG'], results_df['Predicted_NOGO'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze interesting cases\n",
    "    incorrect = results_df[~results_df['Correct_Prediction']]\n",
    "    print(\"\\nIncorrect Predictions Analysis:\")\n",
    "    print(f\"Total incorrect: {len(incorrect)}\")\n",
    "\n",
    "    if len(incorrect) > 0:\n",
    "        print(\"\\nSample of incorrect predictions:\")\n",
    "        for idx, row in incorrect.head().iterrows():\n",
    "            print(f\"\\nCase {idx}:\")\n",
    "            print(f\"Date: {row['DATE']}\")\n",
    "            print(f\"Launch Vehicle: {row['LAUNCH_VEHICLE']}\")\n",
    "            print(f\"Predicted: {'NOGO' if row['Predicted_NOGO'] else 'GO'}\")\n",
    "            print(f\"Actual: {'NOGO' if row['NOGO_FLAG'] else 'GO'}\")\n",
    "            print(f\"Confidence: {row['Prediction_Confidence']:.3f}\")  # No need for [0]\n",
    "            print(f\"Remarks: {row['REMARKS']}\")\n",
    "\n",
    "    # Additional Analysis\n",
    "    print(\"\\nDetailed Statistics:\")\n",
    "    print(\"Average confidence in correct predictions: \"\n",
    "          f\"{results_df[results_df['Correct_Prediction']]['Prediction_Confidence'].mean():.3f}\")\n",
    "    print(\"Average confidence in incorrect predictions: \"\n",
    "          f\"{results_df[~results_df['Correct_Prediction']]['Prediction_Confidence'].mean():.3f}\")\n",
    "\n",
    "    # Plot confidence distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(results_df['Prediction_Confidence'], bins=20, alpha=0.5, label='All predictions')\n",
    "    plt.hist(results_df[~results_df['Correct_Prediction']]['Prediction_Confidence'], \n",
    "             bins=20, alpha=0.5, label='Incorrect predictions')\n",
    "    plt.title('Distribution of Prediction Confidence')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Run predictions on launch corpus\n",
    "results = predict_on_launch_corpus(model, launch_corpus, features)\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "# results.to_csv('launch_predictions.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reason_codes(model, X, features, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Generate reason codes by analyzing feature importance for each prediction\n",
    "    \"\"\"\n",
    "    # Get baseline prediction\n",
    "    baseline_pred = model.predict(X)\n",
    "    \n",
    "    # Store feature impacts\n",
    "    feature_impacts = np.zeros((X.shape[0], len(features)))\n",
    "    \n",
    "    # For each feature, measure its impact\n",
    "    for i, feature in enumerate(features):\n",
    "        # Create copy with feature zeroed out\n",
    "        X_modified = X.copy()\n",
    "        X_modified[:, :, i] = 0\n",
    "        \n",
    "        # Get new prediction\n",
    "        new_pred = model.predict(X_modified)\n",
    "        \n",
    "        # Impact is difference from baseline\n",
    "        feature_impacts[:, i] = np.abs(baseline_pred - new_pred).flatten()\n",
    "    \n",
    "    return feature_impacts\n",
    "\n",
    "def predict_with_reasons(model, launch_corpus, features, n_reasons=3):\n",
    "    \"\"\"\n",
    "    Make predictions with reason codes\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    arrays = []\n",
    "    for feature in features:\n",
    "        feature_array = np.stack(launch_corpus[feature].values)\n",
    "        arrays.append(feature_array)\n",
    "    X = np.stack(arrays, axis=2)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Get feature impacts\n",
    "    feature_impacts = get_reason_codes(model, X, features)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = launch_corpus.copy()\n",
    "    results_df['Predicted_NOGO'] = y_pred.flatten()\n",
    "    results_df['Prediction_Confidence'] = y_pred_proba.flatten()\n",
    "    results_df['Correct_Prediction'] = results_df['Predicted_NOGO'] == results_df['NOGO_FLAG']\n",
    "    \n",
    "    # Add reason codes\n",
    "    for i in range(len(results_df)):  # Use numeric index instead of DataFrame index\n",
    "        impacts = feature_impacts[i]\n",
    "        # Get top N reasons\n",
    "        top_reasons = np.argsort(impacts)[-n_reasons:][::-1]\n",
    "        reasons = []\n",
    "        for reason_idx in top_reasons:\n",
    "            feature = features[reason_idx]\n",
    "            impact = impacts[reason_idx]\n",
    "            if impact > 0.1:  # Only include significant impacts\n",
    "                reasons.append(f\"{feature} ({impact:.3f})\")\n",
    "        results_df.iloc[i, results_df.columns.get_loc('Reason_Codes')] = '; '.join(reasons)\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\nPrediction Analysis:\")\n",
    "    print(f\"Total cases: {len(results_df)}\")\n",
    "    print(f\"Correct predictions: {results_df['Correct_Prediction'].sum()}\")\n",
    "    print(f\"Accuracy: {results_df['Correct_Prediction'].mean():.2%}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(results_df['NOGO_FLAG'], results_df['Predicted_NOGO'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Sample predictions with reasons\n",
    "    print(\"\\nSample Predictions with Reasons:\")\n",
    "    nogo_preds = results_df[results_df['Predicted_NOGO'] == 1]\n",
    "    if len(nogo_preds) > 0:\n",
    "        for _, row in nogo_preds.head().iterrows():\n",
    "            print(f\"\\nDate: {row['DATE']}\")\n",
    "            print(f\"Predicted: {'NOGO' if row['Predicted_NOGO'] else 'GO'}\")\n",
    "            print(f\"Confidence: {row['Prediction_Confidence']:.3f}\")\n",
    "            print(f\"Actual: {'NOGO' if row['NOGO_FLAG'] else 'GO'}\")\n",
    "            print(\"Reason Codes:\")\n",
    "            if isinstance(row['Reason_Codes'], str):\n",
    "                for reason in row['Reason_Codes'].split(';'):\n",
    "                    print(f\"  - {reason}\")\n",
    "            print(f\"Remarks: {row['REMARKS']}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# First add the Reason_Codes column\n",
    "launch_corpus['Reason_Codes'] = ''\n",
    "\n",
    "# Run predictions with reason codes\n",
    "results = predict_with_reasons(model, launch_corpus, features)\n",
    "\n",
    "# Optional: Save detailed results\n",
    "# results.to_csv('launch_predictions_with_reasons.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_reasons(model, launch_corpus, features, n_reasons=3, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions with reasons and adjustable confidence threshold\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    arrays = []\n",
    "    for feature in features:\n",
    "        feature_array = np.stack(launch_corpus[feature].values)\n",
    "        arrays.append(feature_array)\n",
    "    X = np.stack(arrays, axis=2)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X)\n",
    "    \n",
    "    # Try different confidence thresholds\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    print(\"\\nTesting different confidence thresholds:\")\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba > threshold).astype(int)\n",
    "        cm = confusion_matrix(launch_corpus['NOGO_FLAG'], y_pred)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nThreshold: {threshold}\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall: {recall:.3f}\")\n",
    "        print(f\"F1 Score: {f1:.3f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    # Use best threshold based on F1 score\n",
    "    best_threshold = 0.5  # Will be updated based on results\n",
    "    y_pred = (y_pred_proba > best_threshold).astype(int)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = launch_corpus.copy()\n",
    "    results_df['Predicted_NOGO'] = y_pred.flatten()\n",
    "    results_df['Prediction_Confidence'] = y_pred_proba.flatten()\n",
    "    results_df['Correct_Prediction'] = results_df['Predicted_NOGO'] == results_df['NOGO_FLAG']\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(\"Actual NOGO:\", sum(launch_corpus['NOGO_FLAG']))\n",
    "    print(\"Predicted NOGO:\", sum(y_pred))\n",
    "    \n",
    "    # Print high confidence predictions\n",
    "    high_conf = results_df[results_df['Prediction_Confidence'] > 0.8]\n",
    "    print(f\"\\nHigh confidence predictions (>0.8): {len(high_conf)}\")\n",
    "    print(\"High confidence accuracy:\", \n",
    "          (high_conf['Correct_Prediction'].sum() / len(high_conf) if len(high_conf) > 0 else 0))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run analysis with different thresholds\n",
    "results = predict_with_reasons(model, launch_corpus, features)\n",
    "\n",
    "# Optional: Retrain model with class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(launch_corpus['NOGO_FLAG']),\n",
    "    y=launch_corpus['NOGO_FLAG']\n",
    ")\n",
    "\n",
    "print(\"\\nSuggested class weights:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"Class {i}: {weight:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
